
Module 1 : -Introduction To Amazon Web Services

Episode 1 :-

The Client Server Model :-

-In the IT world client is the end user and server is the machine which responds to the clients requests.
-Requests can be anything such as getting the weather data from south africa , a kittens video from youtube etc.
-For eg.
The Coffee Shop :-
-Eddie is in a coffee shop aking Morgan the employee(Barista) of the shop that he wants a coffee.
-Eddie here is a Client and is requesting Morgan The Server to serve his request.

-In the AWS , this server is called as Amazon Elastic Compute Cloud (EC2).
-An EC2 instace, a virtual server.
-The example for eddie and morgan was very simple in itself, but in a mature business level solution , it can get beautifully complex.


You Only Pay For What You Use (Concept):-

-This concept is similar to a coffee shop.
-Morgan being the employee of the the coffee shop gets paid when they are working.
-They will not be paid when they are on a vacation xD.

-The Owner decides how many baristas are needed , and pays them on hourly basis.
-Now lets say the coffee shop is launching a new coffee(The Pumpkin Monster),and the owner has hired lot of baristas for a sudden rush of clients to handle.
-But,that not be the case for a entire day , most of the day can be a few or less customers.
-This can be a issue in a coffee shop or in real world data centers.

The Solution By AWS:-

-When you need baristas or instances , you just click a button and you have them , and when you dont need them , you click a button and they are gone.

Episode 2 :-

Cloud Computing :-

-Cloud computing is the on demand delivery of the IT resources over the internet with pay-as-you-go pricing.
-On demand delivery means AWS has the resources you need when you need them.
-You dont tell them in advance that you will need them.
-All of a sudden you need 200 instances , just a few clicks and you get them.
-All of a sudden you need 2000TB of storage , just few clicks and you get it.
-If you dont need it , you just return it and stop paying for it immediately.

Undifferentiated Heavy Lifting Of IT :-

For eg.

-Your company works with a MYSQL server.
-Now setting up a MYSQL engine on a server is not what differentiates you from your competitors , but how  efficiently you store your data in your database does.

-In AWS this is called as undifferentiated heavy lifting of IT.
-Tasks that are common , often repetitive and ultimately time consuming.
-These are the tasks that aws wants to help you with m so you can focus more on what makes you unique.

Module 1 Assessment Question Notes:-

Question 1 :-

"Deploying applications connected to on-premises infrastructure is a sample use case for a hybrid cloud deployment."

Q.What does the above statement mean?

What is a hybrid cloud deployment?
-A Hybrid cloud is a computing environment that combines :-
1)On-premises infrasturcture(servers/data centers physically located at your site).
2)with Public cloud Services(like AWS,AZURE,GCP)
3)and allows Data and applications to be shared or integrated between your on-premises systems and the cloud systems.

What does the statement say?
-The statement defines a use case for hybrid deployment.
-It means a application is deployed on cloud platform still needs to connect with your existing on-premises systems such as :-
a)Database hosted in your companies data center.
b)Internal API's 
c)Legacy Systems:-
-an old or outdated software platform that a company uses to manage its core business processes, such as:
1)Inventory management
2)Finance and accounting
3)Human resources
4)Order processing
5)Supply chain management
d)Authentication Systems

Example Use Case:-

Let’s say a company has a legacy ERP system in their on-premises data center. Now they build a modern web-based dashboard using React and Node.js and host it on AWS.

This dashboard pulls real-time data from the on-prem ERP system using secure connections (like VPN or Direct Connect). This is a hybrid cloud use case, because:

1)The application is deployed in the cloud,
2)But it is connected to on-premises infrastructure for data.


Q."Running code without needing to manage or provision servers"

What does the above statement mean?

-The above statement refers to a cloud computing model called serverless computing.
-Traditionally when you want to run a code , you have to :-
1)Setup A Server (Physical Or Virtual Unit).
2)Install Software (OS,runtime etc).
3)Manage Updates,Security And Scaling
4)Keep it running 24/7.

-But in serverless computing, you don’t have to worry about any of that. You just:
-Write your code and upload it — the cloud provider runs it for you automatically only when needed.

Who manages the servers then?
-The Cloud Provider(like AWS,GCP or Azure) takes care of :
1)Server Provisioning(starting it up)
2)Infrastructure Management
3)Scaling(Handling more users automatically)
4)Maintainance and security

-You never interact directly with a server.
-AWS Lambda is an AWS service that lets you run code without needing to manage or provision servers.
The AWS Cloud offers three cloud deployment models: cloud, hybrid, and on-premises. 

Question 3 :-

"The aggregated cloud usage from a large number of customers results in lower pay-as-you-go prices."

Q.What does the above statement mean?

Aggregated Cloud Usage :-

-When millions of people and businesses use cloud services ,all their usage adds up (aggregates) into a huge demand for computing , storage and other resources.
-Because of this massive scale cloud providers like (aws,auzre or gcp) can reduce prices for everyone - especially in pay-as-you-go plans where you pay for what you use.

Additional Points To Q3.-
-Not having to invest in technology resources before using them relates to Trade upfront expense for variable expense.
-Accessing services on-demand to prevent excess or limited capacity relates to Stop guessing capacity.
-Quickly deploying applications to customers and providing them with low latency relates to Go global in minutes.


Module 2 :- Compute In The Cloud

The EC2(Elastic Cloud Compute) Instance:-

-Businesses require servers to power their business and applications.
-Businesses require raw compute power to host applications and other business needs.
-The needs for compute power maybe a use case like youtube which streams video content to millions of users.
-In AWS these servers are virtual,and these virtual servers are called as EC2.

-If you go the hard way to get onprem servers, you have to research for which server you want to buy ,it would take weeks to get those servers and you are stuck with those servers when you get them either you use it or not.
-With EC2 , this becomes easier.
-When you need a EC2 instance , you request and get it.If you dont need it,you terminate it.
-In AWS, you only pay for what you use.You only pay for a running EC2 instance and not a stopped or terminated instance.

-EC2 runs on top of physical host machines managed by AWS using virtualization technology.
-When you spin up a EC2 instance,you arent necessarily taking an entire host to yourself.
-Instead , you are sharing the host with multiple other instances, otherwise known as virtual machines.
-A hyperwisor running on the host machine is responsible for sharing the underlying physical resources between the EC2 instances.
-This idea of sharing underlying hardware between multiple virtual machines is called as multitenancy.
-The "hypervisor" is responsible for coordinating this multitenancy and it is managed by aws.
-The hypervisor is reponsible to isolate the virtual machines from each other as they share resources from the host.
-This means EC2 instances are secure.
-Once EC2 instance is not aware of any other instances on the same host sharing the same underlying resources.

-When you provision(start it up) an EC2 instance , you can choose the operating system based on either windows or linux.
-You have this power to configure your own EC2 instance.
-Beyond OS,you also configure what software you want running on your EC2 instance.
-Whether it is a web app,databases , third party software , you have complete control iover what happens in that instance.
-EC2 instances are also resizable, you may start with a small instance , if that instance is being maxed up , you can give more memory and CPU to that instance.
-This process is called as vertically scaling an instance.

-You also control the networking aspect of EC2.
-What type of requests make it to your server and if they are publically or privately accessible is what you decide.

-With EC2, AWS has made it very easy to acquire servers with this Compute-as-a-service model.

Episode 2 :- Amazon EC2 Instance Types

-From the example of coffee shop , the employees are the ec2 instances which server the client requests.
-When the owner hires employees for the coffee shop , he doesnt only hire cashiers.
-He hires waiters,chef's , baristas etc.
-There are tons of roles and responsibilities which employees should have to make a business efficient.
-Its important to look that the employee's skill set suits their role.
-There are different types of ec2 instances that you can spin up and deploy into an aws environment.
-Each Amazon EC2 instance type is grouped under an instance family and is optimized for certain types of tasks.
-Instances with different types of CPU's , Memory , networking capabilities give you the resources you want suitable for your business.

Below are the different Amazon EC2 instance families :-
1)General Purpose:-
-Provide a good balance of COmpute,Memory and networking resources.
-Can be used for general purposes like web servers or copde repositories.
2)Compute Optimized:-
-Optimized for Compute Intensive tasks like gaming servers,High Performance Computing or scientific modelling.
3)Memory optimized:-
-Good for memory intensive tasks
4)Accelerated Computing:-
-Good for floating point number calculations,graphics processing or data pattern matching.
5)Storage Optimized:-
-A good for workloads which require high performance for locally stored data.

Module 2 - Episode 2 - Test Your Knowledge :-

Q1.Which Amazon EC2 Instance Is Suitable For Data Warehousing applications?

-Storage Optimized.
-This is because it is optimized for high local storage throughput.
-Local storage refers to physical storage devices (usually NVMe SSDs) that are directly attached to the host machine running your EC2 instance.
-And optimized for Disk I/O.

Q2.Which Amazon EC2 instance is well suited for High Performance Databases?

-Memory Optimized.
-This is because memory optimized instances provides 100s of GB's or TB's of RAM.
-This allows entire databases to be loaded on to the memory.
-Faster read and write speen due to RAM than disk.
-Better performance for in-memory databases like redis.


Episode 3 :- Amazon EC2 Pricing

-Amazon EC2 purchase options are as follows :-
1)On-demand:-
-Pay on hourly basis or seconds basis depends on what instance and OS you choose.
-Best to test workloads and playaorund when getting started up.

2)Savings Plan:-
-This is like a commitment of consistent usage for 1 or 3 years paid on hourly basis.
-Can save upto 72%.
-Any usage up to the commitment is charged at the discounted Savings Plans rate (for example, $10 per hour). Any usage beyond the commitment is charged at regular On-Demand rates.
-The savings with EC2 Instance Savings Plans are similar to the savings provided by Standard Reserved Instances.
-Unlike Reserved Instances, however, you don't need to specify up front what EC2 instance type and size (for example, m5.xlarge), OS, and tenancy to get a discount. Further, you don't need to commit to a certain number of EC2 instances over a 1-year or 3-year term.

3)Reserved Instances:-
-Often used for steady state workloads.
-This also involves 1 or 3 year term and can pay them using 3 payment options:-
a)All upfront :-All in once payment
b)Partial Upfronmt :-Some partial payment at first
c)No Upfront :-No payment at first.

Reserved Instances require you to state the following qualifications:

i)Instance Type and Size :- for eg. m5.xlarge
ii)Platform Description(OS) :- For eg. Microsoft Windows Server , or  red hat linux enterprise.
iii)Tenancy :-Default or dedicated Tenancy.
-Types of Tenancy :-
1)Default Tenancy :-Your instance runs on shared hardware.Aws hosts your instances on physical servers that are shared with other customers.
2)Reserved Tenancy :-Instance runs on hardware dedicated to your account.But you dont manage the hardware.
-AWS places the instances on dedicated servers,you dont have access to set where they go.
3)Dedicated Host Tenancy :-Gives you full visibility and control over a physical server.
-You can see and control instance placement on that host.

iv)Region :-An instance from which region for eg. Asia(mumbai)
-Types of Reserved Instances :-
i)Standard Reserved Instances:-
-This option is a good fit if you know the EC2 instance type and size you need for your steady-state applications and in which AWS Region you plan to run them.
ii)Convertible Reserved Instances:-
-If you need to run your EC2 instances in different Availability Zones or different instance types, then Convertible Reserved Instances might be right for you.

-At the end of a Reserved Instance term, you can continue using the Amazon EC2 instance without interruption. However, you are charged On-Demand rates until you do one of the following:
i)Terminate the instance.
ii)Purchase a new Reserved Instance that matches the instance attributes (instance family and size, Region, platform, and tenancy).

4)Spot Instances :-
-This allows you to request the spare Amaon EC2 computing capacity for upto 90% off 
the on demand price.
-The catch here is aws can reclaim the instance anytime they need it giving you a 2 minute warning to save up work and save state.
-This is beneficial when your work allows you to be interrupted.
-A good usecase is batch workloads.
-Note:-Spot instances can only be launched if spare capacity is available,other wise it will delay your job.
5)Dedicated Hosts:-
-Which are physical hosts dedicated for your use for EC2.
-Nobody else will share a tenancy of that host.

Episode 4 :- Scaling Amazon EC2

-Another major benefit of AWS is Scalability and Elasticity.
-In on-prem data centers , there are hardwares required.
-The overall consumption of your hardwares can be 10% at non peak hours or 80% at peak hours.
-How do you decide how much resources to buy?
-If you buy resources for average usage lets say which can handle upto 50% customers then they will struggle when there are 80% customers hike which is the key factor for your result.
-If you buy resources for maximum capacity , your customers might be happy but most of the times the overall consumtion of resources might be only 10% and other all will be idle.

How does AWS solves this problem?
-Scalability involves beginning with only the resources you need and designing your architecture to automatically respond to changing demand by scaling out or in. As a result, you pay for only the resources you use. You don’t have to worry about a lack of computing capacity to meet your customers’ needs.
-The AWS service that provides this functionality for Amazon EC2 instances is Amazon EC2 Auto Scaling.
-AWS EC2 Auto Scaling enables you to automatically add or remove Ec2 instances in response to changing application demand.
-Within Amazon EC2 Auto Scaling, you can use two approaches: dynamic scaling and predictive scaling:-
1)Dynamic scaling responds to changing demand. 
2)Predictive scaling automatically schedules the right number of Amazon EC2 instances based on predicted demand.

-Suppose that you are preparing for launch of your application on  EC2 instances.
-When configuring the size of auto-scaling group,you might set the minimum number of EC2 instances at first.
-This means there will be atleast the minimum number of EC2 instances running at all times.

Customizing The AutoScaling Group:-
https://replit.com/@rajdubal87/AWS-Notes#AutoScaling.png

1)Minimum Capacity:-
-Minimum capacity is the minimum number of instances that will immediately launched after you have created a auto-scaling group.

2)Desired Capacity:-
-Next, you can set the desired capacity at two Amazon EC2 instances even though your application needs a minimum of a single Amazon EC2 instance to run.
-Note:-If you do not setup desired capacity,then the desired capacity defaults to your minimum capacity.

3)Maximum Capacity:-
-The third configuration that you can set in an Auto Scaling group is the maximum capacity. For example, you might configure the Auto Scaling group to scale out in response to increased demand, but only to a maximum of four Amazon EC2 instances.
-Because Amazon EC2 Auto Scaling uses Amazon EC2 instances, you pay for only the instances you use, when you use them. You now have a cost-effective architecture that provides the best customer experience while reducing expenses.


Episode 5 :- Directing Traffic With Elastic Load Balancing

-When you have multiple EC2 instances,all server the  same program , does the same job ,
-And if a request comes,how does that request know which EC2 instance to go to?
-How do we ensure there is even distribution of work loads accross multiple EC2 instances.
-You need a way to route requests to instances , to process those requests.
-The solution to this problem is load balancing.
-A load balancer takes in requests and routes them to servers in order to get them processed.


Elastic Load Balancing(ELB) :-

-It is engineerd to address the undifferentiated heavy lifting of load balancing.
-Elastic Load Balancing is a regional construct.
-This service available on your region than on a entire instance.
-ELB is automatically scalable.
-As your traffic grows , it is designed to handle the throughput with no change in the hourly cost.
-When the minimum or desired capacities are maxed out ,the auto scaled out instances make the ELB know that they are ready to handle requests.
-Once the fleet scales in, ELB first stops all new traffic and waits for the current processing requests to get processed completely.
-Once they do that , the autoscaling engine can terminate the instances without disruption to existing customers.
-ELB is not only used for external traffic(Internet-facing traffic like users visiting your website).
-Because ELB is regional ,its a single URL that each frontend instance uses.
-All frontend instances (even across AZs) can use this one URL to reach the backend.
-Then the ELB sends the traffic to the backend that has least outstanding requests.
-When the backend scales , the new instance tells the ELB that it is ready to handle requests and ELB handles it.
-The frontend doesnt have to care how many backend instances are there.
-This is true decoupled architechture.

More About Load Balancer :-

-A load balancer acts as single point of contact for all incoming traffic to your auto scaling group.
-This means that as you add or remove EC2 instances in response to change in demand, this requests route to the lead balancer first.
-Then, the requests spread across multiple resources that will handle them. For example, if you have multiple Amazon EC2 instances, Elastic Load Balancing distributes the workload across the multiple instances so that no single instance has to carry the bulk of it. 


Episode 6 - Messaging and queuing

-In the example of a coffee shop , the cashier and barista are in sync as the cashier gets the order and passes it to barista to prepare it.
-What if the barista is out on a break , the entire process could be delayed or postponed.
-To solve this , the coffee shop can introduce a order board or a buffer.
-This type of idea of using a buffer is known as messaging and queuing.
-Just like cashier sends messages to the barista, applications send messages to communicate.
-If applications communicate directly with each other , just like cashier and barista, then this is called tightly coupled architecture.
-But, in this architecture, if a single component fails , it can create mess for other components working.
-If Application A communicates with Application B regularly, and if the Application B fails , then it can create errors for application as well.
-A more reliable architecture is loosely coupled , single failure cannot cause cascading failures.
-In this architecture , if one component fails, it is isolated and wont cause problems for other components.
-In this , we introduce a buffer/message queue in between.
-App A sends a message to the message queue , message queue sends this message to APp B.
-If for some reason App B is failed , the messages from App A remains in the message queue until the App B is up and running to take requests.

This service is implemented by amazon having name as :-
-Amazon Simple Queue Service (SQS)
-Amazon Simple Notification Service (SNS)

Amazon Simple Queue Server(SQS) :-

-Amazon SQS can Send,Store,Receive messaged between software components at any volumne.
-THis is without loosing messages or without needing support of any other services.
-The data contained within a message is called a payload and is protected until delivery.
-SQS queues is the place where messaged are stored before they are processed.
-AWS manages the underlying infrastucture to host those queues.
-These scale automatically,are reliable and easy to configure and use.


Amazon Simple Notification Service(SNS):-

-Amazon SNS is similar , it is used to send out messages to services,but it can also send out notifications to end users.
-It does it in a different way, in  a publish-subscribe or Pub-Sub model.
-This means you can create something called as SNS topic which is just a channel for messages to be delivered.
-You then configure subscribers to that topic and finally publish those messages for those subscribers.
-With just one go you can send a message in the channel and it will go to all its subscribers.
-These subscribers can also be end points.Such as SQS queues , AWS lambda functions or HTTP/HTTPS web hooks.
Imagine an e-commerce app:

-Order Placed ➜ Message published to SNS.
-Subscribers:
1)Lambda for billing
2)SQS queue for shipping service
3)SQS queue for analytics service

Each component receives the same order event, but processes it independently.
-SNS can also be used to push messages to end users using mobile push,sms and email.
-In Amazon SNS, subscribers can be web servers, email addresses, AWS Lambda functions, or several other options. 

Monolithic Approach :-
https://replit.com/@rajdubal87/AWS-Notes#Monolithic.png

-Applications are made of multiple components. The components communicate with each other to transmit data, fulfill requests, and keep the application running. 

-Suppose that you have an application with tightly coupled components. These components might include databases, servers, the user interface, business logic, and so on. This type of architecture can be considered a monolithic application. 

-In this approach to application architecture, if a single component fails, other components fail, and possibly the entire application fails.

-Here every componenet is packaged together and is typically deployed on the same server.

***To help maintain application availability when a single component fails, you can design your application through a microservices approach.

Microservices approach :-

https://replit.com/@rajdubal87/AWS-Notes#Microservices.png

-In a microservices approach, application components are loosely coupled. In this case, if a single component fails, the other components continue to work because they are communicating with each other. The loose coupling prevents the entire application from failing. 

-When designing applications on AWS, you can take a microservices approach with services and components that fulfill different functions. Two services facilitate application integration: Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS).


Publishing updates from Single Topic(using SNS):-
https://replit.com/@rajdubal87/AWS-Notes#SNS%20Single%20Topic.png

-A coffee shop can have a single newsletter which includes topics such as coffee trivia,coupons,new products.
-All customers who subscribe to the newsletter receive updates about coupons, coffee trivia, and new products.

Publishing updates from multiple topics(using SNS):-

-Suppose now the clients wants a dedicated service for coupons,trivia and new products.
-Now, instead of having a single newsletter for all topics, the coffee shop has broken it up into three separate newsletters. Each newsletter is devoted to a specific topic: coupons, coffee trivia, and new products.
-Subscribers will now receive updates immediately for only the specific topics to which they have subscribed.
-It is possible for subscribers to susbcribe to a single topic or multiple topics.


More About Amazon Simple Queue Service(SQS):-

-Using AWS SQS you can send,store and receive messages between software components, without loosing messages or requiring other services to be available.
-In Amazon SQS, an application sends messages into a queue.
-A user or service retrieves a message from the queue, processes it, and then deletes it from the queue.


**Is a Decoupled Architecture,Loosely Coupled Architecture and Microservices the same?

-Decoupled application components means a loosely coupled architecture, and microservices are one form of that architecture.
-SO:-
1)All microservices are decoupled,
2)But not all decoupled architectures are microservices.

Example:
In a monolithic app, your email module can be decoupled from the order module using events or interfaces — even though it's all one codebase.

Episode 7 :- Additional Compute Services

-EC2 requires you to setup and manage your fleet of instances over time.
-When you are using EC2 you are responsible for patching your instances when new packages come out.
-Setting up the scaling of those instances as well as ensuring that you've architected your solutions to be hosted in highly available manner.
-THis is just not as much management if you hsoted these on premises.

What are some computing services that aws offer that are more convineint from management perspective?

-This is where the term serverless comes in.
-Serverless means you cannot actually see or manage the underlying infrastructure or instances that are hosting your application.
-Instead,all the management of the underlying environment from a provisioning , scaling , high availabilty and management perspective are taken care for you.
-All you have to do is focus on your application and everything else is taken care of.

AWS Lambda :-

-AWS lambda is one aws serverless compute option
-Lambda allows you to upload your code into a lambda function
-COnfigure a trigger , and from there the service waits for the trigger , when the trigger is tiggered the code is run in a managed environment.
-It is automatically scalable , highly available and the maintainance of it is done by aws itself.
-If you have 1 or 1000 triggers,lambda will scale your function to meet demand.
-Lambda is designed to run code under 15 minutes.
-THis isnt long running processes like deep learning , its more suited for quick processing like a web backend , handling requests etc.

-If we are not ready for serverless yet but still want efficiency and protability , you should look at aws container services like Amazon elastic container service(ECS) or amazon elastic kubernetes service(EKS).
-both of these services are container orchestration tools.
-A container in this case is a docker container.
-Docker is a widely used platform , that uses operating system level virtualization to deliver software in containers.
-A container is a package for your code.
-Its where you pakcge your application , its dependencies as well as configurations that you need to run.
-These containers run on top of EC2 instances and run in isolation from each other similar to how virtual machines work.
-But in this case the host is an EC2 instance.
-When you use docker containers on aws , you need processes to stop ,start , restart and monitor containers running accross not just one EC2 instance but a number of them together which is called a cluster.
-The process of doing these tasks is called container orchestration.
-Orchestration tools are created to help you manage your conatiners.
-ECS(elastic container service) is designed to help you run your containerized applications at scale without the hassle of managing your own container orchestration tool.
-EKS does a similar thing but uses different tooling and with different features.
-Both Elastic container service and Elastic kubernetes service can run on top of EC2.
-But if you dont want to think about EC2s to host your containers because you dont need access to underlying OS or you dont want to manage those EC2 instances , you can use a aws platform called AWS Fargate.
-Fargate is a serverless compute platform for ECS or EKS.


The difference between Traditional , serverless and containerzied severless :-

Traditional :-
-If you are trying to host your traditional operations and want full access to underlying operating system , you are going to want a EC2.

Serverless:-
-If you are looking to host short running functions , service oriented or event driven applications
-and you dont want to manage the underlying environment at all , you shoould look into serverless compute model called aws lambda.

Containerized Serverless :-
-If you are looking to run docker container based workloads on aws , you first need to choose your orchestration tool.
-Do you want to use amazon ecs or eks.
-Then you need to select your platform.
-Do you want to host your containers on EC2 that you manage or in a serverless environment called AWS fargate.

More about serverless computing :-

https://replit.com/@rajdubal87/AWS-Notes#ServerlessComputing.png

AWS Lambda Pricing :-

While using AWS Lambda, you pay only for the compute time that you consume. Charges apply only when your code is running. You can also run code for virtually any type of application or backend service, all with zero administration. 

For example, a simple Lambda function might involve automatically resizing uploaded images to the AWS Cloud. In this case, the function triggers when uploading a new image. 

Explaination to above example :-
In AWS Lambda, "set your code to trigger from an event source" means that you configure Lambda to automatically run your function in response to specific events from other AWS services or external systems.

An event source is the place or service where an action (event) happens, and when that event occurs, it triggers your Lambda function.


More About Containers :-

-Containers provide you with a standard way to package your application's code and dependencies into a single object. 
-You can also use containers for processes and workflows in which there are essential requirements for security, reliability, and scalability.


One host with multiple containers :-
https://replit.com/@rajdubal87/AWS-Notes#1hostwithmultiplecontainers.png

-Suppose that a company’s application developer has an environment on their computer that is different from the environment on the computers used by the IT operations staff. The developer wants to ensure that the application’s environment remains consistent regardless of deployment, so they use a containerized approach. This helps to reduce time spent debugging applications and diagnosing differences in computing environments.

The Host Can Be:-
1)An EC2 instance (virtual server in AWS)
2)A dedicated host (a physical server allocated entirely to you)
3)Your own on-premise machine or VM

Ten hosts with hundreds of containers :-
https://replit.com/@rajdubal87/AWS-Notes#10Hosts100Containers.png

When running containerized applications, it’s important to consider scalability. Suppose that instead of a single host with multiple containers, you have to manage tens of hosts with hundreds of containers. Alternatively, you have to manage possibly hundreds of hosts with thousands of containers. At a large scale, imagine how much time it might take for you to monitor memory usage, security, logging, and so on.


Amazon Elastic Container Service (Amazon ECS) :-

Amazon Elastic Container Service (Amazon ECS)(opens in a new tab) is a highly scalable, high-performance container management system that enables you to run and scale containerized applications on AWS. 

Amazon ECS supports Docker containers. Docker(opens in a new tab) is a software platform that enables you to build, test, and deploy applications quickly. AWS supports the use of open-source Docker Community Edition and subscription-based Docker Enterprise Edition. With Amazon ECS, you can use API calls to launch and stop Docker-enabled applications.

Amazon Elastic Kubernetes Service (Amazon EKS):-

Amazon Elastic Kubernetes Service (Amazon EKS)(opens in a new tab) is a fully managed service that you can use to run Kubernetes on AWS. 

Kubernetes(opens in a new tab) is open-source software that enables you to deploy and manage containerized applications at scale. A large community of volunteers maintains Kubernetes, and AWS actively works together with the Kubernetes community. As new features and functionalities release for Kubernetes applications, you can easily apply these updates to your applications managed by Amazon EKS.

AWS Fargate

AWS Fargate(opens in a new tab) is a serverless compute engine for containers. It works with both Amazon ECS and Amazon EKS. 

When using AWS Fargate, you do not need to provision or manage servers. AWS Fargate manages your server infrastructure for you. You can focus more on innovating and developing your applications, and you pay only for the resources that are required to run your containers.

What Does Scaling your instance vertically or horizontally mean?

Scaling Vertically :-

-Resizing your instance with more cpu,memory , storage etc.

Scaling Horizontally :-

-Adding more instances of same size.

Module 2 Quiz :-

Q1.You want to use an Amazon EC2 instance for a batch processing workload. What would be the best Amazon EC2 instance type to use?

Answer :-

Compute Optimized.

Why?

For batch processing, the best EC2 instance type is usually from the Compute Optimized family (like C7g, C6a, C5, etc.) because batch jobs are typically CPU-intensive and time-sensitive, and compute optimized instances give you maximum processing power per dollar.

Batch jobs usually crunch a lot of data and run heavy computations

Why not other options?

-General purpose instances provide a balance of compute, memory, and networking resources. This instance family would not be the best choice for the application in this scenario. Compute optimized instances are more well suited for batch processing workloads than general purpose instances.

-Memory optimized instances are more ideal for workloads that process large datasets in memory, such as high-performance databases.

-Storage optimized instances are designed for workloads that require high, sequential read and write access to large datasets on local storage. The question does not specify the size of data that will be processed. Batch processing involves processing data in groups. A compute optimized instance is ideal for this type of workload, which would benefit from a high-performance processor.

Q.Which process is an example of Elastic Load Balancing?

-Ensuring that no single Amazon EC2 instance has to carry the full workload on its own.

-Elastic Load Balancing is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances. This helps to ensure that no single resource becomes overutilized.


Q.You want to deploy and manage containerized applications. Which service should you use?

-Amazon EKS is a fully managed Kubernetes service. Kubernetes is open-source software that enables you to deploy and manage containerized applications at scale.
-Similarly , Amazon ECS works.
-For fully non-managed containerized service is Amazon Fargate.


Module 3 : Global Infrastructure and Reliability

Episode 1:

-From out coffee shop example lets say there is a monday and we do have lot of customers coming to ouyr coffee shop to get a coffee.
-Now,lets say there is a parade going on the same road as the coffee shop.
-This will make our customers diverted from the coffee shop from getting a coffee,
-What can be a solution?
-This is not the only coffee shop at that location.
-The cafe is actually a chain and we have locations all around the city.
-This way even if there is a parade or power outage , customers can go to other shops and get their coffee.
-This way we make our money and customers get their coffee.

-This is the similar way in which the aws global infrastructure is setup.

The AWS global infrastructure :-

-It is not good that there is this single location where all the data center resources are.
-If something would happen to that data center , like a power outage or natural disaster,everyuones apps would go down all at once.
-You will need high availability and fault tolerence.
-Turns out it is also not good enough to have 2 data centers.
-AWS operates in all sorts of different areas , around the world called regions.


Episode 2 : AWS regions

-The main problem with data centers is any calamities , disasters can happen and no matter who owns the data centers rather be a onprem or aws , it will create problems.
-Most of the businessesd stop because of it.
-Businesses cannot run on the hope that disaster wont happen.

Solution By AWS:-

-AWS builds regions to be closest to where the business traffic demands.
-Locations like , dublin , ohio , mumbai etc
-Inside each region , we have multiple data centers that have all the compute, storage and other services you need to run your applications .
-Each region can be connected to other region with a  high speed fibre optic network controlled by aws.
-Each region is isolated from every other region in the sence that no data goes in and out of your environment from that region without you explicitly granting permissions to move that data out.
-This is a critical security conversation to have.

For eg.

-You might have government compliance requirements that your financial information in frankfurt cannot leave germany.
-This is how aws works , no data from one region moves to another without explicit permissions with right credentials is granted.
-Regional data sovereignty(the power that a country has to control its own government) is part of the critical design of aws regions.

You as a business owner how do you make a decision in which region you pick?

There's four business factors that go into choosing a region :-

1)Compliance :-
-You must first look at your compliance requirements.
-you have a requirement that your data must live in indian boundaries.
-Then you should choose the mumbai region.
-No other thing should come before compliance.
2)Proximity:-
-How close you are to your customer base is a major factor because speed of light , still the law of the universe.
-You having cutsomer base of india and setting up a server at us region can create latency problems.
-Laterncy is the time it takes for data to be sent and received.
-Locating close to your customer base is usually the right call.
3)Feature availability:-
-Sometimes the closest region may not have all the aws features you want.
-For eg. Your developers want to play with Amazon Bracket(AWS's new quantom computing platform) , well then they have to run in the platform that already have the hardware installed.
4)Pricing:-
-Even though the hardware is the same , some locations are more expensive than other locations to operate in.
-For eg.Brazil's tax structure is more expensive for aws to operate than compared to many other countries.
-If price is your primary concern , even if your customer base is in brazil region , you might wanna set your region as oregon,usa .


Episode 3 : Availability zones 

-AWS calls a single data center or a group of data centers an availability zone.
-EAch availability zone is one or more discrete data centers with redundant power,networking and connectivity.
-When you launch an EC2 instance, it launches an virtual server on a physical hawdware that is installed in an  availability zone.
-This means each aws region consist of multiple isolated physically separate availability zones within a geographic region.
-a region like Mumbai contains multiple AZs, which are like multiple AWS-managed, isolated data centers within that city or surrounding area. This setup enables resilient and scalable cloud architecture.
-AWS keeps each availability zone 10s of miles accross so that in the case of huge scale natural disaster , your application is still running.
-AWS advices that to run accross two availabiltiy zones in a region.
-This means redundantly deplopying your application over aws az's to keep your app ruinning everytime.
-Redundant means having duplicate systems/components in place so that if one fails, the other can take over without service interruption.
For eg. Elastic Load Balancing is a regional construct which runs on a region and not on a single EC2 or on a single AZ.
-It runs accross all availability zones communicating with the EC2 instances..
-When you want high availability , any services that is regionally scoped service is always highly available.

More about availabilty zones:-

https://replit.com/@rajdubal87/AWS-Notes#AWSAvailabilityZones.png

-An Availability Zone is a single data center or a group of data centers within a Region. Availability Zones are located tens of miles apart from each other. This is close enough to have low latency (the time between when content requested and received) between Availability Zones. However, if a disaster occurs in one part of the Region, they are distant enough to reduce the chance that multiple Availability Zones are affected.

How is using multiple availability zones in a region useful :-

https://replit.com/@rajdubal87/AWS-Notes#SingleAZ.png

https://replit.com/@rajdubal87/AWS-Notes#MultipleAZ's.png

https://replit.com/@rajdubal87/AWS-Notes#AZFailure.png


Q."A data center that an AWS service uses to perform service-specific operations" ?

-An Edge Location
-An edge location is a data center that an AWS service uses to perform service-specific operations. 

Q."A service that you can use to run AWS infrastructure within your own on-premises data center in a hybrid approach" ?

-AWS Outposts is a service that you can use to run AWS infrastructure, services, and tools in your own on-premises data center in a hybrid approach.

Q."A geographical area that contains AWS resources" ?

-A Region is a geographical area that contains AWS resources.


Episode 4 : Edge Locations 

-One of the major criteria in selecting a aws region was proximity to your customers.
-But what if you have customers all over the world or in cities which are not closer to one of aws regions?
For eg.If you have customers in mumbai , who have access to your data , but your data is hosted out of tokyo region.
-Rather than having mumbai based customers send requests to tokyo to access that data , just place a copy locally or cache a copy in mumbai.
-Caching the data closer to the customers all around the world uses the concept of content delivery networks or CDN's.
-In AWS , CDN's are called as Amazon CloudFront.

Amazon CloudFront:-

-It is a service that helps deliver data,video,applications and API's with customers around the world with low latency and high transfer speeds.
-Amazon Cloudfronts uses what are called edge locations all around the world to help accelerate communication with users , no matter where they are.
-Edge locations are separate from regions.
-So you can push content from inside a region to a collection of edge locations around a world in order to accelerate communication and content delivery.
-AWS Edge Locations , also run more than just cloudfront , they run a domain name service or DNS known as Amazon Route 53 , helping direct customers to the right web locations with reliably low latency.
-But what if your business wants to use wants to use aws services inside of their own building?

Introducing AWS Outposts:-

-Here , AWS will install fully operational mini region, right inside your data center.
-That's owned and operated by aws.
-It uses 100% of aws functionality , but isolated within your own building.


Summary :-

1)Regions are geographically isolaed areas where you can access services needed to run your enterprice.
2)Regions contain availability zones that allows you to run accross physically serparated buildings while keeping your application logically unified.
3)AWS Edge Locatons run Amazon Cloudfront to help get content closer to your customers.

More About Edge Locations :-

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocations.png

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocationsA.png

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocationsB.png

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocationsC.png


Episode 5 : How to provision AWS resources

-How do you actually interact with these services?
-The answer is API's
-In AWS , everything is an API call.
-An API is a application programming interface and what this means is there are pre-determined ways for you to interact with AWS services.

Q.Why is a API called "Application Programming Interface"?

1)Application :- Application here means a software you are building or integrating with.
2)Programming :- You are writing code to interact with it.
3)Interface :- A set of exposed commands , inputs , outputs you can use - like a control pannel.

An interface (set of commands) for programmers to interact with an application or service.

Continuing with how to interact with aws services :-

-You can invoke these API's to provision , configure or manage your AWS resources.
For eg.You can launch a EC2 instance or you can create a AWS lambda function.
-Each of those will be different requests and different api calls to aws.
-You can use the aws management console , the aws command line interface(CLI) , the aws software development kits or various other tools like aws CloudFormation to create requests to send to AWS API's to create and manage aws resources.


1)AWS Management Console :-

-Through AWS console,you can manage your resources visually.
-It is a good starting point.
-This is good for building out test environments or viewing aws bills , vewing monitoring and working wit other non technical resources.
-However when your application is up and running in a production environment you dont want to rely on the point and click style that the console that your console gives you to create and manage your aws resources.
For Example : In order to create an amazon EC2 instance, you need to click through various screens , setting all the configurations you want and then you launch your instance.
-If later you wanted to launch another EC2 instance, you will need to go back into the console and click into the screens again to launch another EC2 instance.
-By having humans do  this sort of manual provisioning , you are opening yourself to potential errors.
-It is easy to forget to check a checkbox or mis-spell something while doing the entire thing manually.

2)AWS CLI :-

The answer to this problem is to use tools that allow you to script or program the API calls.
-One tool you can use is the AWS CLI.
-The CLI allows you to make API calls using the terminal on your machine.
-It makes the complete scene prone to errors if you have pre-written scripts to setup a instance.
-You can have this scripts run automatically like on a schedule or triggered by another process.
-Automation is  very important to having a successfull and predictible cloud deployment over time.

3)AWS SDK :-

-Another way to interact with AWS services is AWS SDK.
-The software development kits allows you to interact with aws resources through various programming languages.
-This helps developers develop programs that use aws without using the low-level api's ,as well as avoiding the manual resource creation.
-Here low level API's mean The raw HTTP requests you would have to send directly to AWS services, using their REST APIs, without any helper tools.
-These APIs are the under-the-hood web interfaces that AWS exposes — and they’re powerful, but very complex and verbose.

 What the AWS SDK Does Instead:

The AWS SDK (Software Development Kit) gives you ready-made libraries in popular languages like:
1)Python (boto3)
2)JavaScript/Node.js (aws-sdk)
3)Java, Go, C#, etc.

You just write few lines of code:-

import boto3

s3 = boto3.client('s3')
s3.create_bucket(Bucket='my-bucket')


More About AWS Management Console :-

The AWS Management Console is a web-based interface for accessing and managing AWS services. You can quickly access recently used services and search for other services by name, keyword, or acronym. The console includes wizards and automated workflows that can simplify the process of completing tasks.

You can also use the AWS Console mobile application to perform tasks such as monitoring resources, viewing alarms, and accessing billing information. Multiple identities can stay logged into the AWS Console mobile app at the same time.

More About AWS Command Line Interface (CLI) :-

To save time when making API requests, you can use the AWS Command Line Interface (AWS CLI). AWS CLI enables you to control multiple AWS services directly from the command line within one tool. AWS CLI is available for users on Windows, macOS, and Linux. 

By using AWS CLI, you can automate the actions that your services and applications perform through scripts. For example, you can use commands to launch an Amazon EC2 instance, connect an Amazon EC2 instance to a specific Auto Scaling group, and more.


More About AWS Software Development Kits (SDK) :-

Another option for accessing and managing AWS services is the software development kits (SDKs). SDKs make it easier for you to use AWS services through an API designed for your programming language or platform. SDKs enable you to use AWS services with your existing applications or create entirely new applications that will run on AWS.

To help you get started with using SDKs, AWS provides documentation and sample code for each supported programming language. Supported programming languages include C++, Java, .NET, and more.


Episode 6: How to provision AWS resources Part 2

There are 3 ways to interact with AWS resources :-

1)AWS management console
2)AWS CLI
3)AWS SDK

-WHich are "do it your own way" interacting options.

To provision a resource :-
You will have to
1)Login into your AWS management console.
2)Write commands
3)Write Programs

There are also other ways you can manage your aws environment using manage tools like 

1)AWS Elastic BeanStalk
2)AWS CloudFormation

AWS Elastic BeanStalk :-
-It is a service that helps you provision Amazon EC2-based environments.
-Instead of clicking around the console or writing multiple commands to build out your network , EC2 instances , scaling and elastic load balancers
-You can instead provide your application code and desired configurations to the aws elastic beanstalk service.
-Which takes that information and builds out your environment for you.
-AWS elastic beanstalk also makes it easy to save environment configurations , so they can be deployed again easily.
-AWS BeanStalk gives you the convinience of not having to provision and manage all of those pieces separately, while still giving you the visibility and control of the underlying resources.
-You get to focus on your business application and not the infrastructure.

AWS CloudFormation :-

-Another service that lets you do automated and repeated deployments is AWS CloudFormation.
-AWS Cloud Formation is a infrastructure as a code tool used to define a wide variety of AWS resources in a declarative way using json or YAML text-based documents called CloudFormation templates.
-A declarative format like this allows you to define what you want to build , without specifying the details of exactly how to build it.
-CloudFormation lets you define what you want and the cloud formation  engine will worry about the details on calling the API's to get everything built out.
-It isnt just limited to EC2 based solutions.
-CloudFormation doesn't only manage resources that you directly launch on EC2.
-Some AWS Services don’t require EC2 instances at all:
1)S3
2)RDS(MySQL)
3)Lambda
4)Athena etc.
-Cloudformation supports many different aws resources from storage , databases , analytics , machine learning and more.
-Once you define your resources in a CloudFormation Template  , cloudformation will parse the template and begin provisioning all the resources you defined in parallel.
-AWS Cloudformation manages all the calls to the backend aws api's for you.
https://replit.com/@rajdubal87/AWS-Notes#AWSCloudFormation.png

-You can run the same cloudformation template with multiple accounts or multiple regions and it will create identical environments accross them.
-There is less room for human error as it is a totally automated process.


More About Elastic BeanStalk :-

With AWS Elastic Beanstalk, you provide code and configuration settings, and Elastic Beanstalk deploys the resources necessary to perform the following tasks:
1)Adjust capacity :- Here capcity means raw compute power
2)Load balancing
3)Automatic scaling
4)Application health monitoring


More About AWS CloudFormation :-

AWS CloudFormation provisions your resources in a safe, repeatable manner, enabling you to frequently build your infrastructure and applications without having to perform manual actions. It determines the right operations to perform when managing your stack and rolls back changes automatically if it detects errors.


Q. "Ability to assign custom permissions to different users"?

-Assigning custom permissions to different users is a feature that is possible in all AWS Regions.
-It is a regional construct.

Q."A service that enables you to run infrastructure in a hybrid cloud approach"?

-AWS Outposts
-AWS Outposts is a service that enables you to run infrastructure in a hybrid cloud approach.

Q."A serverless compute engine for containers"?

-AWS Fargate
-AWS Fargate is a serverless compute engine for containers.

Note :-

Amazon ECS and Amazon EKS are services used to manage container orchestration at scale.
-Orchestration here mean Automating the Management of Containers.

ECS has following options :-

Launch Type?	   Who Manages the Infrastructure?	 Use Case?
ECS on EC2	     You do (manage EC2 instances)	   More control, but more setup
ECS on Fargate	 AWS does (serverless)	           Less work, easy scaling


Q."What is an Origin Server"?

-An origin is the server from which CloudFront gets your files. 
-Examples of CloudFront origins include Amazon Simple Storage Service (Amazon S3) buckets and web servers.


Module 4 : Networking

Episode 1:

-From our coffee shop , what if there are eager customers who want to give their orders directly to the barista than going to a Cashier?
-It is not good to allow all customers to directly interact with baristas since they are already busy in brewing some best coffee's.


Amazon Virtual Private Cloud(VPC) :-

-A VPC lets you provision(setting up) a logically isolated section of the aws cloud where you can launch aws resources in a virtual network that you define.
-These resources can be public facing so they have access to the internet or private with no internet access usually for backend services like databases or application servers.
-When AWS says "private resources have no internet access", it means:
1)They cannot directly connect to or be reached from the public internet (no public IP, no internet gateway).
2)But they can still function perfectly within your private network (VPC).
3)They Serve Internal Traffic Only.

-The public and private grouping of resources are known as subnets.
-and they are ranges of IP addresses in your vpc.
-A subnet (short for sub-network) is a smaller range of IP addresses carved out from your VPC's overall IP space.
-If your VPC is a neighborhood, then subnets are the blocks (streets) inside it, and each IP is like a house address.
 "Range of IP Addresses" Means:
 -When you create a VPC , you get to define a CIDR block(Class-less inter domain routing block) , this defines the total range of IP addresses your virtual network can use.

 Example:

 VPC CIDR block: 10.0.0.0/16

This gives you:

IPs from 10.0.0.0 to 10.0.255.255

That’s 65,536 IP addresses!

You divide the VPC into subnets like:
1)Public Subnet: 10.0.0.0/24 → 256 IPs (used for internet-facing resources like load balancers)
2)Private Subnet: 10.0.1.0/24 → 256 IPs (used for backends, databases, etc.)

Each subnet has its own range of IPs assigned from the VPC’s total pool.


Now In Our Coffee Shop :-

-We have different employee's and 1 is a cashier.
-They take customer's orders and thus we want customers to interact with them, so we put them in what we call a public subnet.
-Hence they can talk to the customers or the internet.
-But for baristas , we want them to focus on making coffee and not interact with customers direclty.
-So we put them in a private subnet.


Episode 2 : Connectivity to AWS 

-VPC or Virtual Private Cloud is your own private network in aws.
-A networking service that you can use to establish boundaries around your aws resources is Amazon Virtual Private Cloud (VPC).
-A VPC allows you to define your private IP range for your aws resources.
-You place things like your EC2 instances , your ELB's inside of your VPC.
-You place these resources into different subnets.
-Subnets are chunks of IP addresses in your VPC that allow you to group resources together.
-Subnets control whether resources are either publicly or privately available.
-For some VPC's you might have internet facing resources that the public should be able to reach like a public website for example.
-However in other scenarios you might have resources that you only want to be reachable if someone is logged into your private network.
-This might be internal services like a backend database.

-In order to allow public traffic flow into and out of your VPC , you must attach an Internet Gateway or IGW to your VPC.
-An internet gateway is like a doorway that is open to public.
-Think of it like a coffee shop , without a doorway , customers cant get in our coffee shop.
-So we install a front door for people to get in and out of our coffee shop.
-The front door here is like a gateway.
-Without it , no one can get into your resources placed inside of your VPC.


VPC With All Internal Private Resources :-

-WE dont want anyone from anywhere to reach these resources.
-So we dont want a internet gateway attached to our VPC.
-Instead , we want a private gateway that will only allow traffic if it is coming form a approved network.
-This private doorway is called a virtual private gateway.
-And it allows you to create a VPN connection between a private network , like your on premises data center or your internal corporate network to your vpc.
-Taking it back to the coffee shop , this would be like a private bus route going from my building to the coffee shop.
-If I want to get coffee , i must first badge into the building thus authenticating my identity and then I can take the internal bus route to the internal coffee shop that only people from my building can use.
-So if you want to establish a encrypted VPN connection to your private internal aws resources you will need to attach a virtual private gateway to your vpc.
-Now the problem with our super secret bus route is that it still uses the open road.
-It is succeptible to traffic jams,parades caused by the rest of the world.
-THe same thing is true for VPN connections.
-They are private and they are encrypted ,they share the same bandwidth that is shared by many people that use the internet.

-To solve this problem , we need a magic doorway which leads directly to the coffee shop from home.
-Like a door in our home to coffee shop.
-The point here is you want a private connection , and you want it dedicated and it should not be shared with anyone else.
-You want the lowest amount of latency possible with highest amount of security possible .
-With aws you can achieve that with what is called AWS Direct Connect.


AWS Direct Connect :-

-Direct Connect allows you to establish a completely private , dedicated fiber connection from your data center to aws.
-You work with a direct connect partner in your aread to establish this connection.
-AWS direct connect provides a physical line that connects your network to your aws VPC.
-Its also important to note that one vpc might have multiple types of gateways attached for multiple types of resources all residing in the same vpc just in different subnets.

More About VPC :-

-Amazon VPC enables you to provision an isolated section of the AWS Cloud. In this isolated section, you can launch resources in a virtual network that you define. Within a virtual private cloud (VPC), you can organize your resources into subnets. A subnet is a section of a VPC that can contain resources such as Amazon EC2 instances.

VPC is a regional construct:
A Virtual Private Cloud (VPC) spans an entire AWS region and provides the networking framework for your resources. It includes routing tables, internet gateways, NAT gateways, and other networking components.

Subnets within a VPC:

Subnets are Availability Zone (AZ)-specific, meaning each subnet resides in a single AZ.
Subnets can be classified as public (accessible from the internet) or private (isolated from direct internet access).
A VPC can have multiple subnets distributed across different AZs within the region.

ELB works inside a VPC:

An Elastic Load Balancer (ELB) operates within a VPC and distributes traffic to targets (e.g., EC2 instances, containers, or IP addresses).
The ELB is a regional construct, meaning it spans multiple AZs and can route traffic to resources in subnets across those AZs.
For example, if your VPC has subnets in three AZs, the ELB can distribute traffic to EC2 instances in all three subnets, ensuring high availability and fault tolerance.


Episode 3 : Subnets and Network Access Control


AWS Wide amount of networking tools that covers every layer of security:-

1)Network Hardening
2)Application Security
3)User Identity
4)Authentication and Authorization
5)Distributed denial of service (DDOS) prevention.
6)Data Integrity
7)Encryption

1)Network Hardening :-

-The only technical reason to use subnets in a VPC is to control access to the gateways.
-The public subnets have access to the internet gateway and the private subnets do not.
-Subnets can also control traffic permissions.
-Packets are messaged from the internet.
-Every packet that crosses subnet boundaries gets checked against something called a network access control list or network ACL.
-This check is to see if the packet has permissions to either leave or enter the subnet based on who it was sent from and how its trying to communicate .
-You can think of Network ACLs as passport officers , if you're on the approved list ,  you get thorugh.
-If you are not on the list or if you are explicitly on the do not enter list , then you getr blocked.
-Network ACL check traffic incoming and outgoing from the subnet.
-Just because you were let in , doesnt necessarily mean they are gonna let you out.
-Approved traffic can be sent on its way and potentially harmful trafic like attempts to gain control of a system through administrative requests , they getr blocked before they ever touch the target.

But is that enough?

-A network ACL only gets to evaluate the packet which crosses a subnet boundary , in or out.
-It doesnt evaluate if a packet can reach a specific EC2 instance or not.
-Sometimes you will have multiple EC2 instances in the same subnet , but they might have different rules around who can send them messages , what port those messages are allowed to be sent to.
-SO you need instacnce level security as well.

To solve instance level access questions we introduce Security Groups :-

-Every EC2 instance when it is launched automatically comes with a security group.
-By default the security group does not allow any traffic into the EC2 instance at all.
-All ports are blocked.
-All IP addresses sending packets are blocked.
-If you actually want an EC2 instance to accept traffic from lets say a frontend message or message from the inertnet.
-So you can modify the security group to accept a specific type of traffic.
-In the case of a website , you want webn based traffic or https request to be accepted but not other traffic lets say Operating system or administration requests.
-In a ec2 instance , we can think of it a building and a doorman being the security group.
-This doorman will allow specific allowed requests in , and by default all the requests can exit without being checked.

The Key Difference between a Security Group And A Network Access Control List:-

-The Security Group is stateful.Meaning it has some kind of memory in order to who to allow in or out.If you allow an incoming request, the response is automatically allowed out — no need to explicitly allow outgoing traffic for that connection.
-On the other hand , the network ACL is stateless , which remembers nothing.Checks every packet that crosses its borders regardless of its circumstances.
-Because NACLs are stateless, they treat inbound and outbound traffic as separate events. 


More :-

-This traffic management  , it doesnt care about the contents of the packet itself.In fact it doesnt even open the envelope.It cant.All it can see is if the sender is on the approved list.

What are ephermal ports :-

-When your device sends a request to  a server , it uses :-
1)A fixed destination port(80 for http or 443 for https)
2)And a random high numbered source port (ranging from 1024–65535)

-This random port is called ephermal port.Its how your system keeps track of which request the incoming response belongs to.


Sending Packets from a subnet to another:-
https://replit.com/@rajdubal87/AWS-Notes#SendingPacketsBetweenSubnets.png

-Consider a example of subnet 1 and subnet 2 which are having EC2 instance A and EC2 instance B respectively.
-Its the same VPC but different subnets.
-Lets say ec2 instance A sends a request to EC2 instance B.
-By default,every outbound request is allowed by a security group of a ec2 instance.
-Then the request goes to a Network ACL.The network ACL checks the outgoping request if it is on the approved list or not.
-If it is approved , then the request goes to the NACL(Network Access Control List) of Subnet 2.
-Every subnet has a unique ACL with their own checklists.
-If the request is approved there,then it goes to the security group of EC2 instance B and if there it is approved , it finally reaches the instance.

Once the transaction is complete, now its the time to come home.

**-Its the return traffic pattern which is the most interesting,because this is where the stateful vs stateless nature of the different engines come into play.Because the packet still needs to be evaluated at each checkpoint.
-The security groups by default allow the return traffic out no matter what.
-In the ACL , every ingress and eggress is checked in the list.
-The ACL dont care dont know if yoyu have been in there or not.The package return address has to be on their approved list in order to cross the ACL border.
-Stateless control means it always checks its lists.
-While going back to subnet 1 , it has to go through its subnet as well.
-Here comes the main meaning of being stateful.
-Since now the request has made it back to instance A , the security group of instance A recognizes the packet from before.So it doesnt need to check of it is allowed in





























