
Module 1 : -Introduction To Amazon Web Services

Episode 1 :-

The Client Server Model :-

-In the IT world client is the end user and server is the machine which responds to the clients requests.
-Requests can be anything such as getting the weather data from south africa , a kittens video from youtube etc.
-For eg.
The Coffee Shop :-
-Eddie is in a coffee shop aking Morgan the employee(Barista) of the shop that he wants a coffee.
-Eddie here is a Client and is requesting Morgan The Server to serve his request.

-In the AWS , this server is called as Amazon Elastic Compute Cloud (EC2).
-An EC2 instace, a virtual server.
-The example for eddie and morgan was very simple in itself, but in a mature business level solution , it can get beautifully complex.


You Only Pay For What You Use (Concept):-

-This concept is similar to a coffee shop.
-Morgan being the employee of the the coffee shop gets paid when they are working.
-They will not be paid when they are on a vacation xD.

-The Owner decides how many baristas are needed , and pays them on hourly basis.
-Now lets say the coffee shop is launching a new coffee(The Pumpkin Monster),and the owner has hired lot of baristas for a sudden rush of clients to handle.
-But,that not be the case for a entire day , most of the day can be a few or less customers.
-This can be a issue in a coffee shop or in real world data centers.

The Solution By AWS:-

-When you need baristas or instances , you just click a button and you have them , and when you dont need them , you click a button and they are gone.

Episode 2 :-

Cloud Computing :-

-Cloud computing is the on demand delivery of the IT resources over the internet with pay-as-you-go pricing.
-On demand delivery means AWS has the resources you need when you need them.
-You dont tell them in advance that you will need them.
-All of a sudden you need 200 instances , just a few clicks and you get them.
-All of a sudden you need 2000TB of storage , just few clicks and you get it.
-If you dont need it , you just return it and stop paying for it immediately.

Undifferentiated Heavy Lifting Of IT :-

For eg.

-Your company works with a MYSQL server.
-Now setting up a MYSQL engine on a server is not what differentiates you from your competitors , but how  efficiently you store your data in your database does.

-In AWS this is called as undifferentiated heavy lifting of IT.
-Tasks that are common , often repetitive and ultimately time consuming.
-These are the tasks that aws wants to help you with m so you can focus more on what makes you unique.

Module 1 Assessment Question Notes:-

Question 1 :-

"Deploying applications connected to on-premises infrastructure is a sample use case for a hybrid cloud deployment."

Q.What does the above statement mean?

What is a hybrid cloud deployment?
-A Hybrid cloud is a computing environment that combines :-
1)On-premises infrasturcture(servers/data centers physically located at your site).
2)with Public cloud Services(like AWS,AZURE,GCP)
3)and allows Data and applications to be shared or integrated between your on-premises systems and the cloud systems.

What does the statement say?
-The statement defines a use case for hybrid deployment.
-It means a application is deployed on cloud platform still needs to connect with your existing on-premises systems such as :-
a)Database hosted in your companies data center.
b)Internal API's 
c)Legacy Systems:-
-an old or outdated software platform that a company uses to manage its core business processes, such as:
1)Inventory management
2)Finance and accounting
3)Human resources
4)Order processing
5)Supply chain management
d)Authentication Systems

Example Use Case:-

Let’s say a company has a legacy ERP system in their on-premises data center. Now they build a modern web-based dashboard using React and Node.js and host it on AWS.

This dashboard pulls real-time data from the on-prem ERP system using secure connections (like VPN or Direct Connect). This is a hybrid cloud use case, because:

1)The application is deployed in the cloud,
2)But it is connected to on-premises infrastructure for data.


Q."Running code without needing to manage or provision servers"

What does the above statement mean?

-The above statement refers to a cloud computing model called serverless computing.
-Traditionally when you want to run a code , you have to :-
1)Setup A Server (Physical Or Virtual Unit).
2)Install Software (OS,runtime etc).
3)Manage Updates,Security And Scaling
4)Keep it running 24/7.

-But in serverless computing, you don’t have to worry about any of that. You just:
-Write your code and upload it — the cloud provider runs it for you automatically only when needed.

Who manages the servers then?
-The Cloud Provider(like AWS,GCP or Azure) takes care of :
1)Server Provisioning(starting it up)
2)Infrastructure Management
3)Scaling(Handling more users automatically)
4)Maintainance and security

-You never interact directly with a server.
-AWS Lambda is an AWS service that lets you run code without needing to manage or provision servers.
The AWS Cloud offers three cloud deployment models: cloud, hybrid, and on-premises. 

Question 3 :-

"The aggregated cloud usage from a large number of customers results in lower pay-as-you-go prices."

Q.What does the above statement mean?

Aggregated Cloud Usage :-

-When millions of people and businesses use cloud services ,all their usage adds up (aggregates) into a huge demand for computing , storage and other resources.
-Because of this massive scale cloud providers like (aws,auzre or gcp) can reduce prices for everyone - especially in pay-as-you-go plans where you pay for what you use.

Additional Points To Q3.-
-Not having to invest in technology resources before using them relates to Trade upfront expense for variable expense.
-Accessing services on-demand to prevent excess or limited capacity relates to Stop guessing capacity.
-Quickly deploying applications to customers and providing them with low latency relates to Go global in minutes.


Module 2 :- Compute In The Cloud

The EC2(Elastic Cloud Compute) Instance:-

-Businesses require servers to power their business and applications.
-Businesses require raw compute power to host applications and other business needs.
-The needs for compute power maybe a use case like youtube which streams video content to millions of users.
-In AWS these servers are virtual,and these virtual servers are called as EC2.

-If you go the hard way to get onprem servers, you have to research for which server you want to buy ,it would take weeks to get those servers and you are stuck with those servers when you get them either you use it or not.
-With EC2 , this becomes easier.
-When you need a EC2 instance , you request and get it.If you dont need it,you terminate it.
-In AWS, you only pay for what you use.You only pay for a running EC2 instance and not a stopped or terminated instance.

-EC2 runs on top of physical host machines managed by AWS using virtualization technology.
-When you spin up a EC2 instance,you arent necessarily taking an entire host to yourself.
-Instead , you are sharing the host with multiple other instances, otherwise known as virtual machines.
-A hyperwisor running on the host machine is responsible for sharing the underlying physical resources between the EC2 instances.
-This idea of sharing underlying hardware between multiple virtual machines is called as multitenancy.
-The "hypervisor" is responsible for coordinating this multitenancy and it is managed by aws.
-The hypervisor is reponsible to isolate the virtual machines from each other as they share resources from the host.
-This means EC2 instances are secure.
-Once EC2 instance is not aware of any other instances on the same host sharing the same underlying resources.

-When you provision(start it up) an EC2 instance , you can choose the operating system based on either windows or linux.
-You have this power to configure your own EC2 instance.
-Beyond OS,you also configure what software you want running on your EC2 instance.
-Whether it is a web app,databases , third party software , you have complete control iover what happens in that instance.
-EC2 instances are also resizable, you may start with a small instance , if that instance is being maxed up , you can give more memory and CPU to that instance.
-This process is called as vertically scaling an instance.

-You also control the networking aspect of EC2.
-What type of requests make it to your server and if they are publically or privately accessible is what you decide.

-With EC2, AWS has made it very easy to acquire servers with this Compute-as-a-service model.

Episode 2 :- Amazon EC2 Instance Types

-From the example of coffee shop , the employees are the ec2 instances which server the client requests.
-When the owner hires employees for the coffee shop , he doesnt only hire cashiers.
-He hires waiters,chef's , baristas etc.
-There are tons of roles and responsibilities which employees should have to make a business efficient.
-Its important to look that the employee's skill set suits their role.
-There are different types of ec2 instances that you can spin up and deploy into an aws environment.
-Each Amazon EC2 instance type is grouped under an instance family and is optimized for certain types of tasks.
-Instances with different types of CPU's , Memory , networking capabilities give you the resources you want suitable for your business.

Below are the different Amazon EC2 instance families :-
1)General Purpose:-
-Provide a good balance of COmpute,Memory and networking resources.
-Can be used for general purposes like web servers or copde repositories.
2)Compute Optimized:-
-Optimized for Compute Intensive tasks like gaming servers,High Performance Computing or scientific modelling.
3)Memory optimized:-
-Good for memory intensive tasks
4)Accelerated Computing:-
-Good for floating point number calculations,graphics processing or data pattern matching.
5)Storage Optimized:-
-A good for workloads which require high performance for locally stored data.

Module 2 - Episode 2 - Test Your Knowledge :-

Q1.Which Amazon EC2 Instance Is Suitable For Data Warehousing applications?

-Storage Optimized.
-This is because it is optimized for high local storage throughput.
-Local storage refers to physical storage devices (usually NVMe SSDs) that are directly attached to the host machine running your EC2 instance.
-And optimized for Disk I/O.

Q2.Which Amazon EC2 instance is well suited for High Performance Databases?

-Memory Optimized.
-This is because memory optimized instances provides 100s of GB's or TB's of RAM.
-This allows entire databases to be loaded on to the memory.
-Faster read and write speen due to RAM than disk.
-Better performance for in-memory databases like redis.


Episode 3 :- Amazon EC2 Pricing

-Amazon EC2 purchase options are as follows :-
1)On-demand:-
-Pay on hourly basis or seconds basis depends on what instance and OS you choose.
-Best to test workloads and playaorund when getting started up.

2)Savings Plan:-
-This is like a commitment of consistent usage for 1 or 3 years paid on hourly basis.
-Can save upto 72%.
-Any usage up to the commitment is charged at the discounted Savings Plans rate (for example, $10 per hour). Any usage beyond the commitment is charged at regular On-Demand rates.
-The savings with EC2 Instance Savings Plans are similar to the savings provided by Standard Reserved Instances.
-Unlike Reserved Instances, however, you don't need to specify up front what EC2 instance type and size (for example, m5.xlarge), OS, and tenancy to get a discount. Further, you don't need to commit to a certain number of EC2 instances over a 1-year or 3-year term.

3)Reserved Instances:-
-Often used for steady state workloads.
-This also involves 1 or 3 year term and can pay them using 3 payment options:-
a)All upfront :-All in once payment
b)Partial Upfronmt :-Some partial payment at first
c)No Upfront :-No payment at first.

Reserved Instances require you to state the following qualifications:

i)Instance Type and Size :- for eg. m5.xlarge
ii)Platform Description(OS) :- For eg. Microsoft Windows Server , or  red hat linux enterprise.
iii)Tenancy :-Default or dedicated Tenancy.
-Types of Tenancy :-
1)Default Tenancy :-Your instance runs on shared hardware.Aws hosts your instances on physical servers that are shared with other customers.
2)Reserved Tenancy :-Instance runs on hardware dedicated to your account.But you dont manage the hardware.
-AWS places the instances on dedicated servers,you dont have access to set where they go.
3)Dedicated Host Tenancy :-Gives you full visibility and control over a physical server.
-You can see and control instance placement on that host.

iv)Region :-An instance from which region for eg. Asia(mumbai)
-Types of Reserved Instances :-
i)Standard Reserved Instances:-
-This option is a good fit if you know the EC2 instance type and size you need for your steady-state applications and in which AWS Region you plan to run them.
ii)Convertible Reserved Instances:-
-If you need to run your EC2 instances in different Availability Zones or different instance types, then Convertible Reserved Instances might be right for you.

-At the end of a Reserved Instance term, you can continue using the Amazon EC2 instance without interruption. However, you are charged On-Demand rates until you do one of the following:
i)Terminate the instance.
ii)Purchase a new Reserved Instance that matches the instance attributes (instance family and size, Region, platform, and tenancy).

4)Spot Instances :-
-This allows you to request the spare Amaon EC2 computing capacity for upto 90% off 
the on demand price.
-The catch here is aws can reclaim the instance anytime they need it giving you a 2 minute warning to save up work and save state.
-This is beneficial when your work allows you to be interrupted.
-A good usecase is batch workloads.
-Note:-Spot instances can only be launched if spare capacity is available,other wise it will delay your job.
5)Dedicated Hosts:-
-Which are physical hosts dedicated for your use for EC2.
-Nobody else will share a tenancy of that host.

Episode 4 :- Scaling Amazon EC2

-Another major benefit of AWS is Scalability and Elasticity.
-In on-prem data centers , there are hardwares required.
-The overall consumption of your hardwares can be 10% at non peak hours or 80% at peak hours.
-How do you decide how much resources to buy?
-If you buy resources for average usage lets say which can handle upto 50% customers then they will struggle when there are 80% customers hike which is the key factor for your result.
-If you buy resources for maximum capacity , your customers might be happy but most of the times the overall consumtion of resources might be only 10% and other all will be idle.

How does AWS solves this problem?
-Scalability involves beginning with only the resources you need and designing your architecture to automatically respond to changing demand by scaling out or in. As a result, you pay for only the resources you use. You don’t have to worry about a lack of computing capacity to meet your customers’ needs.
-The AWS service that provides this functionality for Amazon EC2 instances is Amazon EC2 Auto Scaling.
-AWS EC2 Auto Scaling enables you to automatically add or remove Ec2 instances in response to changing application demand.
-Within Amazon EC2 Auto Scaling, you can use two approaches: dynamic scaling and predictive scaling:-
1)Dynamic scaling responds to changing demand. 
2)Predictive scaling automatically schedules the right number of Amazon EC2 instances based on predicted demand.

-Suppose that you are preparing for launch of your application on  EC2 instances.
-When configuring the size of auto-scaling group,you might set the minimum number of EC2 instances at first.
-This means there will be atleast the minimum number of EC2 instances running at all times.

Customizing The AutoScaling Group:-
https://replit.com/@rajdubal87/AWS-Notes#AutoScaling.png

1)Minimum Capacity:-
-Minimum capacity is the minimum number of instances that will immediately launched after you have created a auto-scaling group.

2)Desired Capacity:-
-Next, you can set the desired capacity at two Amazon EC2 instances even though your application needs a minimum of a single Amazon EC2 instance to run.
-Note:-If you do not setup desired capacity,then the desired capacity defaults to your minimum capacity.

3)Maximum Capacity:-
-The third configuration that you can set in an Auto Scaling group is the maximum capacity. For example, you might configure the Auto Scaling group to scale out in response to increased demand, but only to a maximum of four Amazon EC2 instances.
-Because Amazon EC2 Auto Scaling uses Amazon EC2 instances, you pay for only the instances you use, when you use them. You now have a cost-effective architecture that provides the best customer experience while reducing expenses.


Episode 5 :- Directing Traffic With Elastic Load Balancing

-When you have multiple EC2 instances,all server the  same program , does the same job ,
-And if a request comes,how does that request know which EC2 instance to go to?
-How do we ensure there is even distribution of work loads accross multiple EC2 instances.
-You need a way to route requests to instances , to process those requests.
-The solution to this problem is load balancing.
-A load balancer takes in requests and routes them to servers in order to get them processed.


Elastic Load Balancing(ELB) :-

-It is engineerd to address the undifferentiated heavy lifting of load balancing.
-Elastic Load Balancing is a regional construct.
-This service available on your region than on a entire instance.
-ELB is automatically scalable.
-As your traffic grows , it is designed to handle the throughput with no change in the hourly cost.
-When the minimum or desired capacities are maxed out ,the auto scaled out instances make the ELB know that they are ready to handle requests.
-Once the fleet scales in, ELB first stops all new traffic and waits for the current processing requests to get processed completely.
-Once they do that , the autoscaling engine can terminate the instances without disruption to existing customers.
-ELB is not only used for external traffic(Internet-facing traffic like users visiting your website).
-Because ELB is regional ,its a single URL that each frontend instance uses.
-All frontend instances (even across AZs) can use this one URL to reach the backend.
-Then the ELB sends the traffic to the backend that has least outstanding requests.
-When the backend scales , the new instance tells the ELB that it is ready to handle requests and ELB handles it.
-The frontend doesnt have to care how many backend instances are there.
-This is true decoupled architechture.

More About Load Balancer :-

-A load balancer acts as single point of contact for all incoming traffic to your auto scaling group.
-This means that as you add or remove EC2 instances in response to change in demand, this requests route to the lead balancer first.
-Then, the requests spread across multiple resources that will handle them. For example, if you have multiple Amazon EC2 instances, Elastic Load Balancing distributes the workload across the multiple instances so that no single instance has to carry the bulk of it. 


Episode 6 - Messaging and queuing

-In the example of a coffee shop , the cashier and barista are in sync as the cashier gets the order and passes it to barista to prepare it.
-What if the barista is out on a break , the entire process could be delayed or postponed.
-To solve this , the coffee shop can introduce a order board or a buffer.
-This type of idea of using a buffer is known as messaging and queuing.
-Just like cashier sends messages to the barista, applications send messages to communicate.
-If applications communicate directly with each other , just like cashier and barista, then this is called tightly coupled architecture.
-But, in this architecture, if a single component fails , it can create mess for other components working.
-If Application A communicates with Application B regularly, and if the Application B fails , then it can create errors for application as well.
-A more reliable architecture is loosely coupled , single failure cannot cause cascading failures.
-In this architecture , if one component fails, it is isolated and wont cause problems for other components.
-In this , we introduce a buffer/message queue in between.
-App A sends a message to the message queue , message queue sends this message to APp B.
-If for some reason App B is failed , the messages from App A remains in the message queue until the App B is up and running to take requests.

This service is implemented by amazon having name as :-
-Amazon Simple Queue Service (SQS)
-Amazon Simple Notification Service (SNS)

Amazon Simple Queue Service(SQS) :-

-Amazon SQS can Send,Store,Receive messaged between software components at any volumne.
-THis is without loosing messages or without needing support of any other services.
-The data contained within a message is called a payload and is protected until delivery.
-SQS queues is the place where messaged are stored before they are processed.
-AWS manages the underlying infrastucture to host those queues.
-These scale automatically,are reliable and easy to configure and use.


Amazon Simple Notification Service(SNS):-

-Amazon SNS is similar , it is used to send out messages to services,but it can also send out notifications to end users.
-It does it in a different way, in  a publish-subscribe or Pub-Sub model.
-This means you can create something called as SNS topic which is just a channel for messages to be delivered.
-You then configure subscribers to that topic and finally publish those messages for those subscribers.
-With just one go you can send a message in the channel and it will go to all its subscribers.
-These subscribers can also be end points.Such as SQS queues , AWS lambda functions or HTTP/HTTPS web hooks.
Imagine an e-commerce app:

-Order Placed ➜ Message published to SNS.
-Subscribers:
1)Lambda for billing
2)SQS queue for shipping service
3)SQS queue for analytics service

Each component receives the same order event, but processes it independently.
-SNS can also be used to push messages to end users using mobile push,sms and email.
-In Amazon SNS, subscribers can be web servers, email addresses, AWS Lambda functions, or several other options. 

Monolithic Approach :-
https://replit.com/@rajdubal87/AWS-Notes#Monolithic.png

-Applications are made of multiple components. The components communicate with each other to transmit data, fulfill requests, and keep the application running. 

-Suppose that you have an application with tightly coupled components. These components might include databases, servers, the user interface, business logic, and so on. This type of architecture can be considered a monolithic application. 

-In this approach to application architecture, if a single component fails, other components fail, and possibly the entire application fails.

-Here every componenet is packaged together and is typically deployed on the same server.

***To help maintain application availability when a single component fails, you can design your application through a microservices approach.

Microservices approach :-

https://replit.com/@rajdubal87/AWS-Notes#Microservices.png

-In a microservices approach, application components are loosely coupled. In this case, if a single component fails, the other components continue to work because they are communicating with each other. The loose coupling prevents the entire application from failing. 

-When designing applications on AWS, you can take a microservices approach with services and components that fulfill different functions. Two services facilitate application integration: Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS).


Publishing updates from Single Topic(using SNS):-
https://replit.com/@rajdubal87/AWS-Notes#SNS%20Single%20Topic.png

-A coffee shop can have a single newsletter which includes topics such as coffee trivia,coupons,new products.
-All customers who subscribe to the newsletter receive updates about coupons, coffee trivia, and new products.

Publishing updates from multiple topics(using SNS):-

-Suppose now the clients wants a dedicated service for coupons,trivia and new products.
-Now, instead of having a single newsletter for all topics, the coffee shop has broken it up into three separate newsletters. Each newsletter is devoted to a specific topic: coupons, coffee trivia, and new products.
-Subscribers will now receive updates immediately for only the specific topics to which they have subscribed.
-It is possible for subscribers to susbcribe to a single topic or multiple topics.


More About Amazon Simple Queue Service(SQS):-

-Using AWS SQS you can send,store and receive messages between software components, without loosing messages or requiring other services to be available.
-In Amazon SQS, an application sends messages into a queue.
-A user or service retrieves a message from the queue, processes it, and then deletes it from the queue.


**Is a Decoupled Architecture,Loosely Coupled Architecture and Microservices the same?

-Decoupled application components means a loosely coupled architecture, and microservices are one form of that architecture.
-SO:-
1)All microservices are decoupled,
2)But not all decoupled architectures are microservices.

Example:
In a monolithic app, your email module can be decoupled from the order module using events or interfaces — even though it's all one codebase.

Episode 7 :- Additional Compute Services

-EC2 requires you to setup and manage your fleet of instances over time.
-When you are using EC2 you are responsible for patching your instances when new packages come out.
-Setting up the scaling of those instances as well as ensuring that you've architected your solutions to be hosted in highly available manner.
-THis is just not as much management if you hsoted these on premises.

What are some computing services that aws offer that are more convineint from management perspective?

-This is where the term serverless comes in.
-Serverless means you cannot actually see or manage the underlying infrastructure or instances that are hosting your application.
-Instead,all the management of the underlying environment from a provisioning , scaling , high availabilty and management perspective are taken care for you.
-All you have to do is focus on your application and everything else is taken care of.

AWS Lambda :-

-AWS lambda is one aws serverless compute option
-Lambda allows you to upload your code into a lambda function
-COnfigure a trigger , and from there the service waits for the trigger , when the trigger is tiggered the code is run in a managed environment.
-It is automatically scalable , highly available and the maintainance of it is done by aws itself.
-If you have 1 or 1000 triggers,lambda will scale your function to meet demand.
-Lambda is designed to run code under 15 minutes.
-THis isnt long running processes like deep learning , its more suited for quick processing like a web backend , handling requests etc.

-If we are not ready for serverless yet but still want efficiency and protability , you should look at aws container services like Amazon elastic container service(ECS) or amazon elastic kubernetes service(EKS).
-both of these services are container orchestration tools.
-A container in this case is a docker container.
-Docker is a widely used platform , that uses operating system level virtualization to deliver software in containers.
-A container is a package for your code.
-Its where you pakcge your application , its dependencies as well as configurations that you need to run.
-These containers run on top of EC2 instances and run in isolation from each other similar to how virtual machines work.
-But in this case the host is an EC2 instance.
-When you use docker containers on aws , you need processes to stop ,start , restart and monitor containers running accross not just one EC2 instance but a number of them together which is called a cluster.
-The process of doing these tasks is called container orchestration.
-Orchestration tools are created to help you manage your conatiners.
-ECS(elastic container service) is designed to help you run your containerized applications at scale without the hassle of managing your own container orchestration tool.
-EKS does a similar thing but uses different tooling and with different features.
-Both Elastic container service and Elastic kubernetes service can run on top of EC2.
-But if you dont want to think about EC2s to host your containers because you dont need access to underlying OS or you dont want to manage those EC2 instances , you can use a aws platform called AWS Fargate.
-Fargate is a serverless compute platform for ECS or EKS.


The difference between Traditional , serverless and containerzied severless :-

Traditional :-
-If you are trying to host your traditional operations and want full access to underlying operating system , you are going to want a EC2.

Serverless:-
-If you are looking to host short running functions , service oriented or event driven applications
-and you dont want to manage the underlying environment at all , you shoould look into serverless compute model called aws lambda.

Containerized Serverless :-
-If you are looking to run docker container based workloads on aws , you first need to choose your orchestration tool.
-Do you want to use amazon ecs or eks.
-Then you need to select your platform.
-Do you want to host your containers on EC2 that you manage or in a serverless environment called AWS fargate.

More about serverless computing :-

https://replit.com/@rajdubal87/AWS-Notes#ServerlessComputing.png

AWS Lambda Pricing :-

While using AWS Lambda, you pay only for the compute time that you consume. Charges apply only when your code is running. You can also run code for virtually any type of application or backend service, all with zero administration. 

For example, a simple Lambda function might involve automatically resizing uploaded images to the AWS Cloud. In this case, the function triggers when uploading a new image. 

Explaination to above example :-
In AWS Lambda, "set your code to trigger from an event source" means that you configure Lambda to automatically run your function in response to specific events from other AWS services or external systems.

An event source is the place or service where an action (event) happens, and when that event occurs, it triggers your Lambda function.


More About Containers :-

-Containers provide you with a standard way to package your application's code and dependencies into a single object. 
-You can also use containers for processes and workflows in which there are essential requirements for security, reliability, and scalability.


One host with multiple containers :-
https://replit.com/@rajdubal87/AWS-Notes#1hostwithmultiplecontainers.png

-Suppose that a company’s application developer has an environment on their computer that is different from the environment on the computers used by the IT operations staff. The developer wants to ensure that the application’s environment remains consistent regardless of deployment, so they use a containerized approach. This helps to reduce time spent debugging applications and diagnosing differences in computing environments.

The Host Can Be:-
1)An EC2 instance (virtual server in AWS)
2)A dedicated host (a physical server allocated entirely to you)
3)Your own on-premise machine or VM

Ten hosts with hundreds of containers :-
https://replit.com/@rajdubal87/AWS-Notes#10Hosts100Containers.png

When running containerized applications, it’s important to consider scalability. Suppose that instead of a single host with multiple containers, you have to manage tens of hosts with hundreds of containers. Alternatively, you have to manage possibly hundreds of hosts with thousands of containers. At a large scale, imagine how much time it might take for you to monitor memory usage, security, logging, and so on.


Amazon Elastic Container Service (Amazon ECS) :-

Amazon Elastic Container Service (Amazon ECS)(opens in a new tab) is a highly scalable, high-performance container management system that enables you to run and scale containerized applications on AWS. 

Amazon ECS supports Docker containers. Docker(opens in a new tab) is a software platform that enables you to build, test, and deploy applications quickly. AWS supports the use of open-source Docker Community Edition and subscription-based Docker Enterprise Edition. With Amazon ECS, you can use API calls to launch and stop Docker-enabled applications.

Amazon Elastic Kubernetes Service (Amazon EKS):-

Amazon Elastic Kubernetes Service (Amazon EKS)(opens in a new tab) is a fully managed service that you can use to run Kubernetes on AWS. 

Kubernetes(opens in a new tab) is open-source software that enables you to deploy and manage containerized applications at scale. A large community of volunteers maintains Kubernetes, and AWS actively works together with the Kubernetes community. As new features and functionalities release for Kubernetes applications, you can easily apply these updates to your applications managed by Amazon EKS.

AWS Fargate

AWS Fargate(opens in a new tab) is a serverless compute engine for containers. It works with both Amazon ECS and Amazon EKS. 

When using AWS Fargate, you do not need to provision or manage servers. AWS Fargate manages your server infrastructure for you. You can focus more on innovating and developing your applications, and you pay only for the resources that are required to run your containers.

What Does Scaling your instance vertically or horizontally mean?

Scaling Vertically :-

-Resizing your instance with more cpu,memory , storage etc.

Scaling Horizontally :-

-Adding more instances of same size.

Module 2 Quiz :-

Q1.You want to use an Amazon EC2 instance for a batch processing workload. What would be the best Amazon EC2 instance type to use?

Answer :-

Compute Optimized.

Why?

For batch processing, the best EC2 instance type is usually from the Compute Optimized family (like C7g, C6a, C5, etc.) because batch jobs are typically CPU-intensive and time-sensitive, and compute optimized instances give you maximum processing power per dollar.

Batch jobs usually crunch a lot of data and run heavy computations

Why not other options?

-General purpose instances provide a balance of compute, memory, and networking resources. This instance family would not be the best choice for the application in this scenario. Compute optimized instances are more well suited for batch processing workloads than general purpose instances.

-Memory optimized instances are more ideal for workloads that process large datasets in memory, such as high-performance databases.

-Storage optimized instances are designed for workloads that require high, sequential read and write access to large datasets on local storage. The question does not specify the size of data that will be processed. Batch processing involves processing data in groups. A compute optimized instance is ideal for this type of workload, which would benefit from a high-performance processor.

Q.Which process is an example of Elastic Load Balancing?

-Ensuring that no single Amazon EC2 instance has to carry the full workload on its own.

-Elastic Load Balancing is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances. This helps to ensure that no single resource becomes overutilized.


Q.You want to deploy and manage containerized applications. Which service should you use?

-Amazon EKS is a fully managed Kubernetes service. Kubernetes is open-source software that enables you to deploy and manage containerized applications at scale.
-Similarly , Amazon ECS works.
-For fully non-managed containerized service is Amazon Fargate.


Module 3 : Global Infrastructure and Reliability

Episode 1:

-From out coffee shop example lets say there is a monday and we do have lot of customers coming to ouyr coffee shop to get a coffee.
-Now,lets say there is a parade going on the same road as the coffee shop.
-This will make our customers diverted from the coffee shop from getting a coffee,
-What can be a solution?
-This is not the only coffee shop at that location.
-The cafe is actually a chain and we have locations all around the city.
-This way even if there is a parade or power outage , customers can go to other shops and get their coffee.
-This way we make our money and customers get their coffee.

-This is the similar way in which the aws global infrastructure is setup.

The AWS global infrastructure :-

-It is not good that there is this single location where all the data center resources are.
-If something would happen to that data center , like a power outage or natural disaster,everyuones apps would go down all at once.
-You will need high availability and fault tolerence.
-Turns out it is also not good enough to have 2 data centers.
-AWS operates in all sorts of different areas , around the world called regions.


Episode 2 : AWS regions

-The main problem with data centers is any calamities , disasters can happen and no matter who owns the data centers rather be a onprem or aws , it will create problems.
-Most of the businessesd stop because of it.
-Businesses cannot run on the hope that disaster wont happen.

Solution By AWS:-

-AWS builds regions to be closest to where the business traffic demands.
-Locations like , dublin , ohio , mumbai etc
-Inside each region , we have multiple data centers that have all the compute, storage and other services you need to run your applications .
-Each region can be connected to other region with a  high speed fibre optic network controlled by aws.
-Each region is isolated from every other region in the sence that no data goes in and out of your environment from that region without you explicitly granting permissions to move that data out.
-This is a critical security conversation to have.

For eg.

-You might have government compliance requirements that your financial information in frankfurt cannot leave germany.
-This is how aws works , no data from one region moves to another without explicit permissions with right credentials is granted.
-Regional data sovereignty(the power that a country has to control its own government) is part of the critical design of aws regions.

You as a business owner how do you make a decision in which region you pick?

There's four business factors that go into choosing a region :-

1)Compliance :-
-You must first look at your compliance requirements.
-you have a requirement that your data must live in indian boundaries.
-Then you should choose the mumbai region.
-No other thing should come before compliance.
2)Proximity:-
-How close you are to your customer base is a major factor because speed of light , still the law of the universe.
-You having cutsomer base of india and setting up a server at us region can create latency problems.
-Laterncy is the time it takes for data to be sent and received.
-Locating close to your customer base is usually the right call.
3)Feature availability:-
-Sometimes the closest region may not have all the aws features you want.
-For eg. Your developers want to play with Amazon Bracket(AWS's new quantom computing platform) , well then they have to run in the platform that already have the hardware installed.
4)Pricing:-
-Even though the hardware is the same , some locations are more expensive than other locations to operate in.
-For eg.Brazil's tax structure is more expensive for aws to operate than compared to many other countries.
-If price is your primary concern , even if your customer base is in brazil region , you might wanna set your region as oregon,usa .


Episode 3 : Availability zones 

-AWS calls a single data center or a group of data centers an availability zone.
-EAch availability zone is one or more discrete data centers with redundant power,networking and connectivity.
-When you launch an EC2 instance, it launches an virtual server on a physical hawdware that is installed in an  availability zone.
-This means each aws region consist of multiple isolated physically separate availability zones within a geographic region.
-a region like Mumbai contains multiple AZs, which are like multiple AWS-managed, isolated data centers within that city or surrounding area. This setup enables resilient and scalable cloud architecture.
-AWS keeps each availability zone 10s of miles accross so that in the case of huge scale natural disaster , your application is still running.
-AWS advices that to run accross two availabiltiy zones in a region.
-This means redundantly deplopying your application over aws az's to keep your app ruinning everytime.
-Redundant means having duplicate systems/components in place so that if one fails, the other can take over without service interruption.
For eg. Elastic Load Balancing is a regional construct which runs on a region and not on a single EC2 or on a single AZ.
-It runs accross all availability zones communicating with the EC2 instances..
-When you want high availability , any services that is regionally scoped service is always highly available.

More about availabilty zones:-

https://replit.com/@rajdubal87/AWS-Notes#AWSAvailabilityZones.png

-An Availability Zone is a single data center or a group of data centers within a Region. Availability Zones are located tens of miles apart from each other. This is close enough to have low latency (the time between when content requested and received) between Availability Zones. However, if a disaster occurs in one part of the Region, they are distant enough to reduce the chance that multiple Availability Zones are affected.

How is using multiple availability zones in a region useful :-

https://replit.com/@rajdubal87/AWS-Notes#SingleAZ.png

https://replit.com/@rajdubal87/AWS-Notes#MultipleAZ's.png

https://replit.com/@rajdubal87/AWS-Notes#AZFailure.png


Q."A data center that an AWS service uses to perform service-specific operations" ?

-An Edge Location
-An edge location is a data center that an AWS service uses to perform service-specific operations. 

Q."A service that you can use to run AWS infrastructure within your own on-premises data center in a hybrid approach" ?

-AWS Outposts is a service that you can use to run AWS infrastructure, services, and tools in your own on-premises data center in a hybrid approach.

Q."A geographical area that contains AWS resources" ?

-A Region is a geographical area that contains AWS resources.


Episode 4 : Edge Locations 

-One of the major criteria in selecting a aws region was proximity to your customers.
-But what if you have customers all over the world or in cities which are not closer to one of aws regions?
For eg.If you have customers in mumbai , who have access to your data , but your data is hosted out of tokyo region.
-Rather than having mumbai based customers send requests to tokyo to access that data , just place a copy locally or cache a copy in mumbai.
-Caching the data closer to the customers all around the world uses the concept of content delivery networks or CDN's.
-In AWS , CDN's are called as Amazon CloudFront.

Amazon CloudFront:-

-It is a service that helps deliver data,video,applications and API's with customers around the world with low latency and high transfer speeds.
-Amazon Cloudfronts uses what are called edge locations all around the world to help accelerate communication with users , no matter where they are.
-Edge locations are separate from regions.
-So you can push content from inside a region to a collection of edge locations around a world in order to accelerate communication and content delivery.
-AWS Edge Locations , also run more than just cloudfront , they run a domain name service or DNS known as Amazon Route 53 , helping direct customers to the right web locations with reliably low latency.
-But what if your business wants to use wants to use aws services inside of their own building?

Introducing AWS Outposts:-

-Here , AWS will install fully operational mini region, right inside your data center.
-That's owned and operated by aws.
-It uses 100% of aws functionality , but isolated within your own building.


Summary :-

1)Regions are geographically isolaed areas where you can access services needed to run your enterprice.
2)Regions contain availability zones that allows you to run accross physically serparated buildings while keeping your application logically unified.
3)AWS Edge Locatons run Amazon Cloudfront to help get content closer to your customers.

More About Edge Locations :-

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocations.png

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocationsA.png

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocationsB.png

https://replit.com/@rajdubal87/AWS-Notes#EdgeLocationsC.png


Episode 5 : How to provision AWS resources

-How do you actually interact with these services?
-The answer is API's
-In AWS , everything is an API call.
-An API is a application programming interface and what this means is there are pre-determined ways for you to interact with AWS services.

Q.Why is a API called "Application Programming Interface"?

1)Application :- Application here means a software you are building or integrating with.
2)Programming :- You are writing code to interact with it.
3)Interface :- A set of exposed commands , inputs , outputs you can use - like a control pannel.

An interface (set of commands) for programmers to interact with an application or service.

Continuing with how to interact with aws services :-

-You can invoke these API's to provision , configure or manage your AWS resources.
For eg.You can launch a EC2 instance or you can create a AWS lambda function.
-Each of those will be different requests and different api calls to aws.
-You can use the aws management console , the aws command line interface(CLI) , the aws software development kits or various other tools like aws CloudFormation to create requests to send to AWS API's to create and manage aws resources.


1)AWS Management Console :-

-Through AWS console,you can manage your resources visually.
-It is a good starting point.
-This is good for building out test environments or viewing aws bills , vewing monitoring and working wit other non technical resources.
-However when your application is up and running in a production environment you dont want to rely on the point and click style that the console that your console gives you to create and manage your aws resources.
For Example : In order to create an amazon EC2 instance, you need to click through various screens , setting all the configurations you want and then you launch your instance.
-If later you wanted to launch another EC2 instance, you will need to go back into the console and click into the screens again to launch another EC2 instance.
-By having humans do  this sort of manual provisioning , you are opening yourself to potential errors.
-It is easy to forget to check a checkbox or mis-spell something while doing the entire thing manually.

2)AWS CLI :-

The answer to this problem is to use tools that allow you to script or program the API calls.
-One tool you can use is the AWS CLI.
-The CLI allows you to make API calls using the terminal on your machine.
-It makes the complete scene prone to errors if you have pre-written scripts to setup a instance.
-You can have this scripts run automatically like on a schedule or triggered by another process.
-Automation is  very important to having a successfull and predictible cloud deployment over time.

3)AWS SDK :-

-Another way to interact with AWS services is AWS SDK.
-The software development kits allows you to interact with aws resources through various programming languages.
-This helps developers develop programs that use aws without using the low-level api's ,as well as avoiding the manual resource creation.
-Here low level API's mean The raw HTTP requests you would have to send directly to AWS services, using their REST APIs, without any helper tools.
-These APIs are the under-the-hood web interfaces that AWS exposes — and they’re powerful, but very complex and verbose.

 What the AWS SDK Does Instead:

The AWS SDK (Software Development Kit) gives you ready-made libraries in popular languages like:
1)Python (boto3)
2)JavaScript/Node.js (aws-sdk)
3)Java, Go, C#, etc.

You just write few lines of code:-

import boto3

s3 = boto3.client('s3')
s3.create_bucket(Bucket='my-bucket')


More About AWS Management Console :-

The AWS Management Console is a web-based interface for accessing and managing AWS services. You can quickly access recently used services and search for other services by name, keyword, or acronym. The console includes wizards and automated workflows that can simplify the process of completing tasks.

You can also use the AWS Console mobile application to perform tasks such as monitoring resources, viewing alarms, and accessing billing information. Multiple identities can stay logged into the AWS Console mobile app at the same time.

More About AWS Command Line Interface (CLI) :-

To save time when making API requests, you can use the AWS Command Line Interface (AWS CLI). AWS CLI enables you to control multiple AWS services directly from the command line within one tool. AWS CLI is available for users on Windows, macOS, and Linux. 

By using AWS CLI, you can automate the actions that your services and applications perform through scripts. For example, you can use commands to launch an Amazon EC2 instance, connect an Amazon EC2 instance to a specific Auto Scaling group, and more.


More About AWS Software Development Kits (SDK) :-

Another option for accessing and managing AWS services is the software development kits (SDKs). SDKs make it easier for you to use AWS services through an API designed for your programming language or platform. SDKs enable you to use AWS services with your existing applications or create entirely new applications that will run on AWS.

To help you get started with using SDKs, AWS provides documentation and sample code for each supported programming language. Supported programming languages include C++, Java, .NET, and more.


Episode 6: How to provision AWS resources Part 2

There are 3 ways to interact with AWS resources :-

1)AWS management console
2)AWS CLI
3)AWS SDK

-WHich are "do it your own way" interacting options.

To provision a resource :-
You will have to
1)Login into your AWS management console.
2)Write commands
3)Write Programs

There are also other ways you can manage your aws environment using manage tools like 

1)AWS Elastic BeanStalk
2)AWS CloudFormation

AWS Elastic BeanStalk :-
-It is a service that helps you provision Amazon EC2-based environments.
-Instead of clicking around the console or writing multiple commands to build out your network , EC2 instances , scaling and elastic load balancers
-You can instead provide your application code and desired configurations to the aws elastic beanstalk service.
-Which takes that information and builds out your environment for you.
-AWS elastic beanstalk also makes it easy to save environment configurations , so they can be deployed again easily.
-AWS BeanStalk gives you the convinience of not having to provision and manage all of those pieces separately, while still giving you the visibility and control of the underlying resources.
-You get to focus on your business application and not the infrastructure.

AWS CloudFormation :-

-Another service that lets you do automated and repeated deployments is AWS CloudFormation.
-AWS Cloud Formation is a infrastructure as a code tool used to define a wide variety of AWS resources in a declarative way using json or YAML text-based documents called CloudFormation templates.
-A declarative format like this allows you to define what you want to build , without specifying the details of exactly how to build it.
-CloudFormation lets you define what you want and the cloud formation  engine will worry about the details on calling the API's to get everything built out.
-It isnt just limited to EC2 based solutions.
-CloudFormation doesn't only manage resources that you directly launch on EC2.
-Some AWS Services don’t require EC2 instances at all:
1)S3
2)RDS(MySQL)
3)Lambda
4)Athena etc.
-Cloudformation supports many different aws resources from storage , databases , analytics , machine learning and more.
-Once you define your resources in a CloudFormation Template  , cloudformation will parse the template and begin provisioning all the resources you defined in parallel.
-AWS Cloudformation manages all the calls to the backend aws api's for you.
https://replit.com/@rajdubal87/AWS-Notes#AWSCloudFormation.png

-You can run the same cloudformation template with multiple accounts or multiple regions and it will create identical environments accross them.
-There is less room for human error as it is a totally automated process.


More About Elastic BeanStalk :-

With AWS Elastic Beanstalk, you provide code and configuration settings, and Elastic Beanstalk deploys the resources necessary to perform the following tasks:
1)Adjust capacity :- Here capcity means raw compute power
2)Load balancing
3)Automatic scaling
4)Application health monitoring


More About AWS CloudFormation :-

AWS CloudFormation provisions your resources in a safe, repeatable manner, enabling you to frequently build your infrastructure and applications without having to perform manual actions. It determines the right operations to perform when managing your stack and rolls back changes automatically if it detects errors.


Q. "Ability to assign custom permissions to different users"?

-Assigning custom permissions to different users is a feature that is possible in all AWS Regions.
-It is a regional construct.

Q."A service that enables you to run infrastructure in a hybrid cloud approach"?

-AWS Outposts
-AWS Outposts is a service that enables you to run infrastructure in a hybrid cloud approach.

Q."A serverless compute engine for containers"?

-AWS Fargate
-AWS Fargate is a serverless compute engine for containers.

Note :-

Amazon ECS and Amazon EKS are services used to manage container orchestration at scale.
-Orchestration here mean Automating the Management of Containers.

ECS has following options :-

Launch Type?	   Who Manages the Infrastructure?	 Use Case?
ECS on EC2	     You do (manage EC2 instances)	   More control, but more setup
ECS on Fargate	 AWS does (serverless)	           Less work, easy scaling


Q."What is an Origin Server"?

-An origin is the server from which CloudFront gets your files. 
-Examples of CloudFront origins include Amazon Simple Storage Service (Amazon S3) buckets and web servers.


Module 4 : Networking

Episode 1:

-From our coffee shop , what if there are eager customers who want to give their orders directly to the barista than going to a Cashier?
-It is not good to allow all customers to directly interact with baristas since they are already busy in brewing some best coffee's.


Amazon Virtual Private Cloud(VPC) :-

-A VPC lets you provision(setting up) a logically isolated section of the aws cloud where you can launch aws resources in a virtual network that you define.
-These resources can be public facing so they have access to the internet or private with no internet access usually for backend services like databases or application servers.
-When AWS says "private resources have no internet access", it means:
1)They cannot directly connect to or be reached from the public internet (no public IP, no internet gateway).
2)But they can still function perfectly within your private network (VPC).
3)They Serve Internal Traffic Only.

-The public and private grouping of resources are known as subnets.
-and they are ranges of IP addresses in your vpc.
-A subnet (short for sub-network) is a smaller range of IP addresses carved out from your VPC's overall IP space.
-If your VPC is a neighborhood, then subnets are the blocks (streets) inside it, and each IP is like a house address.
 "Range of IP Addresses" Means:
 -When you create a VPC , you get to define a CIDR block(Class-less inter domain routing block) , this defines the total range of IP addresses your virtual network can use.

 Example:

 VPC CIDR block: 10.0.0.0/16

This gives you:

IPs from 10.0.0.0 to 10.0.255.255

That’s 65,536 IP addresses!

You divide the VPC into subnets like:
1)Public Subnet: 10.0.0.0/24 → 256 IPs (used for internet-facing resources like load balancers)
2)Private Subnet: 10.0.1.0/24 → 256 IPs (used for backends, databases, etc.)

Each subnet has its own range of IPs assigned from the VPC’s total pool.


Now In Our Coffee Shop :-

-We have different employee's and 1 is a cashier.
-They take customer's orders and thus we want customers to interact with them, so we put them in what we call a public subnet.
-Hence they can talk to the customers or the internet.
-But for baristas , we want them to focus on making coffee and not interact with customers direclty.
-So we put them in a private subnet.


Episode 2 : Connectivity to AWS 

-VPC or Virtual Private Cloud is your own private network in aws.
-A networking service that you can use to establish boundaries around your aws resources is Amazon Virtual Private Cloud (VPC).
-A VPC allows you to define your private IP range for your aws resources.
-You place things like your EC2 instances , your ELB's inside of your VPC.
-You place these resources into different subnets.
-Subnets are chunks of IP addresses in your VPC that allow you to group resources together.
-Subnets control whether resources are either publicly or privately available.
-For some VPC's you might have internet facing resources that the public should be able to reach like a public website for example.
-However in other scenarios you might have resources that you only want to be reachable if someone is logged into your private network.
-This might be internal services like a backend database.

-In order to allow public traffic flow into and out of your VPC , you must attach an Internet Gateway or IGW to your VPC.
-An internet gateway is like a doorway that is open to public.
-Think of it like a coffee shop , without a doorway , customers cant get in our coffee shop.
-So we install a front door for people to get in and out of our coffee shop.
-The front door here is like a gateway.
-Without it , no one can get into your resources placed inside of your VPC.


VPC With All Internal Private Resources :-

-WE dont want anyone from anywhere to reach these resources.
-So we dont want a internet gateway attached to our VPC.
-Instead , we want a private gateway that will only allow traffic if it is coming form a approved network.
-This private doorway is called a virtual private gateway.
-And it allows you to create a VPN connection between a private network , like your on premises data center or your internal corporate network to your vpc.
-Taking it back to the coffee shop , this would be like a private bus route going from my building to the coffee shop.
-If I want to get coffee , i must first badge into the building thus authenticating my identity and then I can take the internal bus route to the internal coffee shop that only people from my building can use.
-So if you want to establish a encrypted VPN connection to your private internal aws resources you will need to attach a virtual private gateway to your vpc.
-Now the problem with our super secret bus route is that it still uses the open road.
-It is succeptible to traffic jams,parades caused by the rest of the world.
-THe same thing is true for VPN connections.
-They are private and they are encrypted ,they share the same bandwidth that is shared by many people that use the internet.

-To solve this problem , we need a magic doorway which leads directly to the coffee shop from home.
-Like a door in our home to coffee shop.
-The point here is you want a private connection , and you want it dedicated and it should not be shared with anyone else.
-You want the lowest amount of latency possible with highest amount of security possible .
-With aws you can achieve that with what is called AWS Direct Connect.


AWS Direct Connect :-

-Direct Connect allows you to establish a completely private , dedicated fiber connection from your data center to aws.
-You work with a direct connect partner in your aread to establish this connection.
-AWS direct connect provides a physical line that connects your network to your aws VPC.
-Its also important to note that one vpc might have multiple types of gateways attached for multiple types of resources all residing in the same vpc just in different subnets.

More About VPC :-

-Amazon VPC enables you to provision an isolated section of the AWS Cloud. In this isolated section, you can launch resources in a virtual network that you define. Within a virtual private cloud (VPC), you can organize your resources into subnets. A subnet is a section of a VPC that can contain resources such as Amazon EC2 instances.

VPC is a regional construct:
A Virtual Private Cloud (VPC) spans an entire AWS region and provides the networking framework for your resources. It includes routing tables, internet gateways, NAT gateways, and other networking components.

Subnets within a VPC:

Subnets are Availability Zone (AZ)-specific, meaning each subnet resides in a single AZ.
Subnets can be classified as public (accessible from the internet) or private (isolated from direct internet access).
A VPC can have multiple subnets distributed across different AZs within the region.

ELB works inside a VPC:

An Elastic Load Balancer (ELB) operates within a VPC and distributes traffic to targets (e.g., EC2 instances, containers, or IP addresses).
The ELB is a regional construct, meaning it spans multiple AZs and can route traffic to resources in subnets across those AZs.
For example, if your VPC has subnets in three AZs, the ELB can distribute traffic to EC2 instances in all three subnets, ensuring high availability and fault tolerance.


Episode 3 : Subnets and Network Access Control


AWS Wide amount of networking tools that covers every layer of security:-

1)Network Hardening
2)Application Security
3)User Identity
4)Authentication and Authorization
5)Distributed denial of service (DDOS) prevention.
6)Data Integrity
7)Encryption

1)Network Hardening :-

-The only technical reason to use subnets in a VPC is to control access to the gateways.
-The public subnets have access to the internet gateway and the private subnets do not.
-Subnets can also control traffic permissions.
-Packets are messaged from the internet.
-Every packet that crosses subnet boundaries gets checked against something called a network access control list or network ACL.
-This check is to see if the packet has permissions to either leave or enter the subnet based on who it was sent from and how its trying to communicate .
-You can think of Network ACLs as passport officers , if you're on the approved list ,  you get thorugh.
-If you are not on the list or if you are explicitly on the do not enter list , then you getr blocked.
-Network ACL check traffic incoming and outgoing from the subnet.
-Just because you were let in , doesnt necessarily mean they are gonna let you out.
-Approved traffic can be sent on its way and potentially harmful trafic like attempts to gain control of a system through administrative requests , they getr blocked before they ever touch the target.

But is that enough?

-A network ACL only gets to evaluate the packet which crosses a subnet boundary , in or out.
-It doesnt evaluate if a packet can reach a specific EC2 instance or not.
-Sometimes you will have multiple EC2 instances in the same subnet , but they might have different rules around who can send them messages , what port those messages are allowed to be sent to.
-SO you need instacnce level security as well.

To solve instance level access questions we introduce Security Groups :-

-Every EC2 instance when it is launched automatically comes with a security group.
-By default the security group does not allow any traffic into the EC2 instance at all.
-All ports are blocked.
-All IP addresses sending packets are blocked.
-If you actually want an EC2 instance to accept traffic from lets say a frontend message or message from the inertnet.
-So you can modify the security group to accept a specific type of traffic.
-In the case of a website , you want webn based traffic or https request to be accepted but not other traffic lets say Operating system or administration requests.
-In a ec2 instance , we can think of it a building and a doorman being the security group.
-This doorman will allow specific allowed requests in , and by default all the requests can exit without being checked.

The Key Difference between a Security Group And A Network Access Control List:-

-The Security Group is stateful.Meaning it has some kind of memory in order to who to allow in or out.If you allow an incoming request, the response is automatically allowed out — no need to explicitly allow outgoing traffic for that connection.
-On the other hand , the network ACL is stateless , which remembers nothing.Checks every packet that crosses its borders regardless of its circumstances.
-Because NACLs are stateless, they treat inbound and outbound traffic as separate events. 


More :-

-This traffic management  , it doesnt care about the contents of the packet itself.In fact it doesnt even open the envelope.It cant.All it can see is if the sender is on the approved list.

What are ephermal ports :-

-When your device sends a request to  a server , it uses :-
1)A fixed destination port(80 for http or 443 for https)
2)And a random high numbered source port (ranging from 1024–65535)

-This random port is called ephermal port.Its how your system keeps track of which request the incoming response belongs to.


Sending Packets from a subnet to another:-
https://replit.com/@rajdubal87/AWS-Notes#SendingPacketsBetweenSubnets.png

-Consider a example of subnet 1 and subnet 2 which are having EC2 instance A and EC2 instance B respectively.
-Its the same VPC but different subnets.
-Lets say ec2 instance A sends a request to EC2 instance B.
-By default,every outbound request is allowed by a security group of a ec2 instance.
-Then the request goes to a Network ACL.The network ACL checks the outgoping request if it is on the approved list or not.
-If it is approved , then the request goes to the NACL(Network Access Control List) of Subnet 2.
-Every subnet has a unique ACL with their own checklists.
-If the request is approved there,then it goes to the security group of EC2 instance B and if there it is approved , it finally reaches the instance.

Once the transaction is complete, now its the time to come home.

**-Its the return traffic pattern which is the most interesting,because this is where the stateful vs stateless nature of the different engines come into play.Because the packet still needs to be evaluated at each checkpoint.
-The security groups by default allow the return traffic out no matter what.
-In the ACL , every ingress and eggress is checked in the list.
-The ACL dont care dont know if yoyu have been in there or not.The package return address has to be on their approved list in order to cross the ACL border.
-Stateless control means it always checks its lists.
-While going back to subnet 1 , it has to go through its subnet as well.
-Here comes the main meaning of being stateful.
-Since now the request has made it back to instance A , the security group of instance A recognizes the packet from before.So it doesnt need to check of it is allowed in .


More ABout Network ACL :-

-A network ACL is a virtual firewall that controls inbound and outbound traffic at the subnet level.
-For example, step outside of the coffee shop and imagine that you are in an airport. In the airport, travelers are trying to enter into a different country. You can think of the travelers as packets and the passport control officer as a network ACL. The passport control officer checks travelers’ credentials when they are both entering and exiting out of the country. If a traveler is on an approved list, they are able to get through. However, if they are not on the approved list or are explicitly on a list of banned travelers, they cannot come in.

-Each AWS account includes a default network ACL. When configuring your VPC, you can use your account’s default network ACL or create custom network ACLs. 
-Stateless packet filtering:-
  -Network ACLs perform stateless packet filtering. They remember nothing and check packets that cross the          subnet border each way: inbound and outbound. 
  https://replit.com/@rajdubal87/AWS-Notes#StatelessNACL.png

After a packet has entered a subnet, it must have its permissions evaluated for resources within the subnet, such as Amazon EC2 instances. 

More About EC2 Security Groups :-

Stateful packet filtering

-Security groups perform stateful packet filtering. They remember previous decisions made for incoming packets.

https://replit.com/@rajdubal87/AWS-Notes#NACLandSecurityGroups.png



Episode 4 : Global Network

-We have been talking a lot about how you will be able to interact with your aws infrastructure.
-But how do your customers interact with your aws infrastructure.
-If you have your website hosted on aws , user just enter the website in theri browser and enter it and some magic happens.But what exactly is this magic?

Two services which help in the website case :-

1)Route 53 :-

-Route 53 is aws's domain name service(DNS).
-DNS is a translation service.
-But instead of translating between languages , it translates website names into IP or internet protocol addresses which computers can read.
-When we enter a website in our browser , it contacts Route 53 to obtain the IP address of the site.
-Lets consider the IP address as 192.1.1.1 , then it routes the user computer browser to that address.
-Route 53 can direct traffic to different end points using several different routing policies such as:-

i)Latency based routing

Purpose:-Improve performance
How it works:-It sends users to the aws regions with lowest network latency(fastest response time) for them.
Example:A user from india is sent to mumbai region and a user from USA is sent to oregon region even if both regions host the same website.

ii)Geolocation DNS :-

here we direct traffic based on where the customer is located
eg.Traffic in ireland is routed to Dublin Region.
Purpose:Direct traffic based on actual location of the user.
How it works:You define rules based on countries,continents,states
Example:You set that all users from Ireland go to a Dublin server, and all users from India go to a Mumbai server.

iii)Geoproximity routing :-

Purpose: Direct traffic based on how close the user is to your resources.
How it works: You use Route 53 + AWS traffic flow to manage this. You can also shift traffic more toward a specific region using a bias.
Example: If you want to route more users towards Singapore, even if they are closer to Mumbai, you can increase Singapore's bias to attract more traffic.

iv)Weighted Round Robin:-
Purpose: Split traffic between multiple resources — useful for load balancing, testing, or gradual rollouts.
-Gradual rollouts (also called canary releases or phased deployments) mean releasing a new feature or version of your application to a small portion of users first, and increasing the rollout gradually to more users over time.
-This helps catch bugs, performance issues, or user experience problems before affecting everyone.
How it works: You assign weights (percentages) to different endpoints.
Example: Send 70% of users to Server A and 30% to Server B.


-You can even use Route53 to register domain names,so you can buy and manage domain names on aws.

2)Amazon Cloudfront :-
-It is a service helps speed up delivery of website assets customers.
-We talked about edge locations earlier in the course , these locations are serving contents as close to customers as possible and one part of that is the content delivery network or CDN.
-A CDN is a network that delivers edge content to users based on their geographic locations.
For example , if the users are in north america , wwe host the website in oregon which is the nearest location.
-And deploy all the static web assets like images,GIF's in Cloudfront in north america.
example 2: we deploy the website in dublin for irish users and deploy all the assets in amazon cloudfront in ireland.

Suppose your main website is hosted in Oregon (US):

    -A user from India tries to open your site.

    -Without a CDN: they download images/CSS from Oregon (slow).

    -With CloudFront: those assets are already cached in a CDN Edge Location in Mumbai (faster).

So CloudFront serves assets from the nearest possible server, making the website load faster for everyone.


More About Amazon Route 53:-
-You can use Route 53 to manage DNS for websites, servers, or services that are not hosted on AWS — for example, services running on GoDaddy, Heroku, DigitalOcean, Azure, on-prem servers, etc.
https://replit.com/@rajdubal87/AWS-Notes#AmazonRoute53.png


Module 5 : Storage And Databases

Episode 2 : Instance Stores and Amazon Elastic Block Stores (Amazon EBS)

-When you are running aws to host business applications, those applications need CPU,GPU,memory and storage.
-As applications run , they will oftentimes need access to block-level storage.
-You can think of block level storage as a place to store files.
-A file being a series of bytes that are stored in blocks on disc.
-When a file is updated, the whole series of blocks arent all overwritten.
-Instead it updates just the pieces that change.
-This makes it an efficient storage type when working with applications like databases , enterprice software or file system .
-When you are using your laptop or computer , you are accessing your block level storage.
-All block level storage in this case is your hard drive.
-EC2 instances has hard ddrives as well and there are a few different types.
-Depending on the EC2 instance you have launched , it might provide you with local storage called instance store volumes.
-These volumes are physically attached with the hosts your EC2 instances running on top of.
Picture For Reference:https://replit.com/@rajdubal87/AWS-Notes#InstanceStoreVolumes.png

-You can write to it just like normal hard drive.
-The catch here is since the instance store volume is attached to the underlying physical host,
if you stop or terminate your EC2 instance,all data written to the EC2 instance store volume will be deleted.
-The readson for this is if you start  your instance from the stop state,it's likely that EC2 instance will startup on another host , a host where that volume does not exist.
-Remember,EC2 instance are virtual machines and therefore the underlying host can change , between stopping and starting an instance.
-Because of this ephermal or temporary nature of instance store volumes , they are useful in the situations where youi can loose the data being written to the drive.
-Such as temp files,scratch data and data that can be easily recreated without consequences.

So you will want a storage that persists outside of the life cycle of an EC2 instance.

Amazon Elastic Block Store(EBS) :-

-With EBS you can create virtual hard drives which we call EBS volumes , that you can attach to your EC2 instances.
-These are separate drives from your local instance store volumes , adn they arent tied directly to the host.
-This means data in a EBS volume can persist between stops and start of a EC2 instance.
-EBS volumes comes in all different sizes and types.
-You define-
i)Size
ii)Type
iii)Configurations 

of the volume you need.
-Provision the volume and attach it to your EC2 instance.
-From there you will have to configure your application to write to the volume and you are good to go.
-If you start and stop the EC2 instance,the data in the volume remains.

-Also it is important to back that data up.
-EBS allows you to take incremental backups of your data called snapshots.
-It is recommended to take regular snapshots of data,so even if your drive is corrupted,you dont loose the data.
-Picture for reference for EBS snapshots : https://replit.com/@rajdubal87/AWS-Notes#AmazonEBSSnapshots.png

How does EBS Snapshots work?
-It first takes backup of all data and incrementally backs up data that has changes since the most recent snapshots.


Amazon Simple Storage Service(S3) :-

-It is a data storage that lets you store and retrieve a virtually unlimited amount of data at any scale.
-Instead of storing the data in a file directory , you store them in what we call buckets.
-Think of a file sitting on your hard drive, thats an object.
-Think of a file directory,thats a bucket.
-The maximum object size that you can upload is 5 Terabytes(5TB).
-You can also version objects to prevent them from accidental deletion.
-What it means that you always retain the previous versions of the objects , its similar to how git works.
-You can also create multiple buckets and store them in different classes or tier of data.
-You can then create permissions to set who can see or access the data.
-You can also stage data between tiers.
-These tiers offer mechanisms for different storage use cases such that data that needs to be accessed frequently or audit that that needs to be retained for several years.

Tiers of storages in Simple Storage Service :-

1)Amazon Simple Storage Service(S3) standard :-
-The first storage class is called Amazon S3 standard and comes with 11 9s of durability.
-That means a object stored in S3 standard has a durability of 99.999999999% probabilty that it will remain intact after one year.
-Furthermore , data is stored in such a way that aws can sustain the concurrent loss of data in 2 separate storage facilities.
-THis is because data is stored in atleast 3 facilities so multiple copies reside accross mutliple locations.
-Another useful way of using Amazon Simple Storage Service(S3) is for static website hosting.
-You can do this by storing the html files and static web assets in a bucket and then checking a box to mark it as a static website.
-You can then enter the bucket's URL and then bam instant website.

2)Amazon Simple Storage Service(S3) Infrequent Access or S3 Standard-IA :-

-It is used for data that is accessed less frequently but requires rapid access when needed.
-This means this the perfect place to store backups,disaster recovery files , or any object that requires long term storage.

3)Amazon S3 Glacier Flexible Retrieval :-

-Another storage class or tier that lends itself to the exmaple of audit data.
-say we will have to retain data for several years for auditing purposes and we dont need it to be retrieved very rapidly.
-THen we can use S3 Glacier FLexible Retreival to archieve that data.
-To use S3 glacier flexible retrieval ,you can simply move data to it or you can create vaults and then populate them with archives.
-And if you have compliance requirements for retaining data for say a certain period of time you can employ an S3 glacier vault lock policy and lock your vault.
-You can specify controls such as write once/read many or WORM in a vault lock policy and lock the policy from future edits.
-Once locked, the policy can no longer changed.
-You also have three options for retreival.
-These range from minutes to hours and you can change the option of uploading directly to S3 glacier flexible retrieval or using S3 lifecycle policies.

What are S3 lifecycle policies:-

-they are policies that you can create to move data automatically between tiers.
-For example , say we want to keep an object in S3 standard for 90 days and then we want to move it to S3 standard IA for the next 30 days.
-Then after 120 days of total , we want to move it to S3 glacier flexible retrieval.
-With lifecycle policies , you create that configuration without changing your application code and it will perform those moves for you automatically.
-Its another example of a managed aws service helping take that burden of you so you can focus more onthe business needs.

4)There are also other storages like S3 one zone infrequent access ,S3 glacier instant retrieval , and s3 glacier deep archieve.
-S3 one zone IA is same as S3 IA , just that the data is stored in one Availability zone and is less resilient to regional failures.
-S3 glacier deep archieve is lowest cost and takes upto 12 hours for retireval of data .best for compliance recors which needs to be stored but access almost none.


Object Storage :-

https://replit.com/@rajdubal87/AWS-Notes#ObjectStorage.png

-The data might be an image, video, text document, or any other type of file. Metadata contains information about what the data is, how it is used, the object size, and so on. An object’s key is its unique identifier.

S3 Intelligent-Tiering:-

-Ideal for data with unknown or changing access patterns
-Requires a small monthly monitoring and automation fee per object
-In the S3 Intelligent-Tiering storage class, Amazon S3 monitors objects’ access patterns. If you haven’t accessed an object for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, S3 Standard-IA. If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, S3 Standard.


AWS S3 Outposts :-

 What is AWS Outposts?

AWS Outposts is a hybrid cloud solution from AWS.
✅ Key Idea:

It brings AWS infrastructure and services to your on-premises location. Basically, Amazon ships a rack full of servers to your data center, and it runs AWS services locally, but managed by AWS.

S3 Outposts
Creates S3 buckets on Amazon S3 Outposts

Makes it easier to retrieve, store, and access data on AWS Outposts

Amazon S3 Outposts delivers object storage to your on-premises AWS Outposts environment. Amazon S3 Outposts is designed to store data durably and redundantly across multiple devices and servers on your Outposts. It works well for workloads with local data residency requirements that must satisfy demanding performance needs by keeping data close to on-premises applications.


Episode 3 : Comparing Elastic Block Storage and Simple Storage Service

Amazon EBS :-

-You can create a single EBS volume as large as 16 TiB (Tebibytes).
-Survives termination of their EC2 instance.
-Solid State by default.
-HDD options
Use Case:- You have a 80 GiB video file that you are dooing video corrections on.
-To know the best storage class here we need to understand the difference between object storage and block storage.
-Object storage treats any file as a complete discrete object.Now this is great for documents and images and video files that gets uploaded and gets consumed as entire objects but everytime there is a change to the object , you must re upload the entire file.There are no delta updates.
-This is because S3 treats each object as a single, immutable unit. If you want to modify any part of the object (file), you must upload a new full version of it.
-S3 Versioning allows you to keep multiple versions of the object, but:
-It does not support editing part of an object.
-It just saves each full upload as a new complete version.

-Block storage breaks that component down to small component parts or blocks.
-This means for that 80 GiB file , you make a edit for that 1 scene in the film and save that change , the engine only updates the blocks where these bits live.
-So if you are making a bunch of micro edits , EBS is perfect for it.
-If you would have been doing changes in an object is S3 , the system would have to upload the entire 80GiB file again.

Amazon S3 :-

-Unlimited Storage
-Individual objects upto 5TB in size.
-WORM(Write once read many).
-11 9s(99.999999999)% durable.
Use case:-Lets say you have a photo analysis website where users upload a picture of themselves , adn your website tells which animal looks like you .
-You have potentially millions of animal pictures that all need to be indexed and possibly be viewed by thousands of people at once.
-This is the perfect use case for S3.
-S3 is already web enabled , every obejct has a URL and yopu can put access rights to who can see or access your object.
-Its regionally distributed , so no need to woprry about backup strategy.
-Offers cost savings compared to EBS with additional advantage of being serverless.
-No need of a EC2 instance for this.


Amazon Elastic File System (EFS) :-

-EFS is a managed file system.
-Its common for businesses to have shared file sstem accross their aplications.
-For example you might have mutliple servers running analytics on large amount of data being stored in  a shared file system.
-This data is traditionally hosted on premises.
-In this on-prem data center you would have to ensure that the storage you have can keep up with the amount of data you are storing.
-Make sure backups are taken and the data is stored redundantly as well as manage all of the servers hosting that data.
-With AWS you dont have to worrry for all that buying hardware and managing servers.
-With EFS , you can keep existing file systms in place but let aws do all the heavy lifting of the scaling and the replication .
-EFS allows you to have multiple instances accessing the data in EFS at the same time.
-It scales up and down without you needing to do anything to make that scaling happen.
-When it says "EFS allows you to have multiple instances accessing the data at the same time," it means:
-Multiple EC2 instances (virtual machines) can mount the same EFS file system concurrently and read/write to the same files or folders, just like a shared drive in a local network.
-But even Amazon Elastic Block Storage lets you store files, that I can acces from a EC2 instance.
-What exactly is the difference here?
-Amazon EBS volumes attach to EC2 instances and are an availability zone level resource.
-In order to attach EC2 to EBS you need to be in the same availability zone.
-An EBS volume is physically located in a specific AZ within an AWS Region, and it can only be attached to an EC2 instance that is also in that same AZ.
-You can save files on it or run a database on top of it or store applications on it.Its a harddrive.
-EBS can behave like a hard disk, but what it’s actually running on depends on the type of EBS volume you choose.
-Most users use SSD-backed EBS volumes (gp3 or io2) unless they need low-cost bulk storage.
-If you provision a 2TB EBS volume and fill it up , it doesnt automatically scale to give you more storage.
-Where as Amazon EFS can have multiple EC2 instances reading and writing from it at the same time.
-But it isnt just a  blank hard drive that you can write to.It is a true file system for linux.It is already formatted and knows file directories , access permissions etc unlike EBS.
-It is a regional resource meaning any EC2 instacne from that region can write to the file system.
-As you write more data to EFS , it automatically scale.
-No need to provision any more volumes.
-Additionally, on-premises servers can access Amazon EFS using AWS Direct Connect.


Amazon Relational Database Service (RDS) :-

-From the coffee shop example , you are storing data about your coffee shop in mutiple storage devices.
-But now you have a need to have relationships between data that you have.
-By relations I mean maybe a customer buys same coffee mutliple times, maybe you want to give him a promotional discount for the next time and you need a way to keep track of this relationship somewhere.
-In this case its best to use a relational database management system.
-It means you store data in a way it relates to other pieces of data.
-What are all the databases supported by AWS?
1)MySQL
2)PostgreSQL
3)Oracle
4)Microsoft sql server

-You might be running one of those and they are most likely housed in your data center.
-But is there way to easily move them to cloud?Yes.
-You can do what we call lift and shift and migrate the database to run on amazon EC2.
-This means you have control over the same variables you do,in your on premises environment.
-These variables are such as OS,memory,CPU,storage capacity and so forth.
-It's a quick entry to the cloud and you can migrate them using standard practices or using something like database mitgration service.
-The more managed way to use databases in the cloud is through Amazon Relational Database Service.
-It supports all the major database engines, like the ones we mentioned earlier , but this service comes with added benifits.
-These include automated patching , backups , redundancy ,failover , disaster recovery all of which you will have to manually manage yourself.


Amazon Aurora :-

-It's ur most managed relational database option.
-It comes in two forms , MYSQL and PostgreSQL.
-The data is replicated at different facilities so that you have 6 copies of data at any given time.
-You can also deploy upto 15 read replicas so you can offload your reads and scale performance .
-You can create copies of your primary Aurora database that are read-only and automatically kept in sync. These replicas can handle read queries, which helps reduce load on the main database and improve performance and scalability.
-Additionally there are continuos backups to S3 , so you always have a backup ready to restore.
-You also get point-in-time recovery so you can recover data from a specific period.


More About Amazon RDS :-

-Amazon RDS is a managed service that automates tasks such as hardware provisioning, database setup, patching, and backups.
-Amazon RDS provides a number of different security options. Many Amazon RDS database engines offer encryption at rest (protecting data while it is stored) and encryption in transit (protecting data while it is being sent and received).
-Amazon RDS is available on six database engines, which optimize for memory, performance, or input/output (I/O). Supported database engines include:

Amazon Aurora
PostgreSQL
MySQL
MariaDB
Oracle Database
Microsoft SQL Server

More About Amazon Aurora :-

-Amazon Aurora(opens in a new tab) is an enterprise-class relational database. It is compatible with MySQL and PostgreSQL relational databases. It is up to five times faster than standard MySQL databases and up to three times faster than standard PostgreSQL databases.
-Amazon Aurora helps to reduce your database costs by reducing unnecessary input/output (I/O) operations, while ensuring that your database resources remain reliable and available. 


Amazon Dynamo DB :-

-At most basic level, amazion dynamo DB is a database.
-It's a serverless database.
-Meaning you dont need to manage underlying instances or infrastructure powering it.
-With DynamoDB you create tables.A DynamoDB table is a place where you can store and query data.
-Data is organized into items, and items have attributes.
-Attributes are just different features of your data.
-If you have 1 item in your table or 2 million items in your table , DynamoDB manages the underlying storage for you and you dont have to worry about the scaling of the system , up or down.
-DynamoDB stores this data redundantly accross availability zones and mirrors the data accross mutliple drives under the hood for you .
This refers to replicating (copying) your data onto multiple physical storage devices (SSDs) within the same Availability Zone (AZ).
So:
When you write a record to DynamoDB, it is instantly copied to multiple SSDs (storage drives) within that zone.
This ensures that if one drive fails, your data is still safe and accessible from another one.

-DynamoDB doesnt use widely used query language like SQL.
-Relational databases like MYSQL need a schema defined that store related tables.
-But these kind of databases might have performance issues when under stress.
-These databases might not be the perfect fit when your dataset is less rigid(less strict for datatype).
-This is where NoSQL databases comes in.
-DynamoDB is a non-relational database tend to have simple flexible schemas,not complex rigid schemas laying out multiple tables that all relate to each other.
-With DynamoDB you can add or remove the attributes of items from the table at any time.
-Not every item in the table has to have the same attributes.
-Because of this flexibility you cannot run complex sql queries on it.
-Instead you write queries based on small subset of attributes that are designated as keys.
-Because of this the queries that you run on non-relational databases tend to be simpler and focus on a collection of items from one table not queries that span multiple tables.
-Also , the dynamoDB is purpose built.This will not be the perfect fit for some use cases.
-DynamoDB is serverless, which means that you do not have to provision, patch, or manage servers. 
-You also do not have to install, maintain, or operate software.

Amazon Redshift :-

-You want to run business intelligence or BI projects against data coming from different data stores like your inventory , your financial and your retail sales system.
-A single query against multiple databases sounds nice , but traditional databases dont handle them easily.
-Once data becomes too complex to handle with traditional relational databases , you've entered the world of data warehouses.
-Data warehouses are engineered specifically for this kind of big data where you are looking at historical analysis(To analyze past trends, generate business reports, and make strategic decisions.) as opposed to operational analysis(To support day-to-day operations of an application or system).
-Historical maybe as soon as : show me last hours sales numbers accross all the stores.The thing here is the data is set right now.Like the last hours sales numbers wont change because its in the history now.
-Now compare this question to how many bags of coffee we still have in our inventory right now?
-Which could be changing as we speak.Because its happening right now.
-As long as your business question is looking backwards at all,then a data warehouse is the right solution for that line of business intelligence.
-Now there are many data warehouse solutions out in the market.If you already have a favourite one, running it on aws is just a matter of getting the data transferred.
-But it would be better for a business to focus on the data rather than caring for the tuning of a data warehouse engine and management of it.
-This(Amazon Redshift) is data warehousing as a service.
-Amazon Redshift is a fully managed, petabyte-scale data warehouse service by AWS that’s optimized for analytics — enabling you to run complex queries and generate business intelligence (BI) insights across massive datasets.

AWS Data Migration Service :-

-What happens if you have a database on prem or in the cloud already?
-AWS offers a service called Amazon Database Migration Service(DMS) to help customers migrate the database.
-DMS helps customers migrate existing databases onto aws in a secure and easy fashion.
-You essentially migrate your database from a source to a target database.
-The best part is that te source database remains fully operational during database migration.
-The source and target databases doesnt have to be of the same type.

-Lets start with databases that are of the same type.These migrations are known as homogenous operations and can be from MYSQL to Amazon RDS MySQL or MSSQL Server to AMAZON RDS SQL Server.
-The process is fairly straighforward since schema structures , data types and database code is compaitible between source and target.
-The source database can be running on on-premises , Amazon EC2 instances or it can also be an amazon rds database.
-The target itself can be a database in amazon RDS or Amazon EC2.
-In this case, you create a migration task with connections to source and target databases.
-Then you can start the migration service with a click of a button.
-The second type of migration is heterogenous migration which occurs when source and target databases are of different types.
-Since the schema structure , data types and database code are different between source and target , we first need to convert them using the aws schema conversion tool.
-This will convert the source schema and code to match that of the target database.
-Then next step is to use Data migration service to migrate the data from source database to the target database.
-But these are not the only use cases for DMS.
-Others include developement and test database migrations , database consolidation and even continuos database replication.

Development and Test database migrations :-

-Its when you want to develop this to test against the production data but without affecting production users.
-In this case you use Database Migration Service to migrate a copy of your production database to your dev or test environments either once off or continuosly.

Database Consolidation :- 

-Database consolidation is when you have several databases and want to consolidate(.
to join things together into one; to be joined into one) them into one single database.

Continuos Replication Of Database :-

-Its when you use DMS for continuos data replication , this could be for disaster recovery or geographic replication


Additional Database Services :-

Amazon DocumentDB : Great for content management , catalog , user profiles.
-Amazon DocumentDB(opens in a new tab) is a document database service that supports MongoDB workloads. (MongoDB is a document database program.)

Amazon Neptune : -A graph database , engineered for social networking and recommendation engines, also great for fraud detection needs.
-You can use Amazon Neptune to build and run applications that work with highly connected datasets, such as recommendation engines, fraud detection, and knowledge graphs.

Amazon Managed Blockchain : -Amazon Managed Blockchain(opens in a new tab) is a service that you can use to create and manage blockchain networks with open-source frameworks. 

Amazon quantum ledger database (QLDB) :- A immutable system of record where any antry can never be removed from the audits.
-Amazon Quantum Ledger Database (Amazon QLDB)(opens in a new tab) is a ledger database service. 
-You can use Amazon QLDB to review a complete history of all the changes that have been made to your application data.

Database Accelerators :- Starting with adding caching layers on top of your databases that could improve the read times of common requests from milliseconds to microseconds.

Amazon Elasticache :- It provides those caching layers without your team need to manage it.
-It comes with both memcached and redis flavors.
-Amazon ElastiCache(opens in a new tab) is a service that adds caching layers on top of your databases to help improve the read times of common requests. 

Amazon DynamoDB Accelerator : - If you are using dynamoDB try using DAX, the dynamoDB accelerator.It is a native caching layer designed to dramatically improve read times for your non-relational data. 
-Amazon DynamoDB Accelerator (DAX)(opens in a new tab) is an in-memory cache for DynamoDB. 

Q.What is a enterprise class relational database?
-Amazon Aurora


Module 6 :Security

The Shared Responsibility Model :-

-With the shared responsibilty model , AWS controls the security "of" the cloud , and customers control security "in" the cloud.
-AWS controls the data centers, security of the services etc.
-Refer this image :- https://replit.com/@rajdubal87/AWS-Notes#SharedResponsibiltyModel.png
-The part of the workload that aws customers run "in" the cloud and this is the customers responsibilty to secure.
-The AWS Shared Responsibility Model defines who is responsible for what when you use AWS cloud services — it clearly separates responsibilities between AWS (the cloud provider) and you (the customer).
-The shared responsibilty model is nothing different than securing a house.
-The builder constructed the house with 4 walls and a door.
-It's their responsibility to make sure the walls are strong and the doors are solid.
-Its your responsibility(the home owner) to close and lock the doors.

Layers Of Shared Responsibility :-
1)Physical : This is iron and concrete and fences and security gaurds.Someone has to own the concrete, someone has to staff the perimeter 24/7.This is AWS.
2)Network And Hypervisor : On top of physical layer, we have network and hypervisor which has gone through several third party audits.
3)Operating System:-On EC2 you now get to pick what operating system you want to run.
-This is the dividing line that separates AWS's responsibility and customers responsibility.
-This is customers operating system and customer is 100% incharge of this.
-Customer is the only person who has the encryption key to log on to the root of OS or to create any user accounts there.
-AWS cannot enter customers operating system.
4)Applications:- On top of this OS we can run whatever applications we want.
-Customer owns it and maintains it.
5)Data :- This is always customer's domain to control.
-Who has access to data is customers responsibilty.
-AWS provides all the tools required fot the customer to keep the data safe with the ubiquitous encryption.
-Ubiquitous encryption refers to the practice of encrypting all data—everywhere, all the timeby default, regardless of whether it’s at rest, in transit, or even in use.
-So even if you keep the door open , no one could read the encrypted content.
































