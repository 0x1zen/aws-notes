Section 3: What is Cloud Computing

How do websites work?

-We have a server and we as a browser needs access to that server to visualize a website.
-What we as clients are going to do is use a netork.A network between client and server.
-The client will find the network and use the network to route the packet's , the data into the server.
-Then the server will reply to us and we will get the response , and we can view a website.
-The clients to find the server and the server to find the clients, you need to have IP addresses.
-So clients have IP addresses and a server also have a IP address.
-And so the idea is that when you use an IP address, you can send a request to wherever you want , to the server you want , and the server knows how to find you back.

What is a server composed of?

1)CPU - A server is going to contain a CPU.And a CPU is a little piece that will be doing some computations.
2)RAM - Your server also needs RAM or memory .This is going to be very very fast memory, which will allow us to store information and retrieve it very quickly.
3)Storage - In computers we have storage to store data.For example files.
4)Database - If we want to store the data in a very structured format , we want to be using a database.
-Database will be going to have data which will be well structured and we can search and query it.
5)Network - Its going to be routers , switch , dns servers etc.

-The cloud gives us all of these things on demand.

Network :- cables , routers and servers connected with each other.
Router :- A networking device that forwards data packets between computer networks .They know where to send the packets on the internet.
Switch:- Takes a packet and send it to the correct server / client on your network.

So, our client will send the data to a router , the router will find its way all the way to a switch, and the switch will know which computer in your network to send the data to.

The traditional approach :-

-People who used to start their websites used to go to stores and buy their own servers and put it into their garage or home and start it.Just like how google was started.
-But,if your business grows, you gonna need more servers, more electricity, cooling , maintainance etc.
-Also the scaling will be limited and you will need to have a team who monitors this infrastructure 24/7.
-So,can we extranalize this?Yes.

What is cloud computing?

-Cloud computing is the on demand delivery of the compute power , database storage , applicatons and other IT resources over the internet.
-On demand means you get it when you need it.
-Througha cloud services platform , you are going to get pay-as-you-go pricing.


Amazon Web Services :-

-Amazon web services owns and maintains the network connected hardware required for these application services , while you provision and use what you need via a web application.

The deployment models of the cloud:-

1)Private Cloud : Cloud services used by a single organization , not exposed to the public.
-Organization have complete control over it.
-Security for sensitive applications.
-Meets specific business needs.
For example . rackspace

 What is Rackspace?

Rackspace is a cloud computing company that provides managed cloud services. They help businesses deploy, operate, and optimize cloud infrastructure across platforms like:
1)AWS
2)Microsoft Azure
3)Google Cloud
4)Their own private cloud offering

So they’re not just a cloud provider — they’re more like cloud enablers or partners, especially for companies that want help managing cloud systems.

2)Public Cloud :- Cloud resources owned and operated by a third party cloud service provider delivered over the internet.
For example. AWS , GCP and Azure.

3)Hybrid Cloud :- Here we mix up private and public.
-We keep some servers on premises and extend some of the capability to the cloud.
-Control over sensitive assets in your private infrastructure.
-Flexibility and cost effectiveness of the public cloud.

The Five Characteristics Of Cloud Computing :- 

1)On-demand and self service :- Users can provision resources and use them without human interaction from the service provider.
2)Broad Network Access :- Resources are available over the network , and can be accessed by diverse client platforms.
3)Multitenancy and resource pooling :- Multiple customers can share access the same resources and applications while still having security and privacy.
4)Rapid elasticity and scalability :- Automatically and quickly acquire and dispose resources when needed.
-Quickly and easily scale on demand.
5)Measured service :- Usage is measured, users pay exactly for what they have used.

IaaS – Infrastructure as a Service
💡 What it is:
IaaS provides virtualized computing resources over the internet. This includes servers, storage, networking, and virtualization. Users manage the OS, middleware, and apps themselves.

📌 You manage:
Operating System
Middleware
Applications
Data

Cloud provider manages:
Virtualization
Servers
Storage
Networking

🛠 Examples:
Amazon Web Services (AWS) EC2
Microsoft Azure Virtual Machines
Google Compute Engine

🧠 Use Case:
A startup wants full control over their web server environment and decides to run their own stack (Linux, Apache, MySQL, PHP) on virtual machines in AWS EC2.

✅ 2. PaaS – Platform as a Service
💡 What it is:
PaaS provides a platform for developers to build, test, deploy, and manage applications without managing the underlying infrastructure.

📌 You manage:
Applications
Data
Cloud provider manages:
Runtime
Middleware
OS
Servers
Storage
Networking

🛠 Examples:
1)Google App Engine
2)Heroku
3)Microsoft Azure App Services

🧠 Use Case:
A developer wants to build a web app but doesn't want to manage servers. They deploy their app on Heroku, which automatically handles scaling, deployment, and runtime.

✅ 3. SaaS – Software as a Service
💡 What it is:
SaaS delivers fully functional software applications over the internet on a subscription basis. Users only use the software; everything else is managed by the provider.

📌 You manage:
Nothing (just use the software)

Cloud provider manages:

Everything: App, Data, OS, Infrastructure, etc.

🛠 Examples:
1)Google Workspace (Docs, Sheets, Gmail)
2)Microsoft 365
3)Salesforce
4)Zoom

Use Case:
A company uses Google Docs for document collaboration. They don’t need to install or maintain any software — just login and use it from any browser.

AWS Global Infrastructure :-

1)AWS Region - it is a cluster of data centers.
-Most AWS services are region scoped.
2)Availability Zones - Each region has a minimum of 3 availability zones.
-Each AZ is one or more discrete data centers with redundant power,networking and connectivity.
-They are separate from each other so they are isolated from disasters.
-Connected with each other with high bandwidth low latency.
3)Local Zones
4)Points of presence - These are AWS Edge Locations.
-Content is delivered to end users with lowest latency.
5)Network

Tour Of AWS Console :-

Global Services :-
1)Identity and Access Management.
2)Route53 DNS Service
3)Cloudfront Content Delivery Network
4)WAF(Web Application Firewall)

Region Scoped:-
1)Amazon EC2(Infrastructure as a service)
2)Elastic Beanstalk(platform as a service)
3)Lambda(function as a service)
4)Rekognition(softweare as a service)

The AWS Shared Responsibility Model :-

1)AWS is responsible for the security of the cloud
2)You are responsible of security in the cloud.


IAM : Users and groups :-

-IAM stands for Identity and access management.It is an global service.
-When we created an account , we created an root account.It is created by default.
-This account should not be shared.
-Users are people in your organization, and can be grouped.
For example :- 
Alice bob and charles work as devs.So they can be grouped together in a single group called developers.
Whereas david and edward work in operations , and they can be grouped in a operations group.

-A group can be only of group of users.It does not consist of other groups.
-Users dont have to belong to a group , and a user can belong to multiple groups.

Identity and access management(IAM) Permissions :-

-Users or Groups can be assigned JSON documents called policies.
-These policies define the permissions of the users.
-If we by default give all the permissions to a user, that user can misuse it.
-In AWS you apply the least previlege principle:dont give more permissions than a user needs.

-To create a custom URL , we can use alias.This alias has to be unique.


IAM policies inheritance :-

-When we have a group of developers , we attach a IAM policy at group level.So, every member of that group will have same IAM policy.
-We can inline policies to a user who is not part of any group.

IAM Policies Structure


{
"Version": "2012-10-17",
"Statement": [
  {
    "Sid": "AllowS3ReadAccess",
    "Effect": "Allow",
    "Action": [
      "s3:GetObject",
      "s3:ListBucket"
    ],
    "Resource": [
      "arn:aws:s3:::my-demo-bucket",
      "arn:aws:s3:::my-demo-bucket/*"
    ]
  }
}

1)Version :- Always include '2012-10-17'.
-AWS introduced this policy language version on October 17, 2012, hence "2012-10-17".
-It's not a version of your policy — it indicates the syntax and behavior rules that AWS should use to evaluate the policy.
2)Id :- an Identifier for the policy (optional)
3)Statements :- One or more individual statements.

Statements Consist of :-

1)Sid :- it is an identifier for the statement (optional).
2)Effect :- Whether the statement allows or denies the access(Allow , Deny).
3)Principal :- account/user/role to which this policy applied to
4)Action :- list of actions this policy allows or denies.
5)Resources :- list of resources to which the actions will be applied to
6)Condition :- conditions for when this policy is in effect.

The AdministratorAccess Policy :-

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "*",
            "Resource": "*"
        }
    ]
}

-This means allow all actions and resources wherever this policy is applied.


IAM Password Policy:-

-Identity Access and Management password policy is used to protect the users and groups from being compromised.
-There are two defense mechanisms here:

A)In AWS you can setup a password policy:

1)Set a minimum password length
2)Require specific character types :
-including uppercase letters
-lowercase letters
-numbers
-non-alphanumeric characters 
3)Allow all IAM users to change their own passwords or set password expiration to make them change password after some time.
4)Prevent password reuse

B)Multi Factor Authentication - MFA

-Users have access to your account and can possibly change configurations or delete resources in your aws account.
-You want to protect your root account and IAM users.
-MFA = password you know + security device you own.
-Main benefit here is even if a password is compromised , your account is still not compromised

MFA devices options in AWS:
1)Virtual MFA device eg.google authenticator or authy
2)Universal second factor (U2F) security key eg.Yubikey by Yubico (3rd party).
3)Hardware Key Fob MFA device eg . Provided by Gemalto (3rd party)
4)Hardware Key Fob MFA device for AWS GovCloud(US). eg. Provided by SurePassID (3rd party).

How Users Can Access AWS:-
To access aws , users have three options :-
1)AWS management console (protected by password + MFA)
2)AWS Command Line Interface(CLI) : protected by Access keys (similar to a github Private Access Token)
3)AWS software developer kit (SDK) :- for code: protected by access keys

-Access keys are generated through aws console.
-Users manage their own access keys.
-Users manage their own access keys
-Access keys are secret just like a password.

-Every access key has a Access Key ID and Secret Access Key.

Whats AWS CLI :-

-A tool that enables you to interact with AWS services using commands in your command line shell.
-Direct access to the public API's of AWS services.
-You can develop scripts to manage your resources.
-Its a alternative to aws management console.

Whats AWS SDK :-

-It is embedded within your application.
-Enables you to access and manage AWS services programatically.
-Supports multiple languages like c++,go,nodejs etc.
-Mobile SDK's (android,ios)
-IOT device SDK's ( Embedded C, arduino)

Example: AWS cli is built on AWS SDK for python.

Installing the aws cli on windows the current version is cli 2.

Using CLI - 
1)Create a access key for your IAM account
1)aws configue
2)give access key id
3)give secret access key
4)Default region as ap-south-1 which is mumbai
5)Default output format as directly enter 
6)aws iam list-users : it will give list of all users
7)aws sts get-caller-identity : with this command aws security token service you can know which user is currently calling the command


There is an alternative to terminal to issue commands towards aws:-

AWS Cloudshell :-

-It is a terminal aws cloud which is browser based.
-If you create any file in aws cloudhsell environment, it will stay as saved.For example 
echo "test" >  demo.txt
cat demo.txt

-Above command will create a text file demo.txt with "test" as text inside of it.
-It has upload and download feature to upload and download files from it.


IAM Roles For Services :- 

-Some aws service will need to perform actions on your behalf.
-To do so , we will assign permiossions to aws services with IAM roles .
-These IAM roles are like normal roles but it will be applied to aws services instead of physical people.
For Eg. The EC2 instance may want to perform some actions on aws.To do so we want to create a IAM role.Together make a 1 entity.

AWS Roles :- 

-Role is a way of giving permissions to aws entities to do stuff on aws.
-Allows EC2 instances to call AWS services on your behalf.

IAM Security Tools:-

1)IAM Credentials Report(account - level) :-

-a report thatlists all your accounts users , regardless of who generated the report.
-By account level it means whether you are logged in as root or an IAM user with sufficient permissions , the report includess all the users under that aws account ID.

2)IAM Access Advisor ( user - level) :-

-Access advisor shows the service permissions granted to a user and when those services were last accessed.
-You can use thios information to revise these policies.

IAM Guidelines and Best Practises :-

-Dont use the root account except for aws account setup.
-One physical user = one aws user 
-Assign users to groups and assign permissions to groups 
-Use strong password policy
-Use and enforce the use of Multi factor authntication.
-Create a give roles for giving permissions to aws services.
-Use access keys for programmatic access (CLI/SDK)
-Audit permissions of your account using IAM credentials report and Access Advisor(last accessed )
-Never share IAM users and access keys


Shared Responsibility Model For IAM :-

AWS - 
-Infrastructure(global network security)
-Configuration and vulnerability analysis 
-compliance validation

you -
-Users groups,roles , policies management and monitoring .
-Enable mfa on all accounts.
-Rotate your keys often
-Use IAM tools to apply approprita epermissions.
-Analyze access patterns and review patterns.


AWS Budget Setup :-

-We cannot do this from a IAM user account,we can only do this from root account.
-Allow  IAM to see bills from account tab of root account.
-Set zero spend budget and montly spend budget to get emails on forecasted spending,85%spend etc.


AWS Elastic Clound Compute (EC2) :-

-EC2 = Elastic cloud compute = infrasturcture as a service
-If mainly consists in the capability of :-
1)Renting Virtual Machines(EC2)
2)Storing data on virtuial Drives (Elastic Block Storage(EBS))
3)Distibuting load accross machines (ELB(elastic load balacning))
4)Scaling services using an auto scaling group(ASG)

EC2 Sizing and Configuration Options :- 

-Operating System : Linux or windows or macos
-How much compute power and cores you need(CPU)
-how much random access memory(RAM)
-How much storage space :
  -Network-attached(EBS and EFS)
  -Hardware (EC2 instace store)
-Network card :- speed of the card , public IP address
-Firewall rules : security group
-Bootstrap Script (configure at first launch) : EC2 user data

EC2 User Data :-

-It is possible to bootstrap our instances using an EC2 user data script.
-bootstrapping means launching commands when a machine strarts
-The scipt is only run once at the instance first start.
-EC2 user data is used to automate boot tasks such asd:
1)Installing updates
2)Installing software 
3)Downloaduiing common files from internet

-EC2 User Data Scriupt runs with root user.

Types Of EC2:-

-t2.micro is allowed in the free tier and we will get 750/hours a month.

1)General Prupose - with a t
2)Compute Optimized- with a c
3)Memory Optimized  - with a R (means ram)
4)Storage Optimized

Security Group :-

-Security groups are a firewall on our EC2 instance.
-They regulate
1)Access to prots
2)Authorized IP ranges - IPV4 to IPV6
3)Control of inbound network (from other to instance)
4)Control of outbound network (from isntance to the other )

-Security groups can be attached to  multiple instances
-Locked down to a region / VPC combination
-Does live outside the EC2 - if the traffic is blocked the EC2 instance wont see it.
-Its good to maintain one separate security group for SSH access.
-We can authorize other security groups in the inbound rules of a security group.
-By default all inboundtraffic is blocked and all outbound traffic is authorized.


Classic PORTs to know :-

22 = SSH(secure shell) - log into a linux isntance
21 - FTP(File transfer Protocol) - uplaod files into a file share
22 = SFTP(secure file transfer protocol) - uplioad files using ssh
80 - HTTP - access unsecured websites
443 = HTTPS - access secured websites 
3389 = RDP (Remote desktop protocol) - log into a windows instance

Inbound Rules in Security Groups :-

-Inbound rules are the rules applied to connections coming from outside to the EC2 instance.
-They control the incoming traffic coming to EC2.
-You can attach as many security groups to a single ec2 instance and the rules in it will just add on.
-You can also add a single security group to multiple ec2 instances.

SSH(Secure Shell) :-

-It is a command line utility which lets you securely access and manage remote computers over an unsecured network.
-Encrypts communication between two systems (client and server)
-It works on linux , windows , mac but windows less than 10 (eg. windows 7 or 8) will use something called Putty which does the same thing as SSH.

ssh command - 
ssh -i "EC2 Learning (1).pem" ec2-user@public_ip

Alternative To SSH :-

EC2 Instance Connect : 

-It is a web based terminal like tool where we can connect to our EC2.
-We can also do aws configure in order to setup our access key.But this is a bad practice as anybody with access to your account can go and retirve this key.
-Instead remember we have IAM Roles to give permissions to aws services.
-We add a IAM role to the ec2 instance.
-When you attach an IAM role to an instance, it can access temporary credentials from the Instance Metadata Service (IMDS)

EC2 Insrtances Purchasing options :-

1)On-demand instances : short workload , predictable pricing , pay by second
2)Reserved (1 & 3 years): 
  -Reserved Instances : long workloads
  -Convertible reserved instances : long workloads with flexible instances
3)Savings Plan (1 & 3 years) : commitment to an amount of usage  , long workload
4)Spot instances : short workloads , cheap , can loose instances .
5)Dedicated hosts : book an entire physical server , control instance placement .
6)Dedicated instances : no other customers will share there hardware 
7)Capacity reservations  - reserve capacity in a specif AZ for any duration.

AWS Charges For IPV4 Addresses :- 

Starting from 1st february there is a charge for all public IPv4 created in your account.
-$0.005 per hour of Public IPV4.
-IPV4 is a unique internet facing address assigned to your server or instance.
-This is because IPV4 are becoming rare and are exhausted so the idea is to move to IPV6 which can be scaled.

EC2 Storage :-

-An EBS is (Elastic Block Storage) Volume is a network drive you can attach to your instances while they run.
-It allows your instances to persist data even after termination
-One EBS volume can only be attached to one EC2 instance
-It is bound to a specific availability zone.
-It is like a network drive .It uses network to communicate the instance which means there might be a bit of latency.
-Free trie 30 gb 
-Since they are network drives , they can be detached from a ec2 instance and attahed to another very quickly
-Have a provisioned capacity (size in GB's and IOPS inpur output operations)
-A EBS volume can be kep unattached.It is not necessary to attach it to a EC2 instance.
-EBS volume has a delete on termination attribute.It controls EBS behaviour when EC2 instance is terminated.
- A root EBS volume refers to the EBS volume that is used as the root file system for the EC2 instance.
-This volume contains the operating system and other essential files required for the instance to boot and operate.
-By default the root EBS volume is deleted(the attribute of 'delete on termination') is enabled.
-By default the other attached EBS volume is not deleted(attribute disabled.)
-You can preserver the root volume when instance is terminated.
-While I say that in the previous lecture that EBS volumes cannot be attached to multiple instances, I know it is not true for io1 and io2 volume types: this is called the EBS Multi-Attach feature.

EBS Snapshots :- 

-Make a backup of your EBS volume at a point  in time .
-Not necessary to dettach the volume to do snapshot , but it is recommended to do so.
-We can copy snapshots from a AWS Region or AZ

EBS Snapshots Archieve :- 

-Move your snapshots to archieve tier that is 75% cheaper.
-Takes 24-72 hour to restore from that archieve.

Recycle bin for EBS snapshots :-

-Setup roles to retain deleted snapshots so you can recover them after accidental deletion.
-Specify retention (from 1 day to 1 year).
-This falls under retention policy.


Amazon Machine Image (AMI) :-

-AMI are customization for an EC2 instance.
-we have our own OS , config , softwares , monitoring etc.
-Faster boot/config time because all your software is pre-packaged.
-AMI are built for a specific region and can be copied accross region
-You can launch EC2 instance from a public AMI : AWS provided
-If you make your own AMI , you will have to make and maintain it yourself.
-An AWS MarketPlace AMI : An AMI someone else made ( and potentially sells).


AMI Process :-

-Start an EC2 instance and customize it.
-Stop the instance for data integrity.
-Build a AMI out of it - this will also create a EBS snapshot
-Launch instances from other AMI's .

EC2 image builder :-

-Used to automate the creation of virtual machines or container images.
-Automate the creation , maintain and validate and test EC2 AMI's.
-These AMI's can be distributed accross regions or AZ's.

EC2 instance store :-

-EBS volumes are netowork drives with good but limited performance.
-instance store is a hardware disk attached to your ec2 instance.
-EC2 instance store storeage will be lost when ec2 instance is stopped or terminated.
-They are epheremal.
-Risk of data loss if hardware fails.

Elastic File System (EFS) :-

-Managed NFS(network file system) that can be mounted on 100s of EC2.
-EFS works only with Linux EC2 instances woth multi-AZ.

Difference between EBS and EFS :-

-EBS can only be attached to a single EC2 instance where as EFS is shared file system for multiple ec2 instances accross multiple availablity zones.
-EBS can be backed up as snapshots then can be restored in a different ec2 in different az.

EFS Infrequent Access :- 

-Storage class that is cost-optimized for files not accessed everyday.
-Upto 92% lower cost compared to EFS standard
-EFS will automatically move your files to EFS-IA based on last time they were accessed.

Amazon FSx :-

-Launch 3rd party high performance file systems on AWS.
-Fully managed service.

Amazon FSx for Windows File Server :-

-A fully managed , highly reliable and scalable Windows native shared file system.
-Built on Windows file server..so this is only for windows instances.
-Supports SMB protocol and windows NTFS
-SMB is a network file-sharing protocol that allows applications and users to access files, printers, and other resources on a network.
-NTFS is a file system developed by Microsoft for storing and managing data on disk drives. It is the default file system for Windows operating systems.

Amazon FSx for Lustre :-

-A fully managed high performance scalable file storage for High Performance Computing(HPC).
-The name is Lustre because it is derived from Linux and Cluster.
-eg. for machine learning , analytics etc.
-Behind the scenes it will be storing data in a s3.

Elastic Load balancing and auto-scaling groups :-

-Scalability means a application/system can handle greater loads by adapting.
-There are two kinds of scalability.
1)Vertical Scalability - For example we upgrade a t2.micro to t2 large this means we vertically scaled it.
-Vertical scalability means increasing the size of the instance
-Vertical scalability is very common for non distributed systems , such as database.
-There is a limit on how much you can scale.That is the limit of the hardware.

2)Horizontal Scalability - 
-Here you increase the number of instances / systems for your application.
-Horizontal scaling implies distributed systems.

High availablity :-

-High availability usually goes hand - in - hand with horizontal scalability
-High availablity means running your application / system in atleast 2 availability zones.


Scalability vs Elasticity vs Agility :-

1)Scalability : ability to accomodate a larger load by making the hardware stronger (sclae up) or by adding nodes (scale out)
-Vertical Scaling = Scale Up
-Horizontal Scaling = Scale Out

2)Elasticity : 
-Once a system is scalable,elasticity means there will be some "auto-scaling" so that the system can scale based on the load.This is cloud friendly : pay-per-use , match demand , optimize costs.

3)Agility: (not related to scalability - distractor) new IT resources are only a click away , which means you reduce the time to make those resources available to your developers from weeks to within minutes.


What is load balancing :-

-Load balancers are servers that forward internet traffic to multiple servers(EC2 instances) downstream.
-Spread load accross multiple instances downstream.
-Expose a single point of access to your application
-Seamlessly handle failures of downstream instances.
-Provide SSL termination (HTTPS) for your websites.
-High availablity accross multiple AZ's.

Elastic Load Balancer (ELB) :-

-An ELB is a managed load balancer.
-AWS gaurantees that it will be working and takes cares of upgrades, maintainance , high availablity .
-AWS provided only a few configuration knobs.

4 Types Of Load Balancers offered by AWS :

-Application load balancer (HTTP / HTTPS / gRPC only) - Layer 7
-Network Load Balancer (ultra high performance , allows for TCP and UDP) - Layer 4
-Gateway load balancer - layer 3
-Classic load balancer(retired in 2023)  - Layer 4 and 7 (this was retired and newly launched were ALB and NLB)

Difference -

Application Load Balancer(ALB) : HTTP/HTTPS/gRPC protocols (Layer 7).HTTP routing features.Statis DNS(URL).Users hit one of these protocols.

Network Load Balancer : 

-TCP/UDP Protocols(Layer 4).
-High performance --- millions of requests per second.
-Static IP through elastic IP.an Elastic IP address remains static. It does not change until you release it.

Gateway Load Balancer :- 

-GENEVE(Generic Network Virtualization Encapsulation) Protocol on IP packets(Layer 3).
-Route traffic to firewalls that you manage on EC2 instances.
-Intrusion detection
-This is also called 3rd party security virtual appliances because it is used to do analysis.
-The traffic is first sent to GWLB, then it is forwaded to 3rd party security virtual appliances , after analyzing it is sent back to GWLB and GWLB directs traffic to EC2.

Listeners and Routing in AWS :-

-A listener is a process that checks for connection requests using the port and protocol you configure .
-The rules that you define for a listener determines how the lead balancer routes requests to its registered targets.
-We have to create a target group in listeners and routing section .A group of EC2 instances.
-We can also create a group of Lambda functions , or even Application load balancers.
-All the instances in your group will be on same port (80) in case of http.
-Here targets are basically instances.


What is an auto scaling group :-

-In real-life , the load on your websites and applications can change.
-The goal of ASG is to:
1)Scale out(add EC2 instances) to match an increased load.
2)Scale in (remove ec2 instances) to macth decreased load.
3)Ensures we have an minimum and maximum number of machines running
4)Automatically registers new instances to load balancer
5)Replace unhealthy instances


While creating an Auto Scaling Group :-

1)We create an launch template.It is the same as launching a ec2 instance.We basically configure our EC2 here.
2)Then we setup our VPC and we choose in what all availability zones and subnets your auto scaling groups.
3)Then we select our load balancer.
4)Configure Group Size And Scaling.Here we define desired capacity ,the minimum desired capacity , maximum desired capacity.For eg. 2,1,4
5)The autoscaling groups can automatically replace unhealthy instances.They are integrated with load balancers so they automatically can add target instances to load balancers.

Auto-scaling groups scaling strategies :-

Desired Capacity: The number of instances you want to maintain in the group.
Minimum Capacity: The minimum number of instances the group can scale down to.
Maximum Capacity: The maximum number of instances the group can scale up to.

-so if the load is less on desired capacity so it will basically scale down to minimum capacity and if load grows from desired capacity then it scales out to maxiomum capacity.
-When there is no scaling event (e.g., no alarms triggered or no scheduled scaling), the group will maintain the number of instances equal to the desired capacity.

1)Manual Scaling :-  update the size of an asg manually
2)Dynamic Scaling :- To respond to changing demand
    A)Simple / step scaling : When a CloudWatch alarm is triggered (example CPU > 70%) then add 2 units
                              When a CloudWatch alarm is triggered (example CPU < 30%) then add 2 units
    B)Target Tracking Scaling : Eg. I want the average ASG CPU to stay at around 40%
    C)Scheduled Scaling : Anticipate a scaling based on known usage patterns.
                          -For eg. Increase the minimum capacity to 10 at 5Pm on fridays.
    D)Predictive Scaling : Using machine learning to predict future traffic ahead of time
                           Useful when your load has predictable time based patterns.

-In order to terminate a auto scaling group , we cannot terminate instances..becuase a auto scaling group will start them again.We have to delete the entire auto scaling group for that.

What exactly is a target group?
-a target group can indeed be a group of EC2 instances, but it is more accurate to think of it as a logical grouping of "targets" (which can include EC2 instances, IP addresses, or Lambda functions) that the load balancer routes traffic to.
-The target group does not directly start or stop EC2 instances. It is simply a logical grouping of targets for the load balancer.

AWS Simple Storage Service (S3) :-

-Amazon S3 allows people store objects(files) in buckets (directories).
-Buckets must have globally unique name (accross all regions all accounts).
-Buckets are defined in a region level.
-S3 looks like a global services..but it is a regional construct.
-Naming convention :-
  1)No uppercase , No underscore
  2)3-63 characters long
  3)Not an IP
  4)Must start with lowercase letter or number
  5)Must not start with the prefix xn--
  6)Must not end with suffix -s3alias

-Amazon S3 objects(files) have keys.
-These keys are full path of the object.eg.s3://my-bucket/my_file.txt
-The key is composed of prefix + object name
                        s3://my-bucket/prefix->my_folder/objectname->my_file.txt
-There is no concept of directories within buckets (although the UI will trick you to think otherwise).

Amazon S3 Content : 

-When someone says "object values are the content of the body in Amazon S3", they are referring to the actual data stored inside an S3 object.
-Max obejct size is 5000GB(5TB).
-If uploading more than 5GB , must use "multi-part upload".
-Metadata (list of text key/value paris - system or user metadata).
-Tags(unicode key / value pair - up to 10) - useful for security / lifecycle
-Version ID (versioning is enabled).

Amazon S3 Security :

User-Based
  • IAM Policies – which API calls should be allowed for a specific user from IAM
Resource-Based
  • Bucket Policies – bucket wide rules from the S3 console - allows cross account
  • Object Access Control List (ACL) – finer grain (can be disabled)
  • Bucket Access Control List (ACL) – less common (can be disabled)
• Note: an IAM principal can access an S3 object if
  •  The user IAM permissions ALLOW it OR the resource policy ALLOWS it
  •  AND there’s no explicit DENY
• Encryption: encrypt objects in Amazon S3 using encryption keys

S3 Bucket Policies :
• JSON based policies 
  • Resources: buckets and objects 
  • Effect: Allow / Deny 
  • Actions: Set of API to Allow or Deny 
  • Principal: The account or user to apply the policy to

• Use S3 bucket for policy to: • Grant public access to the bucket 
• Force objects to be encrypted at upload 
• Grant access to another account (Cross Account)

Examples : 

1)Public Access : Use Bucket Policy
-Add a bucket policy to bucket to allow public access
-Then public can access it
2)User Access to S3 : IAM user (same account)
-We can IAM permissions on the user.Then user can access the S3 bucket
3)EC2 instance access - Use IAM Roles
-we create a EC2 instance role with correct IAM permissions on the instance
4)Cross Account Access : use bucket policy
-We create a bucket policy that allows cross account.
-This way iam user from other account can access our bucket.

*** - Evene if you have policies which allow certain public users , and if you have Block public access (bucket settings) turned 'On' then no public users can access your bucket even if they are allowed in policies.

Amazon S3 : static website hosting

-S3 can host static websites and have them accessible on internet.
-The website URL will be depending on the region
-If we get 403 forbidden error after trying to access anything in bucket..that means you have your 
Block public access (bucket settings) turned 'On'.


AWS S3 versioning : 

-You can version your files in amazon s3
-It is enabled at the bucket level.
-Same key if overwrite will change the version "version":1..2..3...
-It is a best practice to version your buckets
-Protects against unintended deletes
-Easy rollback to previous version

Notes : any file that is not versioned prior to enabling version will have version 'null'
-Susspending versioning does not delete the previous versions.

-We can add 'delete marker' and delete a object from S3.It is not permanently deleted.It can be restored.


Amazon S3 Replication (CRR & SRR) :-

1)Cross Region Replication (CRR)
2)Same Region Replication (SRR)

-Must enable versioning source and destinatiuon buckets
-Cross region replication(CRR)
-Same Region Replication (SRR)
-Buckets can be in different aws accounts
-Copying is asynchronous
-Must give proper IAM permissions to S3
-Must give proper IAM permissdions to S3

Use case :
1)CRR - compliance , lower latency , replication accross accounts
2)SRR - log aggregation , live replication between production and test accounts.


Amazon S3 Storage Classes :-

1)Amazon S3 standard - General Purpose
2)Amazon S3 Standard Infrequent Access (IA)
3)Amazon S3 One Zone - Infrequent Access
4)Amazon S3 Glacier Instant Retrieval
5)Amazon S3 Glacier Flexible Retrieval
6)Amazon S3 Glacier Deep Archieve
7)Amazon S3 Intelligent Tiering

-Can move between classes manually or using S3 lifecycle configurations

S3 Durability and Availability :-

-High durability (99.9999999% || 9s) of objects accross multiple AZs.
-This durability is same for all storage classes 

Availability : measures how readily available a service is.
-Varies depending on storage class.
-Example : S3 Standard has 99.99% availability = not available 53 minutes a year

1)Amazon S3 standard - General Purpose :-

• 99.99% Availability
• Used for frequently accessed data
• Low latency and high throughput
• Sustain 2 concurrent facility failures
• Use Cases: Big Data analytics, mobile & gaming applications, content
distribution.

2)S3 Storage Classes – Infrequent Access :-

• For data that is less frequently accessed, but requires rapid access when needed
• Lower cost than S3 Standard but you will have a cost on retrieval

• Amazon S3 Standard-Infrequent Access (S3 Standard-IA)
• 99.9% Availability
• Use cases: Disaster Recovery, backups

• Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)
• High durability (99.999999999%) in a single AZ; data lost when AZ is destroyed
• 99.5% Availability
• Use Cases: Storing secondary backup copies of on-premise data, or data you can recreate

3)Amazon S3 Glacier Storage Classes

• Low-cost object storage meant for archiving / backup
• Pricing: price for storage + object retrieval cost

• Amazon S3 Glacier Instant Retrieval
• Millisecond retrieval, great for data accessed once a quarter
• Minimum storage duration of 90 days
--You will be charged for storing an object for at least 90 days, even if you delete it earlier.

• Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):
• Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free
• Minimum storage duration of 90 days
-You will be charged for storing an object for at least 90 days, even if you delete it earlier.

• Amazon S3 Glacier Deep Archive – for long term storage:
• Standard (12 hours), Bulk (48 hours)
• Minimum storage duration of 180 days
-You will be charged for storing an object for at least 180 days, even if you delete it earlier.

S3 Intelligent-Tiering
• Small monthly monitoring and auto-tiering fee
• Moves objects automatically between Access Tiers based on usage
• There are no retrieval charges in S3 Intelligent-Tiering
• Frequent Access tier (automatic): default tier
• Infrequent Access tier (automatic): objects not accessed for 30 days
• Archive Instant Access tier (automatic): objects not accessed for 90 days
• Archive Access tier (optional): configurable from 90 days to 700+ days
• Deep Archive Access tier (optional): config. from 180 days to 700+ days

S3 Lifecycle Policy :-

-We can set rules to change the S3 tiering after certain days.For eg . after 30 days go to  transition storage class from frequent access to standard IA etc.

S3 encryption :-

1)Server Side Encryption :-

-Server encrypts the file after receiving it.

2)Client Side Encryption :-

-User encrypts the file before uploading it.

IAM Access analyzer for S3 :-

Ensures that only intended people have access to your S3 buckets
• Example: publicly accessible bucket, bucket shared with other AWS account…
• Evaluates S3 Bucket Policies, S3 ACLs, S3 Access Point Policies
• Powered by IAM Access Analyzer

-IAM access analyzer helps you find resources in your account that are shared with other entities.

AWS Snowball :-

Highly-secure, portable devices to collect and process data at the
edge, and migrate data into and out of AWS
• Helps migrate up to Petabytes of data
-It is a physical device that you get from aws.
-Data migrations are bandwidth heavy so you get that snowball physical device

Job Types in Snowball :-

1)Import into amazon s3 :-
-aws will ship an empty device to you for storage and compute workloads.You'll transfer your data onto it and ship it back.After aws gets it your data will be moved.

2)Export from amazon s3 :-

-Choose what data you want to export from your s3 buckets for storage and compute workloads.AWS will load that data into snowball and ship it to you.When you are done ship the device back for erasing.

3)Local compute and storage only :-
-For edge computing.

1)Snowball Edge Storage Optimized 104 vCPUs 416 GB 210 TB
2)Snowball Edge Compute Optimized 104 vCPUs 416 GB 28 TB

What is Edge Computing?

• Process data while it’s being created on an edge location
• A truck on the road, a ship on the sea, a mining station underground...
• These locations may have limited internet and no access to computing power
• We setup a Snowball Edge device to do edge computing
• Snowball Edge Compute Optimized (dedicated for that use case) & Storage Optimized
• Run EC2 Instances or Lambda functions at the edge
• Use cases: preprocess data, machine learning, transcoding media

Snowball Edge Pricing
• You pay for device usage and data transfer out of AWS
• Data transfer IN to Amazon S3 is $0.00 per GB
• On-Demand
• Includes a one-time service fee per job, which includes:
• 10 days of usage for Snowball Edge Storage Optimized 80TB
• 15 days of usage for Snowball Edge Storage Optimized 210TB
• Shipping days are NOT counted towards the included 10 or 15 days
• Pay per day for any additional days
• Committed Upfront
• Pay in advance for monthly, 1-year, and 3-years of usage (Edge Computing)
• Up to 62% discounted pricing

Hybrid Cloud Storage :-

S3 is a proprietary storage technology (unlike EFS / NFS), so how do you
expose the S3 data on-premise?
• AWS Storage Gateway!
-Bridge between on-premise data and cloud data in S3
• Hybrid storage service to allow on- premises to seamlessly use the AWS
Cloud 
-The AWs storage gateway uses Amazon EBS , S3 and glacier behind the scenes.

***-OpsHub is a desktop application that you get to manage snow family devices and add data onto them.

Databases and shared responsibility model :

Operating System Patching is handled by AWS
• Monitoring, alerting
• Note: many databases technologies could be run on EC2, but you must
handle yourself the resiliency, backup, patching, high availability, fault
tolerance, scaling… 

Amazon RDS :- 

-Amazon relational database service
-RDS stands for Relational Database Service
• It’s a managed DB service for DB use SQL as a query language.
• It allows you to create databases in the cloud that are managed by AWS
• Postgres
• MySQL
• MariaDB
• Oracle
• Microsoft SQL Server
• IBM DB2
• Aurora (AWS Proprietary database)

Advantage over using RDS versus deploying
DB on EC2 :-

• RDS is a managed service:
• Automated provisioning, OS patching
• Continuous backups and restore to specific timestamp (Point in Time Restore)!
• Monitoring dashboards
• Read replicas for improved read performance
• Multi AZ setup for DR (Disaster Recovery)
• Maintenance windows for upgrades
• Scaling capability (vertical and horizontal)
• Storage backed by EBS
• BUT you can’t SSH into your instances

RDS Solution Architecture :-

-Elastic Load balancer to EC2 instances to amazon RDS for read/write.

Amazon Aurora :-

Aurora is a proprietary technology from AWS (not open sourced)
• PostgreSQL and MySQL are both supported as Aurora DB
• Aurora is “AWS cloud optimized” and claims 5x performance improvement
over MySQL on RDS, over 3x the performance of Postgres on RDS
• Aurora storage automatically grows in increments of 10GB, up to 128 TB
• Aurora costs more than RDS (20% more) – but is more efficient
• Not in the free tier

it comes in two types :-

1)AWS Aurora Provisioned : You provision the instance size and capacity up front
2)AWS Aurora Serverless

AWS Aurora Serverless :-

Automated database instantiation and
auto
-scaling based on actual usage
• PostgreSQL and MySQL are both
supported as Aurora Serverless DB
• No capacity planning needed • Least management overhead • Pay per second, can be more cost
- effective
• Use cases: good for infrequent,
intermittent or unpredictable
workloads…

***AWS Aurora Serverles is a AWS service with no management overhead

AWS RDS has snapshot feature to take a snapshot of data and create new database out of it.
-You can also share this snapshot.

RDS Deployment : Read Replicas , Multi -AZ

-Read Replicas:
• Scale the read workload of your DB
• Can create up to 15 Read Replicas
• Data is only written to the main DB

Multi-AZ:
• Failover in case of AZ outage (high availability)
• Data is only read/written to the main database
• Can only have 1 other AZ as failover

**There is a multi region deployment as well where we can deploy a read only replica in a deifferent region.If the application wants to write something in database..it has to write it to the original region where the original rds is deployed.

Amazon ElastiCache Overview:-

• The same way RDS is to get managed Relational Databases…
• ElastiCache is to get managed Redis or Memcached
• Caches are in-memory databases with high performance, low latency
• Helps reduce load off databases for read intensive workloads
• AWS takes care of OS maintenance / patching, optimizations, setup,
configuration, monitoring, failure recovery and backups

-AnyTime something comes up like in-memory database..think of ElastiCache.


DynamoDB : 

1)NoSQL Database
2)Serverless
3)Single digit - Millisecond latency
4)Key value database
5)Partition key determines where the item is stored
6)All items with the same partition key value go to the same bin.

DynamoDB Accelerator - DAX :-

Fully Managed in-memory cache for
DynamoDB
• 10x performance improvement – single- digit millisecond latency to microseconds
latency – when accessing your DynamoDB
tables
• Secure, highly scalable & highly available
• Difference with ElastiCache at the CCP
level: DAX is only used for and is
integrated with DynamoDB, while
ElastiCache can be used for other
databases


DynamoDB – Global Tables
• Make a DynamoDB table accessible with low latency in multiple-regions
• Active-Active replication (read/write to any AWS Region)

-We can here create a global table in two or more different regions and users can actively read and write to table at the same time the data is replicated accross all regions.

Redshift Overview
• Redshift is based on PostgreSQL, but it’s not used for OLTP
• It’s OLAP – online analytical processing (analytics and data warehousing)
Columnar storage of data (instead of row based)
Has a SQL interface for performing the queries
• BI tools such as AWS Quicksight or Tableau integrate with it

Redshift Serverless
• Automatically provisions and scales data warehouse underlying capacity
• Run analytics workloads without managing data warehouse infrastructure
Use cases: Reporting, dashboarding applications, real-time analytics…

Amazon EMR
• EMR stands for “Elastic MapReduce”
• EMR helps creating Hadoop clusters (Big Data) to analyze and process
vast amount of data
• The clusters can be made of hundreds of EC2 instances
Use cases: data processing, machine learning, web indexing, big
data…

Amazon Athena
• Serverless query service to analyze data stored in Amazon S3
• Uses standard SQL language to query the files
• Supports CSV, JSON, ORC, Avro, and Parquet (built on Presto)
-Sends data to amazon quicksight which is like dashboards
Exam Tip: analyze data in S3 using serverless SQL, use Athena


Amazon QuickSight
• Serverless machine learning-powered business intelligence service to
create interactive dashboards
Integrated with RDS, Aurora,
Athena, Redshift, S3…

DocumentDB
• Aurora is an “AWS-implementation” of PostgreSQL / MySQL …
• DocumentDB is the same for MongoDB (which is a NoSQL database)
DocumentDB storage automatically grows in increments of 10GB
• Automatically scales to workloads with millions of requests per seconds

Amazon Neptune:-

• Fully managed graph database • A popular graph dataset would be a social network

Amazon Timestream:-

• Fully managed, fast, scalable, serverless time
series database
-Its like date and time.Date evolves with time and the data like this is stored in time series database.

Amazon QLDB
• QLDB stands for ”Quantum Ledger Database”
• A ledger is a book recording financial transactions
• Fully Managed, Serverless, High available, Replication across 3 AZ
• Used to review history of all the changes made to your application data over time
• Immutable system: no entry can be removed or modified, cryptographically verifiable

Amazon Managed Blockchain:-

• Blockchain makes it possible to build applications where multiple parties
can execute transactions without the need for a trusted, central
authority.
-Compatible with the frameworks Hyperledger Fabric & Ethereum

AWS Glue :-

• Managed extract, transform, and load (ETL) service • Useful to prepare and transform data for analytics • Fully serverless service

DMS – Database Migration Service
• Quickly and securely migrate databases
to AWS, resilient, self healing
• The source database remains available
during the migration
• Supports:
• Homogeneous migrations: ex Oracle to
Oracle
• Heterogeneous migrations: ex Microsoft
SQL Server to Aurora

Q.Which aws service is always serverless and uses SQL :-

-AWS Athena. (Remember its not aurora because it can be provisioned and is not strictly serverless)

q.What is the name of a central repository to store structural and operational metadata for data assets in AWS Glue?
-Glue data catalog

q.RDS multi AZ deployments main purpose is high availability , and RDS read replicas main purpose is scalability?

-True.Also Multi region deployments main purpose is disaster recovery and local performance.


What is Docker?
• Docker is a software development platform to deploy apps
• Apps are packaged in containers that can be run on any OS
• Apps run the same, regardless of where they’re run
• Any machine
• No compatibility issues
• Predictable behavior
• Less work
• Easier to maintain and deploy
• Works with any language, any OS, any technology
• Scale containers up and down very quickly (seconds)

Where Docker images are stored? 
• Docker images are stored in Docker Repositories
• Public: Docker Hub https://hub.docker.com/ 
• Find base images for many technologies or OS: • Ubuntu • MySQL • NodeJS, Java… 
• Private: Amazon ECR (Elastic Container Registry)

ECS
• ECS = Elastic Container Service
• Launch Docker containers onAWS
• You must provision & maintain the infrastructure (the EC2instances)
• AWS takes care of starting /stopping containers
• Has integrations with the Application Load Balancer

-The ECS service is smart enough to know which ec2 instance to place the new docker container which you add.

Fargate 
• Launch Docker containers on AWS
• You do not provision the infrastructure (no EC2 instances to manage)
– simpler!
• Serverless offering 
• AWS just runs containers for you based on the CPU / RAM you need

ECR:-
• Elastic Container Registry
• Private Docker Registry on AWS
• This is where you store your Docker images so they can be run by ECS or Fargate

Amazon EKS
• EKS = Elastic Kubernetes Service
• Allows you to launch managed Kubernetes clusters on AWS
• Kubernetes is an open-source system for management, deployment, and scaling of containerized apps (Docker)
• Containers can be hosted on:
• EC2 instances
• Fargate (Serverless)
• Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP…)

What’s serverless?
• Serverless is a new paradigm in which the developers don’t have to manage servers anymore…
• They just deploy code
• They just deploy… functions !

Why AWS Lambda ?

EC2 : 
• Virtual Servers in the Cloud
• Limited by RAM and CPU
• Continuously running
• Scaling means intervention to add / remove servers

AWS Lambda 
• Virtual functions – no servers to manage!
• Limited by time - short executions
• Run on-demand
• Scaling is automated! 

Benefits of AWS Lambda
• Easy Pricing:
• Pay per request and compute time
• Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time
• Integrated with the whole AWS suite of services
• Event-Driven: functions get invoked by AWS when needed (This makes lambda a reactive type of service)
• Integrated with many programming languages
• Easy monitoring through AWS CloudWatch (Monitering service)

Lambda Container Image
• The container image must implement the Lambda Runtime API
• ECS / Fargate is preferred for running arbitrary Docker images

Use case : Serverless thumbnail creation / Serverless CRON Job which triggers on cloudwatch alarm

Payment for lambda : -
1)Pay per calls
2)Pay per duration

-Lambda function has to be given a xecution role which is same as a IAM role given to ec2 instances.
-Cloudwatch can be used to monitor lambda functions.


Amazon API gateway :-

Example: building a serverless API
-Fully managed service for developers to easily create, publish, maintain,
monitor, and secure APIs
• Serverless and scalable
• Supports RESTful APIs and WebSocket APIs
• Support for security, user authentication, API throttling, API keys, monitoring... 

AWS Batch
• Fully managed batch processing at any scale
• Efficiently run 100,000s of computing batch jobs on AWS
• A “batch” job is a job with a start and an end (opposed to continuous)
• Batch will dynamically launch EC2 instances or Spot Instances
• AWS Batch provisions the right amount of compute / memory
• You submit or schedule batch jobs and AWS Batch does the rest!
• Batch jobs are defined as Docker images and run on ECS
• Helpful for cost optimizations and focusing less on the infrastructure

Batch vs Lambda:-
• Lambda:
• Time limit 
• Limited runtimes 
• Limited temporary disk space 
• Serverless 

• Batch:
• No time limit 
• Any runtime as long as it’s packaged as a Docker image 
• Rely on EBS / instance store for disk space 
• Relies on EC2 (can be managed by AWS)

Amazon Lightsail
• Virtual servers, storage, databases, and networking
• Low & predictable pricing
• Simpler alternative to using EC2, RDS, ELB, EBS, Route 53…
• Great for people with little cloud experience!
• Can setup notifications and monitoring of your Lightsail resources
• Use cases:
• Simple web applications (has templates for LAMP, Nginx, MEAN, Node.js…)
• Websites (templates for WordPress, Magento, Plesk, Joomla)
• Dev / Test environment
• Has high availability but no auto-scaling, limited AWS integrations


What is CloudFormation
• CloudFormation is a declarative way of outlining your AWS
Infrastructure, for any resources (most of them are supported).
• For example, within a CloudFormation template, you say:
• I want a security group
• I want two EC2 instances using this security group
• I want an S3 bucket
• I want a load balancer (ELB) in front of these machines
• Then CloudFormation creates those for you, in the right order, with the
exact configuration that you specify

Infrastructure as code
• No resources are manually created, which is excellent for control
• Changes to the infrastructure are reviewed through code
• Cost
• Each resources within the stack is tagged with an identifier so you can easily see how
much a stack costs you
• You can estimate the costs of your resources using the CloudFormation template
• Savings strategy: In Dev, you could automation deletion of templates at 5 PM and
recreated at 8 AM, safely

-From exam perspective , cloudformation is going to be used when we have infrastructure as a code , when we need to repeat an architecture in different environments,different regions or in even different aws accounts.

-Infrastructure Composer gives us visual understanding of our templates(Like a chart / diagram).
-EIP is Elastic Internet Protocol

CloudFormation + Infrastructure Composer
• Example: WordPress CloudFormation Stack
• We can see all the resources
• We can see the relations between the components

• Define your cloud infrastructure using a familiar language:
• JavaScript/TypeScript, Python, Java, and .NET
• The code is “compiled” into a CloudFormation template (JSON/YAML)
• You can therefore deploy infrastructure and application runtime code together
• Great for Lambda functions
• Great for Docker containers in ECS / EKS

AWS Elastic BeanStalk :-

-In aws we follow 3 tier architechture.
-Client -> ELB -> EC2 instances managed by ASG -> RDS and Elasticache

-Developer problems on AWS
• Managing infrastructure
• Deploying Code
• Configuring all the databases, load balancers, etc
• Scaling concerns
• Most web apps have the same architecture (ALB + ASG)
• All the developers want is for their code to run!
• Possibly, consistently across different applications and environments

AWS Elastic Beanstalk Overview:-
• Elastic Beanstalk is a developer centric view of deploying an application on AWS
• It uses all the component’s we’ve seen before: EC2, ASG, ELB, RDS, etc…
• But it’s all in one view that’s easy to make sense of! 
• We still have full control over the configuration 
• Beanstalk = Platform as a Service (PaaS) 
• Beanstalk is free but you pay for the underlying instances

-Managed service:-

• Instance configuration / OS is handled by Beanstalk 
• Deployment strategy is configurable but performed by Elastic Beanstalk 
• Capacity provisioning 
• Load balancing & auto-scaling 
• Application health-monitoring & responsiveness

Elastic Beanstalk – Health Monitoring :-

• Health agent pushes metrics to CloudWatch
•Checks for app health, publishes health events

Elastic Beanstalk Hands on :-

-We have to configure the service  access for elastic beanstalk.This is basically giving IAM roles to beanstalk.


AWS CodeDeploy :-

-Codedeploy lets you deploy your code.
-If we want to move from appllication version 1 to application version 2..codedeploy will find a way to do it.
-We want to deploy our application automatically
• Works with EC2 Instances
• Works with On-Premises Servers
• Hybrid service
• Servers / Instances must be provisioned and configured ahead of time with the CodeDeploy Agent

AWS CodeCommit :-

• Before pushing the application code to servers, it needs to be stored somewhere
• Developers usually store code in a repository, using the Git technology
• A famous public offering is GitHub, AWS’ competing product is
CodeCommit
• CodeCommit:
• Source-control service that hosts Git-based repositories
• Makes it easy to collaborate with others on code
• The code changes are automatically versioned 

AWS CodeBuild
• Code building service in the cloud (name is obvious)
• Compiles source code, run tests, and produces packages that are ready to
be deployed (by CodeDeploy for example)
-Pulls code from codecommit , Compiles source code, run tests, and produces packages that are ready to be deployed like a 'Ready To Deply Artifact'.And this can be deployed using codedeploy.

Benefits:
• Fully managed, serverless
• Continuously scalable & highly available
• Secure
• Pay-as-you-go pricing – only pay for the build time

AWS CodePipeline
• Orchestrate the different steps to have the code automatically pushed to production
• Code => Build => Test => Provision => Deploy
• Basis for CICD (Continuous Integration & Continuous Delivery)
Fully managed, compatible with CodeCommit, CodeBuild, CodeDeploy, Elastic Beanstalk,
CloudFormation, GitHub, 3rd-party services (GitHub…) & custom plugins…

AWS CodeArtifact
• Software packages depend on each other to be built (also called code dependencies), and new ones are created
• Storing and retrieving these dependencies is called artifact management
-Developers and CodeBuild can then retrieve dependencies straight from
CodeArtifact

AWS Systems Manager (SSM)
• Helps you manage your EC2 and On-Premises systems at scale
• Another Hybrid AWS service
-Fleet manager is a place where all the EC2 instances with SSM agent appear

Most important features are:
• Patching automation for enhanced compliance
• Run commands across an entire fleet of servers
• Store parameter configuration with the SSM Parameter Store

-SSM installs its ssm agents on all the EC2 instances and manages all patching compiance and running of commands on all the instances.It is hybrid

Systems Manager – SSM Session Manager
• Allows you to start a secure shell on your EC2 and on-premises servers
• No SSH access, bastion hosts, or SSH keys needed
• No port 22 needed (better security)

Systems Manager Parameter Store :-

• Secure storage for configuration and secrets
• API Keys, passwords, configurations…
• Serverless, scalable, durable, easy SDK
• Control access permissions using IAM
• Version tracking & encryption (optional)

Q.What is called the declaration of the AWS resources that make up a stack?

-Cloudformation Templates

Why make a global application?
• A global application is an application deployed in multiple geographies
• On AWS: this could be Regions and / or Edge Locations
-For decreased latency , disaster recovery and attack protection.

Global Applications in AWS :-

• Global DNS: Route 53
• Great to route users to the closest deployment with least latency
• Great for disaster recovery strategies

• Global Content Delivery Network (CDN): CloudFront
• Replicate part of your application to AWS Edge Locations – decrease latency
• Cache common requests – improved user experience and decreased latency

• S3 Transfer Acceleration
• Accelerate global uploads & downloads into Amazon S3

• AWS Global Accelerator:
• Improve global application availability and performance using the AWS global network


Amazon Route 53 Overview
• Route53 is a Managed DNS (Domain Name System)
• DNS is a collection of rules and records which helps clients understand how to reach a server through URLs.

Route 53 Routing Policies :-

1)Simple Routing Policy :- (No Health Checks)

-Client sends request to Domain name
-Route 53 replies with a IPv4

2)Weighted Routing Policy :-

-Instances with same application are distributed with weights.
For eg. If a ec2 instance has 70% weight...70% of the users will go to this instance.
-Its like some kind of load balacning
-In this weighted routing policy we can have health checks

3)Latency Routing Policy :-

-Will route users closest to a instance.Based on minimal latency users will be redirected.
-This also has a health check.

4)Failover Routing Policy :-

-Route 53 knows which instance to connect to based on the health of that instance.
-Ifwe have a primary and secondary server..it will perform health checks on primary..if primary is not healthy it will redirect traffic to secondary.

Amazon CloudFront
• Content Delivery Network (CDN)
• Improves read performance,content is cached at the edge
• Improves users experience
• Many Points of Presence globally (Edge Locations, Edge Caches…)
• DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall

CloudFront Origins :-

-Cloudfront has several origins which are backends for which we want to connect cloudfront to.

CloudFront – Origins :-

• S3 bucket 
• For distributing files and caching them at the edge 
• For uploading files to S3 through CloudFront 
• Secured using Origin Access Control (OAC) 

• VPC Origin :-
• For applications hosted in VPC private subnets 
• Application Load Balancer / Network Load Balancer / EC2 Instances 

• Custom Origin (HTTP) :-
• S3 website (must first enable the bucket as a static S3 website) 
• Any public HTTP backend you want

CloudFront vs S3 Cross Region Replication:-

• CloudFront:
CloudFront is a Content Delivery Network (CDN) that caches content closer to the user using edge locations. This is ideal for static content, such as:
1)HTML, CSS, JavaScript files
2)Images, videos
3)PDFs, static web assets
• Global Edge network
• Files are cached for a TTL (maybe a day)
• Great for static content that must be available everywhere

• S3 Cross Region Replication:-S3 CRR is designed to replicate objects to another S3 bucket in a different region, automatically and asynchronously.

• Must be setup for each region you want replication to happen
• Files are updated in near real-time
• Read only
• Great for dynamic content that needs to be available at low-latency in few regions

S3 Transfer Acceleration
• Increase transfer speed by transferring file to an AWS edge location
which will forward the data to the S3 bucket in the target region


AWS Global Accelerator :-

• Improve global application availability and performance using the AWS global network
• Leverage the AWS internal network to optimize the route to your application (60% improvement)
• 2 Anycast IP are created for your application and traffic is sent through Edge Locations
• The Edge locations send the traffic to your application
-Suppose we have a server in india.Users from all over the world wants to access our website hostedf on server.Then AWS Global accelerator will find nearest edge location and this edge location will redirect to server in india with aws private network.

AWS Outposts :-

-Hybrid Cloud
-AWS Outposts are “server racks” that offers the  same AWS infrastructure, services, APIs & tools  to build your own applications on
-premises just as in the cloud
• AWS will setup and manage “Outposts Racks”  within your on
-premises infrastructure and you can start leveraging AWS services on-premises
• You are responsible for the Outposts Rack physical security

AWS Wavelength :-

-If 5G in question , it is aws wavelength
-WaveLength Zones are infrastructure deployments embedded within the telecommunications providers’ datacenters at the edge of the 5G networks
• Brings AWS services to the edge of the 5G networks
• Example: EC2, EBS, VPC
-Gives ultra low latency for people who use 5G networks.

AWS Local Zones :-

- Places AWS compute, storage, database, and other selected AWS services closer to end users to run latency sensitive applications
• Extend your VPC to more locations – “Extension of an AWS Region”

Global Applications Architecture:-

1)Single Region , Single AZ :-
-No High Availability
-High Global Latency
-Less Difficulty

2)Single Region , Multi AZ :-
-High Availability
-High Global Latency
-Medium difficulty

3)Multi-region , Active Passive :-
-Here Instance from one region is active , meaning where we can do both read/write.
-A instance from passive region is which can be only used to read but not write.This way latency is reduced for read operations but for write operations latency is still high.

4)Multi-Region , Active Active :-
-Ec2 instances from both the regions can be used to read/write.
-Less latency but high difficulty managing.

Q.Which of the following statements is NOT a reason for a global application?

Decreased Latency?
-A global application can be used to decrease latency.Because if users from africa,we have a deployment in africa so they have low latency.So this is a reason

Distaser Recovery?
-Yes.If we have global application then we have mutli region deployments which means disaster at one place cannot affect our system.So this is a region

Scale elastically on demand?
-A global applicaiton is not specifically used to scale on demand as we have ASG groups for that.So this is not a reason.THis is the correct option

Attack Protection?
-A global application can be used for attack protection because it has ddos firewalls.


Q.With which services does CloudFront integrate to protect against web attacks?

-You can use AWS WAF(Web Application Firewall) Web Access control lists(web ACLs) to help minimize Distributed Denail of service attack.FOr advanced protection aws also offers AWS Shield Standard and AWS Shield Advanced.


Cloud Integration Section :-

When we start deploying multiple applications, they will inevitably need 
to communicate with one another
• There are two patterns of application communication :-
1) Synchronous communications (application to application)
2) Asynchronous / Event based  (application to queue to application)

AWS SQS (Simple Queue Service) :-

-Amazon SQS can Send,Store,Receive messaged between software components at any volumne.
-THis is without loosing messages or without needing support of any other services.
-The data contained within a message is called a payload and is protected until delivery.
-SQS queues is the place where messaged are stored before they are processed.
-AWS manages the underlying infrastucture to host those queues.
-These scale automatically,are reliable and easy to configure and use.
-This is used to build a decoupled architecture which will not cause cascading failures if a single component fails.
-No limit to how many messages can be in the queue
• Messages are deleted after they’re read by consumers
-Consumers share the work to read messages & scale horizontally

For eg.If we have a ASG where users make requests for video processing , it will send requests to a SQS queue.
-And this SQS queue will send the requests to another ASG which will in turn process the videos.
-Best part here is based on the messages in the queue , we can setup the scaling for ASG of video processing instances.
-It is FIFO.
-It is a high throughput , system to system messaging service.
-Messages get deleted after they are processed.

Amazon Kinesis Data Streams :-

• For the exam: Kinesis = real-time big data streaming
• Managed service to collect, process, and analyze real-time streaming 
data at any scale
Amazon Kinesis Data Streams: low latency streaming to ingest data at scale from hundreds of thousands of sources
• Amazon Data Firehose: load Kinesis Data Streams into Amazon S3, Redshift, 
OpenSearch, etc…

Amazon SNS

Amazon Simple Notification Service(SNS):-

-Amazon SNS is similar , it is used to send out messages to services,but it can also send out notifications to end users.
-It does it in a different way, in  a publish-subscribe or Pub-Sub model.
-This means you can create something called as SNS topic which is just a channel for messages to be delivered.
-You then configure subscribers to that topic and finally publish those messages for those subscribers.
-With just one go you can send a message in the channel and it will go to all its subscribers.
-These subscribers can also be end points.Such as SQS queues , AWS lambda functions or HTTP/HTTPS web hooks.
-If any time in exam you find subscribers , topics , publish subscribe , its SNS.

Amazon MQ :-

Amazon MQ
• SQS, SNS are “cloud-native” services: proprietary protocols from AWS
• Traditional applications running from on-premises may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS
• When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ 
• Amazon MQ is a managed message broker service for
1)RabbitMQ
2)ActiveMQ

Amazon MQ doesn’t “scale” as much as SQS / SNS
• Amazon MQ runs on servers, can run in Multi-AZ with failover
• Amazon MQ has both queue feature (~SQS) and topic features (~SNS)


Cloud Monitoring Section :-

Amazon CloudWatch Metrics
• CloudWatch provides metrics for every services in AWS
• Metric is a variable to monitor (CPUUtilization, NetworkIn…)
• Metrics have timestamps
• Can create CloudWatch dashboards of metrics

Amazon CloudWatch Alarms
• Alarms are used to trigger notifications for any metric
• Alarms actions…
• Auto Scaling: increase or decrease EC2 instances “desired” count
• EC2 Actions: stop, terminate, reboot or recover an EC2 instance
• SNS notifications: send a notification into an SNS topic
• Various options (sampling, %, max, min, etc…)
• Can choose the period on which to evaluate an alarm
• Example: create a billing alarm on the CloudWatch Billing metric (Only available in us-east-1)
 Alarm States: OK. INSUFFICIENT_DATA, ALARM

 
CloudWatch Logs can collect log from:
• Elastic Beanstalk: collection of logs from application
• ECS: collection from containers
• AWS Lambda: collection from function logs
• CloudTrail based on filter
• CloudWatch log agents: on EC2 machines or on-premises servers
• Route53: Log DNS queries
• Enables real-time monitoring of logs
• Adjustable CloudWatch Logs retention

Eg.Can be used for troubleshooting

CloudWatch Logs for EC2:-

• By default, no logs from your EC2 instance will go to CloudWatch
• You need to run a CloudWatch agent on EC2 to push the log files you want
• Make sure IAM permissions are correct
• The CloudWatch log agent can be setup on-premises too

Amazon EventBridge (formerly CloudWatch Events):-

• Schedule: Cron jobs (scheduled scripts) . Its like schedule a cron job to trigger a event which will trigger a lambda function
• Event Pattern: Event rules to react to a service doing something . Everytime anyone signs in to a root ID , SNS topic will send a email.
• Trigger Lambda functions, send SQS/SNS messages…

In AWS EventBridge you can have :-

1)Default Event Bus
2)Partner Event Bus
3)Custom Event Bus

-There is a schema registry to model how the event looks like
-You can archieve events (All/Filter) sent to an event bus (indefinitely or set period)
-Ability to replay archieved events.
-Event Bridge Scheduler is used to schedule events.

For eg. If we want a notification for EC2 state change we can use rule using event- rule pattern in aws eventbridge.

AWS CloudTrail :-

• Provides governance, compliance and audit for your AWS Account
• CloudTrail is enabled by default!
• Get an history of events / API calls made within your AWS Account by:
• Console
• SDK
• CLI
• AWS Services
• Can put logs from CloudTrail into CloudWatch Logs or S3
• A trail can be applied to All Regions (default) or a single Region.
• If a resource is deleted in AWS, investigate CloudTrail first!

CLoud trail will have all the logs information for Console , SDK , CLI , AWS Services .If we want to retain this logs , audit , governance data , we can move it to cloudwatch logs or S3.

AWS X-Ray
• Debugging in Production, the good old way:
• Test locally
• Add log statements everywhere
• Re-deploy in production
• Log formats differ across applications and log analysis is hard. 
• Debugging: one big monolith “easy”, distributed services “hard”
• No common views of your entire architecture
• Enter… AWS X-Ray!

-Using Xray we can troubleshoot entire app like microservices

Amazon CodeGuru
• An ML-powered service for automated code reviews and application performance recommendations 
• Provides two functionalities :-
  • CodeGuru Reviewer: automated code reviews for static code analysis (development)
  • CodeGuru Profiler: visibility/recommendations about application performance during runtime (production)

Code Profiler Eg. :- Example: identify if your application is consuming 
excessive CPU capacity on a logging routine

Code Guru Eg. - Example: common coding best practices, resource leaks, security detection, input validation


AWS Health Dashboard - Service History :-

• Shows all regions, all services health
• Shows historical information for each day
• Has an RSS feed you can subscribe to
• Previously called AWS Service Health Dashboard

AWS Health Dashboard –Your Account
• Global service 
• Shows how AWS outages directly impact you & 
your AWS resources
• Alert, remediation, proactive, scheduled activities

***Service HEalth is for i general health of aws services.Gives health conditions of aws services offered worldwide.
***whereas Account health is related to your account issues.Your event log will have your created issues regarding aws services.

Q.Which CloudWatch feature would you use to trigger notifications when a metric reaches a threshold you specify?
->Cloudwatch Alarm

Q.If a resource is deleted in AWS, which service should you use to investigate first?
-> Cloudtrail.Cloudtrail can record the history of events/API calls made within your aws account , which will help determine who or what deleted the resource  .You should investigate it first.


VPC (Virtual Private Cloud) :-

IP Addresses in AWS
• IPv4 – Internet Protocol version 4 (4.3 Billion Addresses)
• Public IPv4 – can be used on the Internet
• EC2 instance gets a new a public IP address every time you stop then start it (default)
• Private IPv4 – can be used on private networks (LAN) such as internal AWS networking 
(e.g., 192.168.1.1)
• Private IPv4 is fixed for EC2 Instances even if you start/stop them
• Elastic IP – allows you to attach a fixed public IPv4 address to EC2 instance
• Note: all public IPv4 on AWS will be charged $0.005 per hour (including EIP)
• Free Tier: 750 hours usage per month
• IPv6 – Internet Protocol version 6 (3.4 × 10!" Addresses)
• Every IP address is public in AWS (no private range)
• Example: 2001:db8:3333:4444:cccc:dddd:eeee:ffff
• Free

VPC & Subnets Primer :-

• VPC - Virtual Private Cloud: private network to deploy your resources (regional resource)
• Subnets allow you to partition your network inside your VPC (Availability Zone resource)
• A public subnet is a subnet that is accessible from the internet
• A private subnet is a subnet that is not accessible from the internet
• To define access to the internet and between subnets, we use Route Tables

Internet Gateway & NAT(Network Address Translation) Gateways:-

• Internet Gateways helps our VPC instances connect with the internet
• Public Subnets have a route to the internet gateway.
• NAT Gateways (AWS-managed) & NAT Instances (self-managed) allow your instances in your Private Subnets to access the internet while remaining private
-Internet gateway is at the VPC level.

Network ACL & Security Groups :-

NACL (Network ACL):-

• A firewall which controls traffic from and to subnet
• Can have ALLOW and DENY rules
• Are attached at the Subnet level (Subnet is at AZ level.A AZ can have both public and private subnets).
• Rules only include IP addresses
-NACL is only applied to public subnets because these are the internet facing subnets.Private subnets anyways doesnt face internet and is private/ unknown to public.

Security Groups:-

• A firewall that controls traffic to and from an EC2 Instance
• Can have only ALLOW rules
• Rules include IP addresses and other security groups

VPC Flow Logs:-

• Capture information about IP traffic going into your interfaces:
• VPC Flow Logs
• Subnet Flow Logs
• Elastic Network Interface Flow Logs
• Helps to monitor & troubleshoot connectivity issues. Example: 
• Subnets to internet
• Subnets to subnets
• Internet to subnets
• Captures network information from AWS managed interfaces too: Elastic Load Balancers, ElastiCache, RDS, Aurora, etc… 
• VPC Flow logs data can go to S3, CloudWatch Logs, and Amazon Data Firehose

VPC Peering :-
• Connect two VPC, privately using AWS’ network
• Make them behave as if they were in the same network
• Must not have overlapping CIDR (IP address range)
• VPC Peering connection is not transitive (must be established for each VPC that need to communicate with one another)

VPC Endpoints :-

• Endpoints allow you to connect to AWS Services using a private network instead of the public www network
• This gives you enhanced security and lower latency to access AWS services
• VPC Endpoint Gateway: S3 & DynamoDB
• VPC Endpoint Interface: most services (including S3 & DynamoDB)

-Pretty much every service has a Interface endpoint and AWS S3 and DynamoDB has also a gateway endpoint.These endpoints are made to access private subnets using aws's private network.

AWS PrivateLink (VPC Endpoint Services) :-

You have a private office network, and you want to connect to a service (like a file server or a database) without going through the public internet. You want the connection to be secure and private.

That’s exactly what AWS PrivateLink does—but for cloud services.

🔐 What AWS PrivateLink Does
It allows you to:

Connect privately to AWS services (like S3, EC2) or third-party services.
Use private IP addresses instead of public ones.
Avoid exposing your data to the internet.
🏢 Real-World Analogy
Think of it like a private tunnel between your office and a trusted partner’s office. You don’t use public roads (internet); you use a dedicated, secure path.

🛠️ How It Works (Simplified)
A service (like a database or app) is hosted in AWS.
You want to access it securely from your own AWS environment.
You create a PrivateLink connection.
Now, your data travels privately inside AWS—never touching the public internet.

Site to Site VPN & Direct Connect :-

• Site to Site VPN
• Connect an on-premises VPN to AWS
• The connection is automatically encrypted
• Goes over the public internet

• Direct Connect (DX)
• Establish a physical connection between on-premises and AWS
• The connection is private, secure and fast
• Goes over a private network
• Takes at least a month to establish

Site-to-Site VPN
• On-premises: must use a Customer Gateway (CGW)
• AWS: must use a Virtual Private Gateway (VGW)

AWS Client VPN
• Connect from your computer using OpenVPN to your private network in AWS and on-premises
• Allow you to connect to your EC2 instances over a private IP (just as if you were in the private VPC network)
• Goes over public Internet

Transit Gateway :-
• For having transitive peering between thousands of VPC and on-premises, 
hub-and-spoke (star) connection
• One single Gateway to provide this functionality
• Works with Direct Connect Gateway, VPN connections

-A single gateway for 1000s of VPC and connections

ENI - Elastic Network Interface
Elastic IP –fixed public IPv4, ongoing cost if not in-use

AWS Shared Responsibility Model :-

• AWS responsibility - Security of the Cloud
• Protecting infrastructure (hardware, software, facilities, and networking) that runs all the AWS services
• Managed services like S3, DynamoDB, RDS, etc.
• Customer responsibility - Security in the Cloud
• For EC2 instance, customer is responsible for management of the guest OS (including security patches and updates), firewall & network configuration, IAM 
• Encrypting application data 
• Shared controls:
• Patch Management, Configuration Management, Awareness & Training

Example, for RDS
• AWS responsibility:
• Manage the underlying EC2 instance, disable SSH access
• Automated DB patching
• Automated OS patching
• Audit the underlying instance and disks & guarantee it functions
• Your responsibility:
• Check the ports / IP / security group inbound rules in DB’s SG
• In-database user creation and permissions
• Creating a database with or without public access
• Ensure parameter groups or DB is configured to only allow SSL connections
• Database encryption setting

Example, for S3 :-
• AWS responsibility:
• Guarantee you get unlimited storage 
• Guarantee you get encryption 
• Ensure separation of the data between different customers 
• Ensure AWS employees can’t access your data 
• Your responsibility: 
• Bucket configuration 
• Bucket policy / public setting 
• IAM user and roles 
• Enabling encryption


DDOS Protection on AWS :-
• AWS Shield Standard: protects against DDOS attack for your website and applications, for all customers at no additional costs
• AWS Shield Advanced: 24/7 premium DDoS protection
• AWS WAF: Filter specific requests based on rules
• CloudFront and Route 53: 
• Availability protection using global edge network
• Combined with AWS Shield, provides attack mitigation at the edge
• Be ready to scale – leverage AWS Auto Scaling

AWS Shield :-

• AWS Shield Standard:
• Free service that is activated for every AWS customer
• Provides protection from attacks such as SYN/UDP Floods, Reflection attacks and other layer 3/layer 4 attacks
• AWS Shield Advanced: 
• Optional DDoS mitigation service ($3,000 per month per organization) 
• Protect against more sophisticated attack on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53
• 24/7 access to AWS DDoS response team (DRP)
• Protect against higher fees during usage spikes due to DDoS

AWS WAF – Web Application Firewall
• Protects your web applications from common web exploits (Layer 7)
• Layer 7 is HTTP (vs Layer 4 is TCP)
• Deploy on Application Load Balancer, API Gateway, CloudFront
• Define Web ACL (Web Access Control List):
• Rules can include IP addresses, HTTP headers, HTTP body, or URI strings
• Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
• Size constraints, geo-match (block countries)
• Rate-based rules (to count occurrences of events) – for DDoS protection



Q.How do we protect Your VPC overall?
-Using 

AWS Network Firewall :-

AWS Network Firewall
• Protect your entire Amazon VPC
• From Layer 3 to Layer 7 protection 
• Any direction, you can inspect
• VPC to VPC traffic
• Outbound to internet
• Inbound from internet
• To / from Direct Connect & Site-to-Site VPN

AWS Firewall Manager
• Manage security rules in all accounts of an AWS Organization
• Security policy: common set of security rules
• VPC Security Groups for EC2, Application Load Balancer, etc…
• WAF rules
• AWS Shield Advanced
• AWS Network Firewall
• Rules are applied to new resources as they are created (good for 
compliance) across all and future accounts in your Organization

Penetration Testing on your AWS Cloud
• Prohibited Activities
• DNS zone walking via Amazon Route 53 Hosted Zones
• Denial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS
• Port flooding
• Protocol flooding
• Request flooding (login request flooding, API request flooding)
• For any other simulated events, contact aws-security-simulatedevent@amazon.com
• Read more: https://aws.amazon.com/security/penetration-testing/

Data at rest vs. Data in transit
• At rest: data stored or archived on a device
• On a hard disk, on a RDS instance, in S3 Glacier Deep Archive, etc.
• In transit (in motion): data being moved from one location to another
• Transfer from on-premises to AWS, EC2 to DynamoDB, etc.
• Means data transferred on the network
• We want to encrypt data in both states to protect it!
• For this we leverage encryption keys

AWS KMS (Key Management Service)
• Anytime you hear “encryption” for an AWS service, it’s most likely KMS
• KMS = AWS manages the encryption keys for us

CloudHSM :-

• KMS => AWS manages the software for encryption
• CloudHSM => AWS provisions encryption hardware
• Dedicated Hardware (HSM = Hardware Security Module)
• You manage your own encryption keys entirely (not AWS)
• HSM device is tamper resistant

Types of KMS Keys
• Customer Managed Key:
• Create, manage and used by the customer, can enable or disable
• Possibility of rotation policy (new key generated every year, old key preserved)
• Possibility to bring-your-own-key 
• AWS Managed Key:
• Created, managed and used on the customer’s behalf by AWS
• Used by AWS services (aws/s3, aws/ebs, aws/redshift)
• AWS Owned Key:
• Collection of CMKs that an AWS service owns and manages to use in multiple accounts
• AWS can use those to protect resources in your account (but you can’t view the keys)
• CloudHSM Keys (custom keystore): 
• Keys generated from your own CloudHSM hardware device
• Cryptographic operations are performed within the CloudHSM cluster

AWS Certificate Manager (ACM) 
• Let’s you easily provision, manage, and deploy 
SSL/TLS Certificates
• Used to provide in-flight encryption for websites (HTTPS)

AWS Secrets Manager
• Newer service, meant for storing secrets
• Capability to force rotation of secrets every X days
• Automate generation of secrets on rotation (uses Lambda)
• Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)

AWS Artifact (not really a service)
• Portal that provides customers with on-demand access to AWS 
compliance documentation and AWS agreements
Can be used to support internal audit or compliance

• Artifact Reports - Allows you to download AWS security and compliance 
documents from third-party auditors, like AWS ISO certifications, Payment 
Card Industry (PCI), and System and Organization Control (SOC) reports
• Artifact Agreements - Allows you to review, accept, and track the status of 
AWS agreements such as the Business Associate Addendum (BAA) or the 
Health Insurance Portability and Accountability Act (HIPAA) for an individual 
account or in your organization

Amazon GuardDuty
• Intelligent Threat discovery to protect your AWS Account 
• Uses Machine Learning algorithms, anomaly detection, 3rd party data
• One click to enable (30 days trial), no need to install software
• Input data includes:
• CloudTrail Events Logs – unusual API calls, unauthorized deployments
• CloudTrail Management Events – create VPC subnet, create trail, …
• CloudTrail S3 Data Events – get object, list objects, delete object, …
• VPC Flow Logs – unusual internal traffic, unusual IP address
• DNS Logs – compromised EC2 instances sending encoded data within DNS queries
• Optional Features – EKS Audit Logs, RDS & Aurora, EBS, Lambda, S3 Data Events…
• Can setup EventBridge rules to be notified in case of findings
• EventBridge rules can target AWS Lambda or SNS
• Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)

Amazon Inspector
• Automated Security Assessments 
• For EC2 instances 
• Leveraging the AWS System Manager (SSM) agent
• Analyze against unintended network accessibility
• Analyze the running OS against known vulnerabilities
• For Container Images push to Amazon ECR
• Assessment of Container Images as they are pushed
• For Lambda Functions
• Identifies software vulnerabilities in function code and package 
dependencies
• Assessment of functions as they are deployed
• Reporting & integration with AWS Security Hub
• Send findings to Amazon Event Bridge

What does Amazon Inspector evaluate?
• Remember: only for EC2 instances, Container Images & Lambda 
functions
A risk score is associated with all vulnerabilities for prioritization


AWS Config :-

AWS Config
• Helps with auditing and recording compliance of your AWS resources
• Helps record configurations and changes over time

-Gives all the information about all the resources created by people from your org are compliant or not.

• View compliance of a resource over time 
• View configuration of a resource over time 
• View CloudTrail API calls if enabled

AWS Macie :-

• Amazon Macie is a fully managed data security and data privacy service 
that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.
• Macie helps identify and alert you to sensitive data, such as personally


AWS Security Hub
• Central security tool to manage security across several AWS accounts and automate 
security checks
• Integrated dashboards showing current security and compliance status to quickly take 
actions
• Automatically aggregates alerts in predefined or personal findings formats from various 
AWS services & AWS partner tools: 
• Config
• GuardDuty
• Inspector
• Macie
• IAM Access Analyzer
• AWS Systems Manager
• AWS Firewall Manager
• AWS Health
• AWS Partner Network Solutions
• Must first enable the AWS Config Service

Amazon Detective
• GuardDuty, Macie, and Security Hub are used to identify potential 
security issues, or findings
• Sometimes security findings require deeper analysis to isolate the root 
cause and take action – it’s a complex process
• Amazon Detective analyzes, investigates, and quickly identifies the 
root cause of security issues or suspicious activities (using ML and graphs)
• Automatically collects and processes events from VPC Flow Logs, 
CloudTrail, GuardDuty and create a unified view
• Produces visualizations with details and context to get to the root cause

AWS Abuse
• Report suspected AWS resources used for abusive or illegal purposes
• Abusive & prohibited behaviors are: 
• Spam – receving undesired emails from AWS-owned IP address, websites & forums 
spammed by AWS resources
• Port scanning – sending packets to your ports to discover the unsecured ones
• DoS or DDoS attacks – AWS-owned IP addresses attempting to overwhlem or crash your servers/softwares
• Intrusion attempts – logging in on your resources
• Hosting objectionable or copyrighted content – distributing illegal or copyrighted content without consent
• Distributing malware – AWS resources distributing softwares to harm computers or machines
• Contact the AWS Abuse team: AWS abuse form, or abuse@amazonaws.com

Root user privileges
• Root user = Account Owner (created when the account is created)
• Has complete access to all AWS services and resources
• Lock away your AWS account root user access keys!
• Do not use the root account for everyday tasks, even administrative tasks
• Actions that can be performed only by the root user:
• Change account settings (account name, email address, root user password, root user access keys)
• View certain tax invoices
• Close your AWS account
• Restore IAM user permissions
• Change or cancel your AWS Support plan
• Register as a seller in the Reserved Instance Marketplace
• Configure an Amazon S3 bucket to enable MFA
• Edit or delete an Amazon S3 bucket policy that includes an invalid VPC ID or VPC endpoint ID
• Sign up for GovCloud

AWS Access Analyzer :-

-is a security and visibility tool that helps you identify and monitor resources in your AWS account that are shared externally — either with other AWS accounts, services, or the public internet.

Q.According to the Shared Responsibility Model, who is responsible for firewall and network configuration for EC2 Instances?
->Customer

Q.A company would like to protect its web applications from common web exploits that may affect availability, compromise security, or consume excessive resources. Which AWS service should they use?
->Web application firewall(WAF)

Q.You want to record configurations and changes over time. Which service allows you to do this?
-AWS Config

Q.According to the Shared Responsibility Model, who is responsible for Patch Management?
AWS is responsible for patching and fixing flaws within the infrastructure , but customers are responsible for patching their guest OS and applications.Shared controls also includes configuration management , awareness and training.

Question 13:
A company would like to automate security on EC2 instances to assess security and vulnerabilities in these instances. Which AWS service should it use?

Amazon Rekognition
• Find objects, people, text, scenes in images and videos using ML
• Facial analysis and facial search to do user verification, people counting
• Create a database of “familiar faces” or compare against celebrities
• Use cases:
• Labeling
• Content Moderation
• Text Detection
• Face Detection and Analysis (gender, age range, emotions…)
• Face Search and Verification
• Celebrity Recognition
• Pathing (ex: for sports game analysis)

Amazon Transcribe
• Automatically convert speech to text
• Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately
• Automatically remove Personally Identifiable Information (PII) using Redaction
• Supports Automatic Language Identification for multi-lingual audio

Amazon Polly 
• Turn text into lifelike speech using deep learning 
• Allowing you to create applications that talk

Amazon Translate
• Natural and accurate language translation
• Amazon Translate allows you to localize content - such as websites and
applications - for international users, and to easily translate large
volumes of text efficiently.

Amazon Lex & Connect
• Amazon Lex: (same technology that powers Alexa)
• Automatic Speech Recognition (ASR) to convert speech to text
• Natural Language Understanding to recognize the intent of text, callers
• Helps build chatbots, call center bots
• Amazon Connect:
• Receive calls, create contact flows, cloud-based virtual contact center

Amazon Comprehend
• For Natural Language Processing – NLP
• Fully managed and serverless service
• Uses machine learning to find insights and relationships in text

Amazon SageMaker
• Fully managed service for developers / data scientists to build ML models
• Typically, difficult to do all the processes in one place + provision servers
• Machine learning process (simplified): predicting your exam score

Amazon SageMaker
• Fully managed service for developers / data scientists to build ML models
• Typically, difficult to do all the processes in one place + provision servers
• Machine learning process (simplified): predicting your exam score


Amazon Kendra
• Fully managed document search service powered by Machine Learning
• Extract answers from within a document (text, pdf, HTML, PowerPoint, MS Word, FAQs…)
• Natural language search capabilities

Amazon Personalize
• Fully managed ML-service to build apps with real-time personalized recommendations
• Example: personalized product recommendations/re-ranking, customized direct marketing
• Example: User bought gardening tools, provide recommendations on the next one to buy
• Same technology used by Amazon.com
• Integrates into existing websites, applications, SMS, email marketing systems, …

Amazon Textract
• Automatically extracts text, handwriting, and data from any scanned
documents using AI and ML

Q.A research team would like to group articles by topics using Natural Language Processing (NLP). Which service should they use?

->AWS Comprehend

Q.Which AWS service makes it easy to convert speech-to-text?
->Transcribe

Q.Which of the following services is a document search service powered by machine learning?
->Kendra

AWS Organizations
• Global service
• Allows to manage multiple AWS accounts
• The main account is the master account
• Cost Benefits:
• Consolidated Billing across all accounts - single payment method
• Pricing benefits from aggregated usage (volume discount for EC2, S3…)
• Pooling of Reserved EC2 instances for optimal savings
• API is available to automate AWS account creation
• Restrict account privileges using Service Control Policies (SCP)

Multi Account Strategies
• Create accounts per department, per cost center, per dev / test /
prod, based on regulatory restrictions (using SCP), for better
resource isolation (ex: VPC), to have separate per-account
service limits, isolated account for logging

Organizational Units (OU) - Examples :-

-In this we have a single master Account which can have multiple organizational units .
-Like a master account will have dev OU , Test OU, Production OU

Service Control Policies (SCP)
• Whitelist or blacklist IAM actions
• Applied at the OU or Account level
• Does not apply to the Master Account
• SCP is applied to all the Users and Roles of the Account, including Root user
• The SCP does not affect service-linked roles
• Service-linked roles enable other AWS services to integrate with AWS Organizations
and can't be restricted by SCPs.
• SCP must have an explicit Allow (does not allow anything by default)
• Use cases:
• Restrict access to certain services (for example: can’t use EMR)
• Enforce PCI compliance by explicitly disabling services

AWS Organization – Consolidated Billing
• When enabled, provides you with:
• Combined Usage – combine the usage across all AWS accounts in the AWS Organization to
share the volume pricing, Reserved Instances and Savings Plans discounts
• One Bill – get one bill for all AWS Accounts in the AWS Organization
• The management account can turn off Reserved Instances discount sharing for any
account in the AWS Organization, including itself

You purchase:

    🎟️ 1 Reserved Instance for t3.medium in Account A

Now:

    If Account B or Account C uses a t3.medium instance that matches the RI attributes (region, AZ, tenancy, etc.):

        The RI discount is automatically applied to that account’s usage.

        No need to reserve separately in each account.

AWS Control Tower
• Easy way to set up and govern a secure and compliant multi-account
AWS environment based on best practices

Benefits:
• Automate the set up of your environment in a few clicks
• Automate ongoing policy management using guardrails
• Detect policy violations and remediate them
• Monitor compliance through an interactive dashboard
• AWS Control Tower runs on top of AWS Organizations:
• It automatically sets up AWS Organizations to organize accounts and implement
SCPs (Service Control Policies)

AWS Control Tower is a governance and automation tool that helps you set up and manage a secure, multi-account AWS environment based on best practices.

AWS Resource Access Manager (AWS RAM) :-

• Share AWS resources that you own with other AWS accounts
• Share with any account or within your Organization
• Avoid resource duplication!
• Supported resources include Aurora, VPC Subnets, Transit Gateway, Route 53, EC2 Dedicated Hosts,License Manager Configurations…


AWS Service Catalog
• Users that are new to AWS have too many options, and may create
stacks that are not compliant / in line with the rest of the organization
• Some users just want a quick self-service portal to launch a set of
authorized products pre-defined by admins
• Includes: virtual machines, databases, storage options, etc…

Pricing Models in AWS
• AWS has 4 pricing models:
• Pay as you go: pay for what you use, remain agile, responsive, meet scale
demands
• Save when you reserve: minimize risks, predictably manage budgets,
comply with long-terms requirements
• Reservations are available for EC2 Reserved Instances, DynamoDB Reserved
Capacity, ElastiCache Reserved Nodes, RDS Reserved Instance, Redshift Reserved
Nodes
• Pay less by using more: volume-based discounts
• Pay less as AWS grows

Compute Pricing EC2 :-

• On demand instances:
• Minimum of 60s • Pay per second (Linux/Windows) or per hour (other)
• Reserved instances: • Up to 75% discount compared to On demand on hourly rate
-1 or 3 years commitment
• All upfront, partial upfront, no upfront

• Spot instances:
• Up to 90% discount compared to On demand on hourly rate
-Bid for unused capacity 

• Dedicated Host:
• On-demand 
• Reservation for 1 year or 3 years commitment 

• Savings plans as an alternative to save on sustained usage

AWS Lambda :-
    Pay for:
    ✅ Each invocation
    ✅ Execution duration

    No server management

    Best for: Small, event-driven tasks

🐳 ECS (EC2 Launch Type)

    ECS is free

    Pay for:
    ✅ EC2 instances
    ✅ Other AWS resources (e.g., ALB, EBS)

    You manage EC2s

    Best for: Cost-saving if EC2 is well utilized

🧱 ECS (Fargate Launch Type)

    Pay for:
    ✅ vCPU + Memory per container
    ✅ Task duration

    No EC2 to manage

    Best for: Simple container apps without infra management

Storage Pricing - EBS :-
• Volume type (based on performance) 
• Storage volume in GB per month provisionned 
• IOPS: • General Purpose SSD: Included 
• Provisioned IOPS SSD: Provisionned amount in IOPS 
• Magnetic: Number of requests 
• Snapshots: 
• Added data cost per GB per month 
• Data transfer: 
• Outbound data transfer are tiered for volume discounts 
• Inbound is free

Database Pricing - RDS
• Per hour billing
• Database characteristics:
• Engine
• Size
• Memory class
• Purchase type:
• On-demand
• Reserved instances (1 or 3 years) with optional up-front
• Backup Storage: There is no additional charge for backup storage up to
100% of your total database storage for a region.

• Additional storage (per GB per month) • Number of input and output requests per month • Deployment type (storage and I/O are variable): • Single AZ • Multiple AZs • Data transfer: • Outbound data transfer are tiered for volume discounts • Inbound is free

Content Delivery
– CloudFront
• Pricing is different across different geographic regions • Aggregated for each edge location, then applied to your bill • Data Transfer Out (volume discount) • Number of HTTP/HTTPS requests

AWS Networking Costs :-

• Use Private IP instead of Public IP for good savings and better network performance
• Use same AZ for maximum savings (at the cost of high availability)

🧩 Two Types of Savings Plans
-Commit a certain dollar amount for every hour for 1 or 3 years
1. 🖥️ EC2 Instance Savings Plan

    ✅ Up to 72% discount

    You commit to a specific instance family in a specific region
    Example: m5 in us-east-1

    Flexibility within that family:

        Any size: m5.large, m5.4xlarge

        Any OS: Linux, Windows

        Any Availability Zone (AZ) or Tenancy (Shared/Dedicated)

📌 Cheaper than Compute Plan, but less flexible
2. ⚙️ Compute Savings Plan

    ✅ Up to 66% discount

    Most flexible option:

        Any instance family

        Any region

        Any size, OS, tenancy

        Also applies to Fargate and Lambda

📌 Great if you use multiple services, move between regions, or want maximum freedom

AWS Compute Optimizer • Reduce costs and improve performance by recommending optimal AWS resources for your
workloads
• Helps you choose optimal configurations and right
- size your workloads (over/under provisioned)
• Uses Machine Learning to analyze your resources’
configurations and their utilization CloudWatch
metrics

Billing and Costing Tools :-
• Estimating costs in the cloud: 
• Pricing Calculator 
• Tracking costs in the cloud: 
• Billing Dashboard 
• Cost Allocation Tags 
• Cost and Usage Reports 
• Cost Explorer 
• Monitoring against costs plans: 
• Billing Alarms 
• Budgets

Cost Allocation Tags
• Use cost allocation tags to track your AWS costs on a detailed level
• AWS generated tags
• Automatically applied to the resource you create
• Starts with Prefix aws: (e.g. aws: createdBy)
• User-defined tags
• Defined by the user
• Starts with Prefix user: 

Tags can be used to create Resource Groups
• Create, maintain, and view a collection of resources that share common tags
• Manage these tags using the Tag Editor

-Tags are used for organizing resources

Cost and Usage Reports
• Dive deeper into your AWS costs and usage
• The AWS Cost & Usage Report contains the most comprehensive set of AWS cost and usage data available, including additional metadata
about AWS services, pricing, and reservations (e.g., Amazon EC2 Reserved Instances (RIs)).

Cost Explorer
• Visualize, understand, and manage your AWS costs and usage over time
• Create custom reports that analyze cost and usage data.
• Analyze your data at a high level: total costs and usage across all accounts
• Or Monthly, hourly, resource level granularity
• Choose an optimal Savings Plan (to lower prices on your bill)
• Forecast usage up to 12 months based on previous usage

- a visual explorer used to forecast your usage.

Billing Alarms in CloudWatch:-

• Billing data metric is stored in CloudWatch us-east1
• Billing data are for overall worldwide AWS costs

AWS Budgets
• Create budget and send alarms when costs exceeds the budget
• 4 types of budgets: Usage, Cost, Reservation, Savings Plans
• For Reserved Instances (RI)
• Track utilization
• Supports EC2, ElastiCache, RDS, Redshift
• Up to 5 SNS notifications per budget
• Can filter by: Service, Linked Account, Tag, Purchase Option, Instance
Type, Region, Availability Zone, API Operation, etc…
• Same options as AWS Cost Explorer!
• 2 budgets are free, then $0.02/day/budget

AWS Cost Anomaly Detection
• Continuously monitor your cost and usage using ML to detect unusual spends
• It learns your unique, historic spend patterns to detect one-time cost spike and/or
continuous cost increases (you don’t need to define thresholds)
• Monitor AWS services, member accounts, cost allocation tags, or cost categories
• Sends you the anomaly detection report with root-cause analysis
• Get notified with individual alerts or daily/weekly summary (using SNS)

AWS Service Quotas
• Notify you when you’re close to a service quota value threshold
• Create CloudWatch Alarms on the Service Quotas console
• Example: Lambda concurrent executions

Trusted Advisor • No need to install anything
– high level AWS account assessment
• Analyze your AWS accounts and provides
recommendation on 6 categories: • Cost optimization • Performance • Security • Fault tolerance • Service limits • Operational Excellence
• Business & Enterprise Support plan • Full Set of Checks • Programmatic Access using AWS Support API

AWS Support plans :-

1)Basic Support Plan : 

Customer Service & Communities - 24x7 access to customer service,
documentation, whitepapers, and support forums.
• AWS Trusted Advisor - Access to the 7 core Trusted Advisor checks
and guidance to provision your resources following best practices to
increase performance and improve security.
• AWS Personal Health Dashboard - A personalized view of the health
of AWS services, and alerts when your resources are impacted

AWS Developer Support Plan:-
• All Basic Support Plan + • Business hours email access to Cloud Support Associates • Unlimited cases / unlimited contacts • Case severity / response times: • General guidance: < 24 business hours • System impaired: < 12 business hours

AWS Business Support Plan (24/7)
• Intended to be used if you have production workloads
• Trusted Advisor – Full set of checks + API access
• 24x7 phone, email, and chat access to Cloud Support Engineers
• Unlimited cases / unlimited contacts
• Access to Infrastructure Event Management for additional fee.
• Case severity / response times:
• General guidance: < 24 business hours
• System impaired: < 12 business hours
• Production system impaired: < 4 hours
• Production system down: < 1 hour

AWS Enterprise On-Ramp Support Plan (24/7)
• Intended to be used if you have production or business critical workloads
• All of Business Support Plan +
• Access to a pool of Technical Account Managers (TAM)
• Concierge Support Team (for billing and account best practices)
• Infrastructure Event Management, Well-Architected & Operations Reviews
• Case severity / response times:
• …
• Production system impaired: < 4 hours
• Production system down: < 1 hour
• Business-critical system down: < 30 minutes

AWS Enterprise Support Plan (24/7)
• Intended to be used if you have mission critical workloads
• All of Business Support Plan +
• Access to a designated Technical Account Manager (TAM)
• Concierge Support Team (for billing and account best practices)
• Infrastructure Event Management, Well-Architected & Operations Reviews
• Access to AWS Incident Detection and Response (for an additional fee)
• Business-critical system down: < 15 minutes

Q.Which services are free to use in AWS?
->VPC,IAM,Consolidated Billing and elastic beanstalk

Q.CloudFront pricing is the same in every geographic region.
->DIffers accross every geographic region

q.When you reserve, the larger the upfront payment, the smaller the discount.
->Larger the upfront payment , bigger the discount

Q.EBS Snapshots are added cost in GB per month?
->True.The added data storage by EBS snapshots are added cost in GB per month to EBS pricing.

Question 8:
Which pricing model allows you to minimize risks, predictably manage budgets, and comply with long-term requirements, and is available for EC2, DynamoDB, ElastiCache, RDS, and Redshift?
->Save when you reserve











































































